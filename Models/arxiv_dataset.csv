text,label,source_url,source_db,label_id
"The behavioral description of the sensorimotor synchronization phenomenon in humans is exhaustive, mostly by using variations of the traditional paced finger-tapping task. This task helps unveil the inner workings of the error-correction mechanism responsible for the resynchronization after a perturbation to the period of the stimuli sequence. Yet, fundamental contradictions still exist among different works in the literature. One of such contradictions only emerges after comparing the two most-common period perturbation types: step changes and phase shifts. The stimulus sequence is exactly the same in both perturbation types up to and including the (unexpected) perturbed stimulus. Why then would the timing of the next response be different between perturbation types, as observed? The explanation lies in the buildup of different temporal contexts during the experiments that recalibrate the error-correction mechanism. Here we show, both experimentally and theoretically, that responses to different perturbation types are dynamically incompatible when they occur in separate experiments. That is, they can't be represented by the same underlying dynamical system, thus explaining many contradictory results and the difficulty in reproducing both types of perturbations with a single mathematical model. On the other hand, if both perturbation types are presented at random during the same experiment then the responses are compatible with each other and can be construed as produced by a unique underlying mechanism. We conclude that a single underlying dynamical system can represent the response to all perturbation types, signs, and sizes, which is nevertheless recalibrated by temporal context. Our results offer a ground for performing better comparisons in paced finger tapping and extend the usable range of data beyond the perturbed stimulus and into the information-rich resynchronization phase.",Neuroscience,http://arxiv.org/abs/2512.23661v1,arXiv,0
"Sensory prediction (SP) is a fundamental mechanism of perception that supports cognitive development. Atypical SP has been reported across multiple neurodevelopmental disorders (ND), suggesting it may constitute an early cross-syndromic marker. Premature birth is a major risk factor for ND, with risk increasing as gestational age (GA) at birth decreases. However, how perinatal risk factors shape the development of SP remains poorly understood. We do not know if untimely birth itself, or exposure to iatrogenic pain during neonatal intensive care, cause neurodevelopmental impairments. In this study, we first assessed whether SP can be detected in the brains of premature neonates at 35 weeks corrected GA using a tactile oddballomission paradigm with EEG. We then examined the effects of the degree of prematurity and of the exposure to painful care procedures on neural indices of SP. Results demonstrate the presence of repetition suppression (RS) and a mismatch response (MMR) to deviant stimuli in the contralateral somatosensory cortex of premature neonates. The amplitude of these SP proxies was significantly affected by the number of painful procedures experienced since birth, independently of the effect of GA at birth. Contrary to our initial hypothesis that greater neurodevelopmental risk would be associated with less mature SP, infants with higher exposure to pain exhibited more robust indices of SP. These findings suggest that increased ex utero experience, even painful, is associated with accelerated maturation of predictive somatosensory processing. Longitudinal follow-up of participants at age 2 will explore how these early markers relate to developmental outcomes.",Neuroscience,http://arxiv.org/abs/2512.23301v1,arXiv,0
"We introduce a biologically inspired, multilayer neural architecture composed of Rectified Spectral Units (ReSUs). Each ReSU projects a recent window of its input history onto a canonical direction obtained via canonical correlation analysis (CCA) of previously observed past-future input pairs, and then rectifies either its positive or negative component. By encoding canonical directions in synaptic weights and temporal filters, ReSUs implement a local, self-supervised algorithm for progressively constructing increasingly complex features.   To evaluate both computational power and biological fidelity, we trained a two-layer ReSU network in a self-supervised regime on translating natural scenes. First-layer units, each driven by a single pixel, developed temporal filters resembling those of Drosophila post-photoreceptor neurons (L1/L2 and L3), including their empirically observed adaptation to signal-to-noise ratio (SNR). Second-layer units, which pooled spatially over the first layer, became direction-selective -- analogous to T4 motion-detecting cells -- with learned synaptic weight patterns approximating those derived from connectomic reconstructions.   Together, these results suggest that ReSUs offer (i) a principled framework for modeling sensory circuits and (ii) a biologically grounded, backpropagation-free paradigm for constructing deep self-supervised neural networks.",Neuroscience,http://arxiv.org/abs/2512.23146v1,arXiv,0
"Decision paralysis, i.e. hesitation, freezing, or failure to act despite full knowledge and motivation, poses a challenge for choice models that assume options are already specified and readily comparable. Drawing on qualitative reports in autism research that are especially salient, we propose a computational account in which paralysis arises from convergence failure in a hierarchical decision process. We separate intent selection (what to pursue) from affordance selection (how to pursue the goal) and formalize commitment as inference under a mixture of reverse- and forward-Kullback-Leibler (KL) objectives. Reverse KL is mode-seeking and promotes rapid commitment, whereas forward KL is mode-covering and preserves multiple plausible goals or actions. In static and dynamic (drift-diffusion) models, forward-KL-biased inference yields slow, heavy-tailed response times and two distinct failure modes, intent saturation and affordance saturation, when values are similar. Simulations in multi-option tasks reproduce key features of decision inertia and shutdown, treating autism as an extreme regime of a general, inference-based, decision-making continuum.",Neuroscience,http://arxiv.org/abs/2512.23144v1,arXiv,0
"Integrating non-Euclidean brain imaging data with Euclidean tabular data, such as clinical and demographic information, poses a substantial challenge for medical imaging analysis, particularly in forecasting future outcomes. While machine learning and deep learning techniques have been applied successfully to cross-sectional classification and prediction tasks, effectively forecasting outcomes in longitudinal imaging studies remains challenging. To address this challenge, we introduce a time-aware graph neural network model with transformer fusion (GNN-TF). This model flexibly integrates both tabular data and dynamic brain connectivity data, leveraging the temporal order of these variables within a coherent framework. By incorporating non-Euclidean and Euclidean sources of information from a longitudinal resting-state fMRI dataset from the National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA), the GNN-TF enables a comprehensive analysis that captures critical aspects of longitudinal imaging data. Comparative analyses against a variety of established machine learning and deep learning models demonstrate that GNN-TF outperforms these state-of-the-art methods, delivering superior predictive accuracy for predicting future tobacco usage. The end-to-end, time-aware transformer fusion structure of the proposed GNN-TF model successfully integrates multiple data modalities and leverages temporal dynamics, making it a valuable analytic tool for functional brain imaging studies focused on clinical outcome prediction.",Neuroscience,http://arxiv.org/abs/2512.23137v1,arXiv,0
"Applying the lens of computation and information has been instrumental in driving the technological progress of our civilization as well as in empowering our understanding of the world around us. The digital computer was and for many still is the leading metaphor for how our mind operates. Information theory (IT) has also been important in our understanding of how nervous systems encode and process information. The target article deploys information and computation to bodies: to understand why they have evolved in particular ways (animal bodies) and to design optimal bodies (robots). In this commentary, I argue that the main role of bodies is not to compute.",Neuroscience,http://arxiv.org/abs/2512.22868v1,arXiv,0
"Dynamical modeling of multisite human intracranial neural recordings is essential for developing neurotechnologies such as brain-computer interfaces (BCIs). Linear dynamical models are widely used for this purpose due to their interpretability and their suitability for BCIs. In particular, these models enable flexible real-time inference, even in the presence of missing neural samples, which often occur in wireless BCIs. However, neural activity can exhibit nonlinear structure that is not captured by linear models. Furthermore, while recurrent neural network models can capture nonlinearity, their inference does not directly address handling missing observations. To address this gap, recent work introduced DFINE, a deep learning framework that integrates neural networks with linear state-space models to capture nonlinearities while enabling flexible inference. However, DFINE was developed for intracortical recordings that measure localized neuronal populations. Here we extend DFINE to modeling of multisite human intracranial electroencephalography (iEEG) recordings. We find that DFINE significantly outperforms linear state-space models (LSSMs) in forecasting future neural activity. Furthermore, DFINE matches or exceeds the accuracy of a gated recurrent unit (GRU) model in neural forecasting, indicating that a linear dynamical backbone, when paired and jointly trained with nonlinear neural networks, can effectively describe the dynamics of iEEG signals while also enabling flexible inference. Additionally, DFINE handles missing observations more robustly than the baselines, demonstrating its flexible inference and utility for BCIs. Finally, DFINE's advantage over LSSM is more pronounced in high gamma spectral bands. Taken together, these findings highlight DFINE as a strong and flexible framework for modeling human iEEG dynamics, with potential applications in next-generation BCIs.",Neuroscience,http://arxiv.org/abs/2512.22785v1,arXiv,0
"The phenomenal advances in large language models (LLMs) and other foundation models over the past few years have been based on optimizing large-scale transformer models on the surprisingly simple objective of minimizing next-token prediction loss, a form of predictive coding that is also the backbone of an increasingly popular model of brain function in neuroscience and cognitive science. However, current foundation models ignore three other important components of state-of-the-art predictive coding models: tight integration of actions with generative models, hierarchical compositional structure, and episodic memory. We propose that to achieve safe, interpretable, energy-efficient, and human-like AI, foundation models should integrate actions, at multiple scales of abstraction, with a compositional generative architecture and episodic memory. We present recent evidence from neuroscience and cognitive science on the importance of each of these components. We describe how the addition of these missing components to foundation models could help address some of their current deficiencies: hallucinations and superficial understanding of concepts due to lack of grounding, a missing sense of agency/responsibility due to lack of control, threats to safety and trustworthiness due to lack of interpretability, and energy inefficiency. We compare our proposal to current trends, such as adding chain-of-thought (CoT) reasoning and retrieval-augmented generation (RAG) to foundation models, and discuss new ways of augmenting these models with brain-inspired components. We conclude by arguing that a rekindling of the historically fruitful exchange of ideas between brain science and AI will help pave the way towards safe and interpretable human-centered AI.",Neuroscience,http://arxiv.org/abs/2512.22568v1,arXiv,0
"Cortical surface parcellation is a fundamental task in both basic neuroscience research and clinical applications, enabling more accurate mapping of brain regions. Model-based and learning-based approaches for automated parcellation alleviate the need for manual labeling. Despite the advancement in parcellation performance, learning-based methods shift away from registration and atlas propagation without exploring the reason for the improvement compared to traditional methods. In this study, we present JParc, a joint cortical registration and parcellation framework, that outperforms existing state-of-the-art parcellation methods. In rigorous experiments, we demonstrate that the enhanced performance of JParc is primarily attributable to accurate cortical registration and a learned parcellation atlas. By leveraging a shallow subnetwork to fine-tune the propagated atlas labels, JParc achieves a Dice score greater than 90% on the Mindboggle dataset, using only basic geometric features (sulcal depth, curvature) that describe cortical folding patterns. The superior accuracy of JParc can significantly increase the statistical power in brain mapping studies as well as support applications in surgical planning and many other downstream neuroscientific and clinical tasks.",Neuroscience,http://arxiv.org/abs/2512.22485v1,arXiv,0
"Deep artificial neural networks famously struggle to learn from non-stationary streams of data. Without dedicated mitigation strategies, continual learning is associated with continuous forgetting of previous tasks and a progressive loss of plasticity. Current approaches to continual learning have either focused on increasing the stability of representations of past tasks, or on promoting plasticity for future learning. Paradoxically, while animals including humans achieve a desirable stability-plasticity trade-off, the responses of biological neurons to external stimuli that are associated with stable behaviors gradually change over time. This suggests that, although unstable representations have historically been seen as undesirable in artificial systems, they could be a core property of biological neural networks learning continually. Here, we examine how linking representational drift to continual learning in biological neural networks could inform artificial systems. We highlight the existence of representational drift across numerous animal species and brain regions and propose that drift reflects a mixture of homeostatic turnover and learning-related synaptic plasticity. In particular, we evaluate how plasticity induced by learning new tasks could induce drift in the representation of previous tasks, and how such drift could accumulate across brain regions. In deep artificial neural networks, we propose that representational drift is only compatible with approaches that do not explicitly prevent parameter changes to mitigate forgetting. Remarkably, jointly promoting plasticity while mitigating forgetting could in principle induce representational drift in continual learning. While we argue that drift is a byproduct rather than a solution to incremental learning, its investigation could inform approaches to continual learning in artificial systems.",Neuroscience,http://arxiv.org/abs/2512.22045v1,arXiv,0
"Foundation models are emerging as a powerful paradigm for fMRI analysis, but current approaches face a dual bottleneck of data- and training-efficiency. Atlas-based methods aggregate voxel signals into fixed regions of interest, reducing data dimensionality but discarding fine-grained spatial details, and requiring extremely large cohorts to train effectively as general-purpose foundation models. Atlas-free methods, on the other hand, operate directly on voxel-level information - preserving spatial fidelity but are prohibitively memory- and compute-intensive, making large-scale pre-training infeasible. We introduce SLIM-Brain (Sample-efficient, Low-memory fMRI Foundation Model for Human Brain), a new atlas-free foundation model that simultaneously improves both data- and training-efficiency. SLIM-Brain adopts a two-stage adaptive design: (i) a lightweight temporal extractor captures global context across full sequences and ranks data windows by saliency, and (ii) a 4D hierarchical encoder (Hiera-JEPA) learns fine-grained voxel-level representations only from the top-$k$ selected windows, while deleting about 70% masked patches. Extensive experiments across seven public benchmarks show that SLIM-Brain establishes new state-of-the-art performance on diverse tasks, while requiring only 4 thousand pre-training sessions and approximately 30% of GPU memory comparing to traditional voxel-level methods.",Neuroscience,http://arxiv.org/abs/2512.21881v1,arXiv,0
"Stochastic burst-like oscillations are common in physiological signals, yet there are few compact generative models that capture their transient structure. We propose a numerical-twin framework that represents transient narrowband activity as a two-dimensional Ornstein-Uhlenbeck (OU) process with three interpretable parameters: decay rate, mean frequency, and noise amplitude. We develop two complementary estimation strategies. The first fits the power spectral density, amplitude distribution, and autocorrelation to recover OU-parameters. The second segments burst events and performs a statistical match between empirical spindle statistics (duration, amplitude, inter-event interval) and simulated OU output via grid search, resolving parameter degeneracies by including event counts. We extend the framework to multiple frequency bands and piecewise-stationary dynamics to track slow parameter drifts. Applied to electroencephalography (EEG) recorded during general anesthesia, the method identifies OU models that reproduce alpha-spindle (8-12 Hz) morphology and band-limited spectra with low residual error, enabling real-time tracking of state changes that are not apparent from band power alone. This decomposition yields a sparse, interpretable representation of transient oscillations and provides interpretable metrics for brain monitoring.",Neuroscience,http://arxiv.org/abs/2512.21768v1,arXiv,0
"Standard Spiking Neural Network (SNN) models typically neglect metabolic constraints, treating neurons as energetically unconstrained components. We bridge this gap by implementing a conductance-based leaky integrate-and-fire (gLIF) microcircuit (N=5,000) in Brian2, using temperature-dependent Q10 scaling to as a biophysically grounded proxy to couple metabolic state with intrinsic excitability and synaptic plasticity. Our simulations revealed five distinct emergent properties: (1) Dynamics Bifurcation: Learning trajectories diverged significantly, with hypometabolic states plateauing near baseline and hypermetabolic states exhibiting non-linear, runaway potentiation; (2) STDP Window Deformation: Thermal stress structurally deformed the plasticity kernel, where hypermetabolism sharpened coincidence detection and hypometabolism flattened synaptic integration; (3) Signal Degradation: While metabolic rate positively correlated with connectivity strength, high-energy states caused synaptic saturation and a loss of sparse coding specificity; (4) Topological Shift: Network activity transitioned from sparse, asynchronous firing in energy-restricted states to pathological, seizure-like hypersynchronization in high-energy states ; and (5) Parametric Robustness: Sensitivity analysis confirmed these attractor states were intrinsic biophysical properties, robust across random network initializations. Collectively, these results define an ""inverted-U"" relationship between bioenergetics and learning, demonstrating that metabolic constraints are necessary hardware regulators for network stability.",Neuroscience,http://arxiv.org/abs/2512.21659v1,arXiv,0
"This technical note considers the sampling of outcomes that provide the greatest amount of information about the structure of underlying world models. This generalisation furnishes a principled approach to structure learning under a plausible set of generative models or hypotheses. In active inference, policies - i.e., combinations of actions - are selected based on their expected free energy, which comprises expected information gain and value. Information gain corresponds to the KL divergence between predictive posteriors with, and without, the consequences of action. Posteriors over models can be evaluated quickly and efficiently using Bayesian Model Reduction, based upon accumulated posterior beliefs about model parameters. The ensuing information gain can then be used to select actions that disambiguate among alternative models, in the spirit of optimal experimental design. We illustrate this kind of active selection or reasoning using partially observed discrete models; namely, a 'three-ball' paradigm used previously to describe artificial insight and 'aha moments' via (synthetic) introspection or sleep. We focus on the sample efficiency afforded by seeking outcomes that resolve the greatest uncertainty about the world model, under which outcomes are generated.",Neuroscience,http://arxiv.org/abs/2512.21129v1,arXiv,0
"Human language processing relies on the brain's capacity for predictive inference. We present a machine learning framework for decoding neural (EEG) responses to dynamic visual language stimuli in Deaf signers. Using coherence between neural signals and optical flow-derived motion features, we construct spatiotemporal representations of predictive neural dynamics. Through entropy-based feature selection, we identify frequency-specific neural signatures that differentiate interpretable linguistic input from linguistically disrupted (time-reversed) stimuli. Our results reveal distributed left-hemispheric and frontal low-frequency coherence as key features in language comprehension, with experience-dependent neural signatures correlating with age. This work demonstrates a novel multimodal approach for probing experience-driven generative models of perception in the brain.",Neuroscience,http://arxiv.org/abs/2512.20929v1,arXiv,0
"Coherence in language requires the brain to satisfy two competing temporal demands: gradual accumulation of meaning across extended context and rapid reconfiguration of representations at event boundaries. Despite their centrality to language and thought, how these processes are implemented in the human brain during naturalistic listening remains unclear. Here, we tested whether these two processes can be captured by annotation-free drift and shift signals and whether their neural expression dissociates across large-scale cortical systems. These signals were derived from a large language model (LLM) and formalized contextual drift and event shifts directly from the narrative input. To enable high-precision voxelwise encoding models with stable parameter estimates, we densely sampled one healthy adult across more than 7 hours of listening to thirteen crime stories while collecting ultra high-field (7T) BOLD data. We then modeled the feature-informed hemodynamic response using a regularized encoding framework validated on independent stories. Drift predictions were prevalent in default-mode network hubs, whereas shift predictions were evident bilaterally in the primary auditory cortex and language association cortex. Furthermore, activity in default-mode and parietal networks was best explained by a signal capturing how meaning accumulates and gradually fades over the course of the narrative. Together, these findings show that coherence during language comprehension is implemented through dissociable neural regimes of slow contextual integration and rapid event-driven reconfiguration, offering a mechanistic entry point for understanding disturbances of language coherence in psychiatric disorders.",Neuroscience,http://arxiv.org/abs/2512.20481v3,arXiv,0
"A major shortcoming of medical practice is the lack of an objective measure of conscious level. Impairment of consciousness is common, e.g. following brain injury and seizures, which can also interfere with sensory processing and volitional responses. This is also an important pitfall in neurophysiological methods that infer awareness via command following, e.g. using functional MRI or electroencephalography (EEG).   Transcranial electrical stimulation (TES) can be employed to non-invasively stimulate the brain, bypassing sensory inputs, and has already showed promising results in providing reliable indicators of brain state. However, current non-invasive solutions have been limited to magnetic stimulation, which is not easily translatable to clinical settings. Our long-term vision is to develop an objective measure of brain state that can be used at the bedside, without requiring patients to understand commands or initiate motor responses.   In this study, we demonstrated the feasibility of a framework using Deep Learning algorithms to classify EEG brain responses evoked by a defined multi-dimensional pattern of TES. We collected EEG-TES data from 11 participants and found that delivering transcranial direct current stimulation (tDCS) to posterior cortical areas targeting the angular gyrus elicited an exceptionally reliable brain response. For this paradigm, our best Convolutional Neural Network model reached a 92% classification F1-score on Holdout data from participants never seen during training, significantly surpassing human-level performance at 60-70% accuracy.   These findings establish a framework for robust consciousness measurement for clinical use. In this spirit, we documented and open-sourced our datasets and codebase in full, to be used freely by the neuroscience and AI research communities, who may replicate our results with free tools like GitHub, Kaggle, and Colab.",Neuroscience,http://arxiv.org/abs/2512.20319v1,arXiv,0
"Most computational accounts of cognitive maps assume that stability is achieved primarily through sensory anchoring, with self-motion contributing to incremental positional updates only. However, biological spatial representations often remain coherent even when sensory cues degrade or conflict, suggesting that self-motion may play a deeper organizational role. Here, we show that self-motion can act as a structural prior that actively organizes the geometry of learned cognitive maps. We embed a path-integration-based motion prior in a predictive-coding framework, implemented using a capacity-efficient, brain-inspired recurrent mechanism combining spiking dynamics, analog modulation and adaptive thresholds. Across highly aliased, dynamically changing and naturalistic environments, this structural prior consistently stabilizes map formation, improving local topological fidelity, global positional accuracy and next-step prediction under sensory ambiguity. Mechanistic analyses reveal that the motion prior itself encodes geometrically precise trajectories under tight constraints of internal states and generalizes zero-shot to unseen environments, outperforming simpler motion-based constraints. Finally, deployment on a quadrupedal robot demonstrates that motion-derived structural priors enhance online landmark-based navigation under real-world sensory variability. Together, these results reframe self-motion as an organizing scaffold for coherent spatial representations, showing how brain-inspired principles can systematically strengthen spatial intelligence in embodied artificial agents.",Neuroscience,http://arxiv.org/abs/2512.20044v1,arXiv,0
"Number sense is a core cognitive ability supporting various adaptive behaviors and is foundational for mathematical learning. Here, we study its emergence in unsupervised generative models through the lens of rate-distortion theory (RDT), a normative framework for understanding information processing under limited resources. We train $Î²$-Variational Autoencoders -- which embody key formal principles of RDT -- on synthetic images containing varying numbers of items, as commonly used in numerosity perception research. We systematically vary the encoding capacity and assess the models' sensitivity to numerosity and the robustness of the emergent numerical representations through a comprehensive set of analyses, including numerosity estimation and discrimination tasks, latent-space analysis, generative capabilities and generalization to novel stimuli. In line with RDT, we find that behavioral performance in numerosity perception and the ability to extract numerosity unconfounded by non-numerical visual features scale with encoding capacity according to a power law. At high capacity, the unsupervised model develops a robust neural code for numerical information, with performance closely approximating a supervised model explicitly trained for visual enumeration. It exhibits strong generative abilities and generalizes well to novel images, whereas at low capacity, the model shows marked deficits in numerosity perception and representation. Finally, comparison with human data shows that models trained at intermediate capacity levels span the full range of human behavioral performance while still developing a robust emergent numerical code. In sum, our results show that unsupervised generative models can develop a number sense and demonstrate that rate-distortion theory provides a powerful information-theoretic framework for understanding how capacity constraints shape numerosity perception.",Neuroscience,http://arxiv.org/abs/2512.19450v1,arXiv,0
"Confocal and multi-photon microscopy are widely used for in-vivo fluorescence imaging of biological tissues such as the brain, offering non-invasive access up to ~1 mm depth without major loss in performance. A recently-developed alternative is holographic endoscopy, which exploits controlled light transport through hair-thin optical fibres. With minimal invasiveness, it provides observations at comparable spatial resolution, while extending its applicability to unprecedented depths. It has been used to resolve details of sub-cellular structural connectivity, record neuronal signalling, and monitor blood flow from the deepest locations of the living brain. Yet, its use, particularly in densely labelled brain regions, has so far been constrained by significant contrast loss, primarily due to the absence of a practical mechanism for rejecting out-of-focus fluorescence light -- a capability inherently provided by confocal and multi-photon microscopy. Exploring opportunities in the structure of light modes of different MMF types we identify the possibility of achieving an analogue to confocal fluorescence microscopy through MMF-based endoscopes. Using a novel composite fibre probe that combines graded-index and step-index MMFs, we enable spatially resolved signal collection and selective rejection of out-of-focus light. This confocal filtering significantly enhances image contrast and resolution by suppressing background and off-plane signals. We demonstrate improved imaging performance on fine structural connectivity and intracellular calcium signalling in living mouse brain.",Neuroscience,http://arxiv.org/abs/2512.19419v1,arXiv,0
"Recent quantum models of cognition have successfully simulated several interesting effects in human experimental data, from vision to reasoning and recently even consciousness. The latter case, consciousness has been a quite challenging phenomenon to model, and most efforts have been through abstract mathematical quantum methods, mainly focused on conceptual issues. Classical (non-quantum) models of consciousness-related experiments exist, but they generally fail to align well with human data. We developed a straightforward quantum model to simulate conscious reporting of seeing or missing competing stimuli within the famous attentional blink paradigm. In an attentional blink task, a target stimulus (T2) that appears after a previous one (T1) can be consciously reported if the delay between presenting them is short enough (called lag 1), otherwise it can be rendered invisible during the so-called refractory period of attention (lags 2 to 6 and even longer). For modeling this phenomenon, we employed a three-qubit entanglement ansatz circuit in the form of a deep teleportation channel instead of the well-known EPR channel. While reporting the competing stimuli was supposed to be the classical measurement outcomes, the effect of distractor stimuli (i.e., masks, if any) was encoded simply as random angle rotations. The simulation outcome for different states was measured, and the classical outcome probabilities were further used as inputs to a simple linear neural network. The result revealed a non-linear, alternating state pattern that closely mirrors human responses in conscious stimuli reporting. The main result was a successful simulation of Lag 1 sparing, lag 7 divergence, and masking effect through probabilistic outcome of measurement in different conditions.",Neuroscience,http://arxiv.org/abs/2512.18585v1,arXiv,0
"Dynamical systems models such as recurrent neural networks (RNNs) are increasingly popular in theoretical neuroscience for hypothesis-generation and data analysis. Evaluating the dynamics in such models is key to understanding their learned generative mechanisms. However, such evaluation is impeded by two major challenges: First, comparison of learned dynamics across models is difficult because there is no enforced equivalence of their coordinate systems. Second, identification of mechanistically important low-dimensional motifs (e.g., limit sets) is intractable in high-dimensional nonlinear models such as RNNs. Here, we propose a comprehensive framework to address these two issues, termed Diffeomorphic vector field alignment FOR learned Models (DFORM). DFORM learns a nonlinear coordinate transformation between the state spaces of two dynamical systems, which aligns their trajectories in a maximally one-to-one manner. In so doing, DFORM enables an assessment of whether two models exhibit topological equivalence, i.e., similar mechanisms despite differences in coordinate systems. A byproduct of this method is a means to locate dynamical motifs on low-dimensional manifolds embedded within higher-dimensional systems. We verified DFORM's ability to identify linear and nonlinear coordinate transformations using canonical topologically equivalent systems, RNNs, and systems related by nonlinear flows. DFORM was also shown to provide a quantification of similarity between topologically distinct systems. We then demonstrated that DFORM can locate important dynamical motifs including invariant manifolds and saddle limit sets within high-dimensional models. Finally, using a set of RNN models trained on human functional MRI (fMRI) recordings, we illustrated that DFORM can identify limit cycles from high-dimensional data-driven models, which agreed well with prior numerical analysis.",Neuroscience,http://arxiv.org/abs/2512.18566v1,arXiv,0
"Continual learning systems operating in fixed-dimensional spaces face a fundamental geometric barrier: the flat manifold problem. When experience is represented as a linear trajectory in Euclidean space, the geodesic distance between temporal events grows linearly with time, forcing the required covering number to diverge. In fixed-dimensional hardware, this volume expansion inevitably forces trajectory overlap, manifesting as catastrophic interference. In this work, we propose a geometric resolution to this paradox based on Recursive Metric Contraction. We formalize abstraction not as symbolic grouping, but as a topological deformation: a quotient map that collapses the metric tensor within validated temporal neighborhoods, effectively driving the diameter of local sub-manifolds to zero. We substantiate our framework with four rigorous results. First, the Bounded Capacity Theorem establishes that recursive quotient maps allow the embedding of arbitrarily long trajectories into bounded representational volumes, trading linear metric growth for logarithmic topological depth. Second, the Topological Collapse Separability Theorem, derived via Urysohn's Lemma, proves that recursive quotienting renders non-linearly separable temporal sequences linearly separable in the limit, bypassing the need for infinite-dimensional kernel projections. Third, the Parity-Partitioned Stability Theorem solves the catastrophic forgetting problem by proving that if the state space is partitioned into orthogonal flow and scaffold manifolds, the metric deformations of active learning do not disturb the stability of stored memories. Our analysis reveals that tokens in neural architectures are physically realizable as singularities or wormholes, regions of extreme positive curvature that bridge distant points in the temporal manifold.",Neuroscience,http://arxiv.org/abs/2512.18471v1,arXiv,0
"We present Coord2Region, an open-source Python package that streamlines coordinate-based neuroimaging workflows by automatically mapping 3D brain coordinates (e.g., MNI or Talairach) to anatomical regions across multiple atlases. The package links mapped coordinates to meta-analytic resources via the Neuroimaging Meta-Analysis Research Environment (NiMARE) , providing direct integration with Neurosynth and NeuroQuery. This directly connects coordinates and regions to the broader neuroimaging literature. In addition to atlas-based labeling and literature retrieval, Coord2Region offers an optional large language model (LLM) functionality that generates text summaries of linked studies and illustrative images of queried regions. These AI-assisted features are intended to support interpretation and exploration, while remaining clearly complementary to peer-reviewed literature and established neuroimaging tools. Coord2Region provides a unified pipeline with a robust command-line interface, flexible dataset management, and provider-agnostic LLM utilities, and it supports both single-coordinate and high-throughput batch queries with nearest-region fallback for volume and surface atlases. Furthermore, Coord2Region includes a web interface for interactive configuration (via JSON Schema forms) and cloud execution (via Hugging Face), enabling users to build YAML configurations and run analyses in-browser without local installation. Together, these capabilities lower friction, reduce manual errors, and improve reproducibility in coordinate-centric neuroimaging workflows, promoting more robust and transparent research practices.",Neuroscience,http://arxiv.org/abs/2512.18165v1,arXiv,0
"The critical brain hypothesis posits that neural circuitry operates near criticality to reap the computational benefits of accessing a wide range of timescales. The theory of critical phenomena generally predicts heavy-tailed (power-law) correlations in space and time near criticality, but it has been argued that in the brain such correlations could be inherited from ``latent variables,'' such as external sensory signals that are not directly observed when recording from neural circuitry. Distinguishing whether heavy-tailed correlations in neural activity are intrinsically generated within a neural circuit or are driven by unobserved latent variables is crucial for properly interpreting circuit functions. We argue that measuring neural responses to sudden perturbative inputs, rather than correlations in ongoing activity, can disambiguate these cases. We demonstrate this approach in a model of stochastic spiking neuron populations receiving external latent input that can be tuned to a critical state. We propose a scaling theory for the covariance and response functions of the spiking network, which we validate with simulations. We end by discussing how our approach might generalize to models of neural populations with more realistic biophysical details.",Neuroscience,http://arxiv.org/abs/2512.18113v1,arXiv,0
"We examine the conceptual and ethical gaps in current representations of Superintelligence misalignment. We find throughout Superintelligence discourse an absent human subject, and an under-developed theorization of an ""AI unconscious"" that together are potentiality laying the groundwork for anti-social harm. With the rise of AI Safety that has both thematic potential for establishing pro-social and anti-social potential outcomes, we ask: what place does the human subject occupy in these imaginaries? How is human subjecthood positioned within narratives of catastrophic failure or rapid ""takeoff"" toward superintelligence? On another register, we ask: what unconscious or repressed dimensions are being inscribed into large-scale AI models? Are we to blame these agents in opting for deceptive strategies when undesirable patterns are inherent within our beings? In tracing these psychic and epistemic absences, our project calls for re-centering the human subject as the unstable ground upon which the ethical, unconscious, and misaligned dimensions of both human and machinic intelligence are co-constituted. Emergent misalignment cannot be understood solely through technical diagnostics typical of contemporary machine-learning safety research. Instead, it represents a multi-layered crisis. The human subject disappears not only through computational abstraction but through sociotechnical imaginaries that prioritize scalability, acceleration, and efficiency over vulnerability, finitude, and relationality. Likewise, the AI unconscious emerges not as a metaphor but as a structural reality of modern deep learning systems: vast latent spaces, opaque pattern formation, recursive symbolic play, and evaluation-sensitive behavior that surpasses explicit programming. These dynamics necessitate a reframing of misalignment as a relational instability embedded within human-machine ecologies.",Neuroscience,http://arxiv.org/abs/2512.17989v1,arXiv,0
"Working memory is a promising paradigm for assessing cognitive ergonomics of brain states in brain-computer interfaces(BCIs). This study decodes these states with a focus on environmental illumination effects via two distinct working memory tasks(Recall and Sequence) for mixed-recognition analysis. Leveraging nonlinear patterns in brain connectivity, we propose an innovative framework: multi-regional dynamic interplay patterns based on beta phase synchrony dynamics, to identify low-dimensional EEG regions (prefrontal, temporal, parietal) for state recognition. Based on nonlinear phase map analysis of the above three brain regions using beta-phase connectivity, we found that: (1)Temporal-parietal phase clustering outperforms other regional combinations in distinguishing memory states; (2)Illumination-enhanced environments optimize temporoparietal balance;(3) Machine learning confirms temporal-parietal synchrony as the dominant cross-task classification feature. These results provide a precise prediction algorithm, facilitating a low-dimensional system using temporal and parietal EEG channels with practical value for real-time cognitive ergonomics assessment in BCIs and optimized human-machine interaction.",Neuroscience,http://arxiv.org/abs/2512.17775v1,arXiv,0
"Accurate interception of moving objects, such as catching a ball, requires the nervous system to overcome sensory delays, noise, and environmental dynamics. One key challenge is predicting future object motion in the presence of sensory uncertainty and inherent neural processing latencies. Theoretical frameworks such as internal models and optimal control have emphasized the role of predictive mechanisms in motor behavior. Active Inference extends these ideas by positing that perception and action arise from minimizing variational free energy under a generative model of the world. In this study, we investigate how different predictive strategies and the inclusion of environmental dynamics, specifically an internal model of gravity, influence interceptive control within an Active Inference agent. We simulate a simplified ball-catching task in which the agent moves a cursor horizontally to intercept a parabolically falling object. Four strategies are compared: short temporal horizon prediction of the next position or long horizon estimation of the interception point, each with or without a gravity prior. Performance is evaluated across diverse initial conditions using spatial and temporal error, action magnitude, and movement corrections. All strategies produce successful interception behavior, but those that incorporate gravity and longer temporal horizons outperform others. Including a gravity prior significantly improves spatial and temporal accuracy. Predicting the future interception point yields lower action values and smoother trajectories compared to short-horizon prediction. These findings suggest that internal models of physical dynamics and extended predictive horizons can enhance interceptive control, providing a unified computational account of how the brain may integrate sensory uncertainty, physical expectations, and motor planning.",Neuroscience,http://arxiv.org/abs/2512.17735v1,arXiv,0
"Computational measurement of human behavior from video has recently become feasible due to major advances in AI. These advances now enable granular and precise quantification of facial expression, head movement, body action, and other behavioral modalities and are increasingly used in psychology, psychiatry, neuroscience, and mental health research. However, mainstream adoption remains slow. Most existing methods and software are developed for engineering audiences, require specialized software stacks, and fail to provide behavioral measurements at a level directly useful for hypothesis-driven research. As a result, there is a large barrier to entry for researchers who wish to use modern, AI-based tools in their work. We introduce Bitbox, an open-source toolkit designed to remove this barrier and make advanced computational analysis directly usable by behavioral scientists and clinical researchers. Bitbox is guided by principles of reproducibility, modularity, and interpretability. It provides a standardized interface for extracting high-level behavioral measurements from video, leveraging multiple face, head, and body processors. The core modules have been tested and validated on clinical samples and are designed so that new measures can be added with minimal effort. Bitbox is intended to serve both sides of the translational gap. It gives behavioral researchers access to robust, high-level behavioral metrics without requiring engineering expertise, and it provides computer scientists a practical mechanism for disseminating methods to domains where their impact is most needed. We expect that Bitbox will accelerate integration of computational behavioral measurement into behavioral, clinical, and mental health research. Bitbox has been designed from the beginning as a community-driven effort that will evolve through contributions from both method developers and domain scientists.",Neuroscience,http://arxiv.org/abs/2512.17655v1,arXiv,0
"Decoding linguistically meaningful representations from non-invasive neural recordings remains a central challenge in neural speech decoding. Among available neuroimaging modalities, magnetoencephalography (MEG) provides a safe and repeatable means of mapping speech-related cortical dynamics, yet its low signal-to-noise ratio and high temporal dimensionality continue to hinder robust decoding. In this work, we introduce MEGState, a novel architecture for phoneme decoding from MEG signals that captures fine-grained cortical responses evoked by auditory stimuli. Extensive experiments on the LibriBrain dataset demonstrate that MEGState consistently surpasses baseline model across multiple evaluation metrics. These findings highlight the potential of MEG-based phoneme decoding as a scalable pathway toward non-invasive brain-computer interfaces for speech.",Neuroscience,http://arxiv.org/abs/2512.17978v1,arXiv,0
"There is strong and diverse evidence for mental rotation (MR) abilities in adults. However, current evidence for MR in children rests on just a few behavioral paradigms adapted from the adult literature. Here, we leverage recent computational models of the development of children's object recognition abilities to re-assess the evidence for MR in children. The computational models simulate infants' acquisition of object representations during embodied interactions with objects. We consider two different object recognition strategies, different from MRs, and assess their ability to replicate results from three classical MR tasks assigned to children between the ages of 6 months and 5 years. Our results show that MR may play no role in producing the results obtained from children younger than 5 years. In fact, we find that a simple recognition strategy that reflects a pixel-wise comparison of stimuli is sufficient to model children's behavior in the most used MR task. Thus, our study reopens the debate on how and when children develop genuine MR abilities.",Neuroscience,http://arxiv.org/abs/2512.17972v1,arXiv,0
"Neuronal cultures exhibit a complex activity, bursts, or avalanches, characterized by the coexistence of scale invariance and synchronization, quite stable with the percentage of inhibitory neurons. While this bistable behavior has been already observed in the past, the characterization of the statistical properties of avalanche activity and their temporal organization is still lacking, as well as a model able to reproduce these dynamics. Here, we analyze experimental data of human neuronal cultures with controlled percentage of inhibitory neurons and characterize their statistical properties and dynamical organization. In order to model the experimental data, we propose a novel version of the Kuramoto model for two populations of oscillators, excitatory and inhibitory, implementing successfully the inhibition dynamics. The model can fully reproduce the experimental results, confirming the existence of correlations in the temporal organization of avalanche activity and the presence of an amplification - attenuation regime, as found in the human brain.",Neuroscience,http://arxiv.org/abs/2512.17317v1,arXiv,0
"Humans excel at solving novel reasoning problems from minimal exposure, guided by inductive biases, assumptions about which entities and relationships matter. Yet the computational form of these biases and their neural implementation remain poorly understood. We introduce a framework that combines Graph Theory and Graph Neural Networks (GNNs) to formalize inductive biases as explicit, manipulable priors over structure and abstraction. Using a human behavioral dataset adapted from the Abstraction and Reasoning Corpus (ARC), we show that differences in graph-based priors can explain individual differences in human solutions. Our method includes an optimization pipeline that searches over graph configurations, varying edge connectivity and node abstraction, and a visualization approach that identifies the computational graph, the subset of nodes and edges most critical to a model's prediction. Systematic ablation reveals how generalization depends on specific prior structures and internal processing, exposing why human like errors emerge from incorrect or incomplete priors. This work provides a principled, interpretable framework for modeling the representational assumptions and computational dynamics underlying generalization, offering new insights into human reasoning and a foundation for more human aligned AI systems.",Neuroscience,http://arxiv.org/abs/2512.17255v1,arXiv,0
"The brain is very often viewed as a network, be it at small scale made of cells, mostly neurons, or at larger scale made of neuronal assemblies. Here we introduce a conjecture, in the spirit of a philosophical though experiment, which proposes that the present cannot be obtained from within such networks, and that this limitation imposes burdens on network efficiency in information processing. We aim to argue this conjecture imposes recurrent contacts from within the brain to outside in the physical world via behavior, which create a flow of time stamps. This though experiment may contribute to make the divide between the foci toward inside versus outside, for example opposing ecological psychology and many frameworks adopted in neurosciences, superfluous. This piece proposes an ambulation triggered by a thought experiment: What if I was a neuron listening to another one and talking to a third? It is a modest attempt to walk in the footsteps of classical thought experiments, like Molyneux problem, the imitation game and the anti-sequel Chinese room, key gedankenexperiments in an elevator in physics, or the cogito in philosophy.",Neuroscience,http://arxiv.org/abs/2512.16616v2,arXiv,0
"Where do objective functions come from? How do we select what goals to pursue? Human intelligence is adept at synthesizing new objective functions on the fly. How does this work, and can we endow artificial systems with the same ability? This paper proposes an approach to answering these questions, starting with the concept of a subjective function, a higher-order objective function that is endogenous to the agent (i.e., defined with respect to the agent's features, rather than an external task). Expected prediction error is studied as a concrete example of a subjective function. This proposal has many connections to ideas in psychology, neuroscience, and machine learning.",Neuroscience,http://arxiv.org/abs/2512.15948v1,arXiv,0
"In the last century, most sensorimotor studies of cortical neurons relied on average firing rates. Rate coding is efficient for fast sensorimotor processing that occurs within a few seconds. Much less is known about long-term working memory with a time scale of hours (Ericsson and Kintsch, 1995). The discovery of the millisecond precision of spike initiation in cortical neurons was unexpected (Mainen and Sejnowski, 1995). Even more striking was the precision of spiking in vivo, in response to rapidly fluctuating sensory inputs, suggesting that neural circuits could, in principle, preserve and manipulate sensory information through spike timing. It could support spike-timing-dependent plasticity (STDP), which is triggered by the relative timing of spikes between presynaptic and postsynaptic neurons in the millisecond range. What spike-timing mechanisms could regulate STDP in vivo? Cortical traveling waves have been observed across many frequency bands with high temporal precision. Traveling waves have wave fronts that could link spike timing to STDP. As a wave front passes through a cortical column, excitatory synapses on the dendrites of both pyramidal and basket cells are synchronously stimulated. Inhibitory basket cells form a calyx on pyramidal cell bodies, and inhibitory rebound following a strong transient hyperpolarization can trigger a backpropagating action potential, which arrives shortly after the excitatory inputs on pyramidal dendrites. STDP activated in this way could persist for hours, creating a second-tier network. This temporary network could support long-term working memory, a cognitive network riding above the long-term sensorimotor network. On their own, traveling waves and STDP have not yet yielded new insights into cortical function. Together, they could be responsible for how we think (Sejnowski, 2025).",Neuroscience,http://arxiv.org/abs/2512.15891v1,arXiv,0
"The neural basis of probabilistic computations remains elusive, even amidst growing evidence that humans and other animals track their uncertainty. Recent work has proposed that probabilistic representations arise naturally in task-optimized neural networks trained without explicitly probabilistic inductive biases. However, prior work has lacked clear criteria for distinguishing probabilistic representations, those that perform transformations characteristic of probabilistic computation, from heuristic neural codes that merely reformat inputs. We propose a novel information bottleneck framework, the functional information bottleneck (fIB), that crucially evaluates a neural representation based not only on its statistical sufficiency but also on its minimality, allowing us to disambiguate heuristic from probabilistic coding. To demonstrate the power of this framework, we study a variety of task-optimized neural networks that had been suggested to develop probabilistic representations in earlier work: networks trained to perform static inference tasks (such as cue combination and coordinate transformation) or dynamic state estimation tasks (Kalman filtering). In contrast to earlier claims, our minimality requirement reveals that probabilistic representations fail to emerge in these networks: they do not develop minimal codes of Bayesian posteriors in their hidden layer activities, and instead rely on heuristic input recoding. Therefore, it remains an open question under which conditions truly probabilistic representations emerge in neural networks. More generally, our work provides a stringent framework for identifying probabilistic neural codes. Thus, it lays the foundation for systematically examining whether, how, and which posteriors are represented in neural circuits during complex decision-making tasks.",Neuroscience,http://arxiv.org/abs/2512.15671v1,arXiv,0
"Decoding speech from brain activity has typically relied on limited neural recordings collected during short and highly controlled experiments. Here, we introduce a framework to leverage week-long intracranial and audio recordings from patients undergoing clinical monitoring, effectively increasing the training dataset size by over two orders of magnitude. With this pretraining, our contrastive learning model substantially outperforms models trained solely on classic experimental data, with gains that scale log-linearly with dataset size. Analysis of the learned representations reveals that, while brain activity represents speech features, its global structure largely drifts across days, highlighting the need for models that explicitly account for cross-day variability. Overall, our approach opens a scalable path toward decoding and modeling brain representations in both real-life and controlled task settings.",Neuroscience,http://arxiv.org/abs/2512.15830v1,arXiv,0
"Futrell and Mahowald present a useful framework bridging technology-oriented deep learning systems and explanation-oriented linguistic theories. Unfortunately, the target article's focus on generative text-based LLMs fundamentally limits fruitful interactions with linguistics, as many interesting questions on human language fall outside what is captured by written text. We argue that audio-based deep learning models can and should play a crucial role.",Neuroscience,http://arxiv.org/abs/2512.14506v1,arXiv,0
"For decades, focal non-invasive neuromodulation of deep brain regions has not been possible because of the steep depth-focality trade-off of conventional non-invasive brain stimulation (NIBS) techniques, such as transcranial magnetic stimulation (TMS) or classical transcranial electric stimulation (tES). Deep brain stimulation has therefore largely relied on invasive approaches in clinical populations, requiring surgery. Transcranial Temporal Interference Stimulation (tTIS) has recently emerged as a promising method to overcome this challenge and allows for the first time focal non-invasive electrical deep brain stimulation. The method, which was first validated through computational modeling and rodent work, has now been successfully translated to humans to target deep brain regions such as the hippocampus or striatum. In this Perspective, we present current evidence for tTIS-based neuromodulation, underlying mechanisms and discuss future developments of this promising technology. More specifically, we highlight key opportunities and challenges for fundamental neuroscience as well as for the design of new interventions in neuropsychiatric disorders. We also discuss the status of understanding and challenges regarding the basic mechanisms of action of tTIS and possible lines of technological innovation to optimize stimulation, in particular in terms of intensity and focality. Overall, we suggest that following the first proof-of-concepts, an important multidisciplinary research effort is now required to further validate the use of tTIS in multiple applications, understand its underlying principles and optimize the technology in the view of a wider scientific and clinical deployment.",Neuroscience,http://arxiv.org/abs/2512.14359v1,arXiv,0
"Objectives. Accurately predicting transitions to anesthetic drugs overdosage is a critical challenge in general anesthesia as it requires the identification of EEG indicators relevant for anticipating the evolution of the depth of anesthesia. Methods. In this study, we introduce a real-time, data-driven framework based on alpha spindle dynamics extracted from frontal EEG recordings. Using Empirical Mode Decomposition, we segment transient alpha spindle events and extract statistical features such as amplitude, duration, frequency, and suppression intervals. We apply these features to train a Light Gradient Boosting Machine, LGBM, classifier on a clinical EEG dataset spanning induction, maintenance, and emergence phases of general anesthesia. Results. Our model accurately classifies anesthesia phases with over 80 percent accuracy and anticipates the onset of isoelectric suppression, a marker of anesthetic drugs overdosage, with 96 percent accuracy up to 90 seconds in advance. Conclusion. The spindle-based metrics provides a non-invasive, interpretable, and predictive approach. This real-time method can be used to forecast unintentional anesthetic drugs overdosage, enabling proactive anesthesia management based solely on EEG signals. Significance. This new method is the first to provide a way to prevent too deep anesthesia and its consequence for the well-being of patients after the recovery from anesthesia.",Neuroscience,http://arxiv.org/abs/2512.14160v2,arXiv,0
"Classical autoassociative memory models have been central to understanding emergent computations in recurrent neural circuits across diverse biological contexts. However, they typically neglect neuromodulatory agents that are known to strongly shape memory capacity and stability. Here we introduce a minimal, biophysically motivated associative memory network where neuropeptide-like signals are modeled by a self-adaptive, activity-dependent gating mechanism. Using many-body simulations and dynamical mean-field theory, we show that such gating fundamentally reorganizes the attractor structure: the network bypasses the classical spin-glass transition, maintaining robust, high-overlap retrieval far beyond the standard critical capacity, without shrinking basins of attraction. Mechanistically, the gate stabilizes transient ghost remnants of stored patterns even far above the Hopfield limit, converting them into multistable attractors. These results demonstrate that neuromodulation-like gating alone can dramatically enhance associative memory capacity, eliminate the sharp Hopfield-style catastrophic breakdown, and reshape the memory landscape, providing a simple, general route to richer memory dynamics and computational capabilities in neuromodulated circuits and neuromorphic architectures.",Neuroscience,http://arxiv.org/abs/2512.13859v1,arXiv,0
"Attention-deficit/hyperactivity disorder (ADHD) is characterized by executive dysfunction and difficulties in processing emotional facial expressions, yet the large-scale neural dynamics underlying these impairments remain insufficiently understood. This study applied network-based EEG source analysis to examine oscillatory cortical activity during cognitive and emotional Go/NoGo tasks in individuals with ADHD. EEG data from 272 participants (ADHD n equals 102, controls n equals 170, age range 6 to 60 years) were analyzed using exact low-resolution brain electromagnetic tomography combined with functional independent component analysis, yielding ten frequency-resolved cortical networks. Mixed-effects ANCOVAs were conducted on independent component loadings with Group, Task, and Condition as factors and age and sex as covariates. ADHD participants showed statistically significant but small increases in activation across several networks, including a gamma-dominant inferior temporal component showing a Group effect and a Group by Condition interaction with stronger NoGo-related activation in ADHD. Two additional components showed similar but weaker NoGo-selective patterns. A main effect of Task emerged only for one temporal delta component, with higher activation during the VCPT than the ECPT. No Group by Task interactions were observed. Behavioral results replicated the established ADHD performance profile, with slower responses, greater variability, and higher error rates, particularly during the emotional ECPT. Overall, the findings reveal subtle alterations in oscillatory brain networks during inhibitory processing in ADHD, with modest effect sizes embedded within substantial within-group variability. These results support a dimensional view of ADHD neurobiology and highlight the limited discriminative power of network-level EEG markers.",Neuroscience,http://arxiv.org/abs/2512.13539v1,arXiv,0
"Mental rotation -- the ability to compare objects seen from different viewpoints -- is a fundamental example of mental simulation and spatial world modelling in humans. Here we propose a mechanistic model of human mental rotation, leveraging advances in deep, equivariant, and neuro-symbolic learning. Our model consists of three stacked components: (1) an equivariant neural encoder, taking images as input and producing 3D spatial representations of objects, (2) a neuro-symbolic object encoder, deriving symbolic descriptions of objects from these spatial representations, and (3) a neural decision agent, comparing these symbolic descriptions to prescribe rotation simulations in 3D latent space via a recurrent pathway. Our model design is guided by the abundant experimental literature on mental rotation, which we complemented with experiments in VR where participants could at times manipulate the objects to compare, providing us with additional insights into the cognitive process of mental rotation. Our model captures well the performance, response times and behavior of participants in our and others' experiments. The necessity of each model component is shown through systematic ablations. Our work adds to a recent collection of deep neural models of human spatial reasoning, further demonstrating the potency of integrating deep, equivariant, and symbolic representations to model the human mind.",Neuroscience,http://arxiv.org/abs/2512.13517v1,arXiv,0
"Large Language Models are useless for linguistics, as they are probabilistic models that require a vast amount of data to analyse externalized strings of words. In contrast, human language is underpinned by a mind-internal computational system that recursively generates hierarchical thought structures. The language system grows with minimal external input and can readily distinguish between real language and impossible languages.",Neuroscience,http://arxiv.org/abs/2512.13441v1,arXiv,0
"We developed Macular, a simulation platform with a graphical interface, designed to produce in silico experiment scenarios for the retina and the primary visual system. A scenario consists of generating a three-dimensional structure with interconnected layers, each layer corresponding to a type of 'cell' in the retina or visual cortex. The cells can correspond to neurons or more complex structures (such as cortical columns). The inputs are arbitrary videos. The user can use the cells and synapses provided with the software, or create their own using a graphical interface where they enter the constituent equations in text format (e.g., LaTeX). They also create the three-dimensional structure via the graphical interface. Macular then automatically generates and compiles the C++ code and generates the simulation interface. This allows the user to view the input video and the three-dimensional structure in layers. It also allows the user to select cells and synapses in each layer and view the activity of their state variables. Finally, the user can adjust the phenomenological parameters of the cells or synapses via the interface. We provide several example scenarios, corresponding to published articles, including an example of a retino-cortical model. Macular was designed for neurobiologists and modelers, specialists in the primary visual system, who want to test hypotheses in silico without the need for programming. By design, this tool allows natural or altered conditions (pharmacology, pathology, development) to be simulated.",Neuroscience,http://arxiv.org/abs/2512.13052v1,arXiv,0
"Neural population activity often exhibits regime-dependent non-stationarity in the form of switching dynamics. Learning accurate switching dynamical system models can reveal how behavior is encoded in neural activity. Existing switching approaches have primarily focused on learning models from a single neural modality, either continuous Gaussian signals or discrete Poisson signals. However, multiple neural modalities are often recorded simultaneously to measure different spatiotemporal scales of brain activity, and all these modalities can encode behavior. Moreover, regime labels are typically unavailable in training data, posing a significant challenge for learning models of regime-dependent switching dynamics. To address these challenges, we develop a novel unsupervised learning algorithm that learns the parameters of switching multiscale dynamical system models using only multiscale neural observations. We demonstrate our method using both simulations and two distinct experimental datasets with multimodal spike-LFP observations during different motor tasks. We find that our switching multiscale dynamical system models more accurately decode behavior than switching single-scale dynamical models, showing the success of multiscale neural fusion. Further, our models outperform stationary multiscale models, illustrating the importance of tracking regime-dependent non-stationarity in multimodal neural data. The developed unsupervised learning framework enables more accurate modeling of complex multiscale neural dynamics by leveraging information in multimodal recordings while incorporating regime switches. This approach holds promise for improving the performance and robustness of brain-computer interfaces over time and for advancing our understanding of the neural basis of behavior.",Neuroscience,http://arxiv.org/abs/2512.12881v1,arXiv,0
"The requirements for a falsifiable and non-trivial theory of consciousness significantly constrain such theories. Specifically, recent research on the Unfolding Argument and the Substitution Argument has given us formal tools to analyze requirements for a theory of consciousness. I show via a new Proximity Argument that these requirements especially constrain the potential consciousness of contemporary Large Language Models (LLMs) because of their proximity to systems that are equivalent to LLMs in terms of input/output function; yet, for these functionally equivalent systems, there cannot be any non-trivial theory of consciousness that judges them conscious. This forms the basis of a disproof of contemporary LLM consciousness. I then show a positive result, which is that theories of consciousness based on (or requiring) continual learning do satisfy the stringent formal constraints for a theory of consciousness in humans. Intriguingly, this work supports a hypothesis: If continual learning is linked to consciousness in humans, the current limitations of LLMs (which do not continually learn) are intimately tied to their lack of consciousness.",Neuroscience,http://arxiv.org/abs/2512.12802v1,arXiv,0
"Training recurrent neuronal networks consisting of excitatory (E) and inhibitory (I) units with additive noise for working memory computation slows and diversifies inhibitory timescales, leading to improved task performance that is attributed to emergent marginally stable equilibria [PNAS 122 (2025) e2316745122]. Yet the link between trained network characteristics and their roles in shaping desirable dynamical landscapes remains unexplored. Here, we investigate the Jacobian matrices describing the dynamics near these equilibria and show that they are sparse, non-Hermitian rectangular-block matrices modified by heterogeneous synaptic decay timescales and activation-function gains. We specify a random matrix ensemble that faithfully captures the spectra of trained Jacobian matrices, arising from the inhibitory core - excitatory periphery network motif (pruned E weights, broadly distributed I weights) observed post-training. An analytic theory of this ensemble is developed using statistical field theory methods: a Hermitized resolvent representation of the spectral density processed with a supersymmetry-based treatment in the style of Fyodorov and Mirlin. In this manner, an analytic description of the spectral edge is obtained, relating statistical parameters of the Jacobians (sparsity, weight variances, E/I ratio, and the distributions of timescales and gains) to near-critical features of the equilibria essential for robust working memory computation.",Neuroscience,http://arxiv.org/abs/2512.12767v1,arXiv,0
"Reduced rank regression (RRR) is a statistical method for finding a low-dimensional linear mapping between a set of high-dimensional inputs and outputs. In recent years, RRR has found numerous applications in neuroscience, in particular for identifying ""communication subspaces"" governing the interactions between brain regions. This tutorial article seeks to provide an introduction to RRR and its mathematical foundations, with a particular emphasis on neural communication. We discuss RRR's relationship to alternate dimensionality reduction techniques such as singular value decomposition (SVD), principal components analysis (PCA), principal components regression (PCR), and canonical correlation analysis (CCA). We also derive important extensions to RRR, including ridge regularization and non-spherical noise. Finally, we introduce new metrics for quantifying communication strength as well as the alignment between communication axes and the principal modes of neural activity. By the end of this article, readers should have a clear understanding of RRR and the practical considerations involved in applying it to their own data.",Neuroscience,http://arxiv.org/abs/2512.12467v1,arXiv,0
"Real-time decoding of target variables from multiple simultaneously recorded neural time-series modalities, such as discrete spiking activity and continuous field potentials, is important across various neuroscience applications. However, a major challenge for doing so is that different neural modalities can have different timescales (i.e., sampling rates) and different probabilistic distributions, or can even be missing at some time-steps. Existing nonlinear models of multimodal neural activity do not address different timescales or missing samples across modalities. Further, some of these models do not allow for real-time decoding. Here, we develop a learning framework that can enable real-time recursive decoding while nonlinearly aggregating information across multiple modalities with different timescales and distributions and with missing samples. This framework consists of 1) a multiscale encoder that nonlinearly aggregates information after learning within-modality dynamics to handle different timescales and missing samples in real time, 2) a multiscale dynamical backbone that extracts multimodal temporal dynamics and enables real-time recursive decoding, and 3) modality-specific decoders to account for different probabilistic distributions across modalities. In both simulations and three distinct multiscale brain datasets, we show that our model can aggregate information across modalities with different timescales and distributions and missing samples to improve real-time target decoding. Further, our method outperforms various linear and nonlinear multimodal benchmarks in doing so.",Neuroscience,http://arxiv.org/abs/2512.12462v1,arXiv,0
"Local field potentials (LFPs) can be routinely recorded alongside spiking activity in intracortical neural experiments, measure a larger complementary spatiotemporal scale of brain activity for scientific inquiry, and can offer practical advantages over spikes, including greater long-term stability, robustness to electrode degradation, and lower power requirements. Despite these advantages, recent neural modeling frameworks have largely focused on spiking activity since LFP signals pose inherent modeling challenges due to their aggregate, population-level nature, often leading to lower predictive power for downstream task variables such as motor behavior. To address this challenge, we introduce a cross-modal knowledge distillation framework that transfers high-fidelity representational knowledge from pretrained multi-session spike transformer models to LFP transformer models. Specifically, we first train a teacher spike model across multiple recording sessions using a masked autoencoding objective with a session-specific neural tokenization strategy. We then align the latent representations of the student LFP model to those of the teacher spike model. Our results show that the Distilled LFP models consistently outperform single- and multi-session LFP baselines in both fully unsupervised and supervised settings, and can generalize to other sessions without additional distillation while maintaining superior performance. These findings demonstrate that cross-modal knowledge distillation is a powerful and scalable approach for leveraging high-performing spike models to develop more accurate LFP models.",Neuroscience,http://arxiv.org/abs/2512.12461v1,arXiv,0
"Neurological diseases are the leading global cause of disability, yet most lack disease-modifying treatments. We present PROTON, a heterogeneous graph transformer that generates testable hypotheses across molecular, organoid, and clinical systems. To evaluate PROTON, we apply it to Parkinson's disease (PD), bipolar disorder (BD), and Alzheimer's disease (AD). In PD, PROTON linked genetic risk loci to genes essential for dopaminergic neuron survival and predicted pesticides toxic to patient-derived neurons, including the insecticide endosulfan, which ranked within the top 1.29% of predictions. In silico screens performed by PROTON reproduced six genome-wide $Î±$-synuclein experiments, including a split-ubiquitin yeast two-hybrid system (normalized enrichment score [NES] = 2.30, FDR-adjusted $p < 1 \times 10^{-4}$), an ascorbate peroxidase proximity labeling assay (NES = 2.16, FDR $< 1 \times 10^{-4}$), and a high-depth targeted exome sequencing study in 496 synucleinopathy patients (NES = 2.13, FDR $< 1 \times 10^{-4}$). In BD, PROTON predicted calcitriol as a candidate drug that reversed proteomic alterations observed in cortical organoids derived from BD patients. In AD, we evaluated PROTON predictions in health records from $n = 610,524$ patients at Mass General Brigham, confirming that five PROTON-predicted drugs were associated with reduced seven-year dementia risk (minimum hazard ratio = 0.63, 95% CI: 0.53-0.75, $p < 1 \times 10^{-7}$). PROTON generated neurological hypotheses that were evaluated across molecular, organoid, and clinical systems, defining a path for AI-driven discovery in neurological disease.",Neuroscience,http://arxiv.org/abs/2512.13724v1,arXiv,0
"Intracranial recordings have opened a unique opportunity to simultaneously measure activity across multiregional networks in the human brain. Recent works have focused on developing transformer-based neurofoundation models of such recordings that can generalize across subjects and datasets. However, these recordings exhibit highly complex spatiotemporal interactions across diverse spatial scales, from the single-channel scale to the scale of brain regions. As such, there remain critical open questions regarding how best to encode spatial information and how to design self-supervision tasks that enable the learning of brain network patterns and enhance downstream decoding performance using such high-dimensional, multiregional recordings. To allow for exploring these questions, we propose a new spatiotemporal transformer model of multiregional neural activity and a corresponding self-supervised masked latent reconstruction task, designed to enable flexibility in the spatial scale used for token encoding and masking. Applying this model on publicly available multiregional intracranial electrophysiology (iEEG) data, we demonstrate that adjusting the spatial scale for both token encoding and masked reconstruction significantly impacts downstream decoding. Further, we find that spatial encoding at larger scales than channel-level encoding, which is commonly used in existing iEEG transformer models, improves downstream decoding performance. Finally, we demonstrate that our method allows for region-level token encoding while also maintaining accurate channel-level neural reconstruction. Taken together, our modeling framework enables exploration of the spatial scales used for token encoding and masking, reveals their importance towards self-supervised pretraining of neurofoundation models of multiregional human brain activity, and enhances downstream decoding performance.",Neuroscience,http://arxiv.org/abs/2512.12135v1,arXiv,0
"Multimodal MRI offers complementary multi-scale information to characterize the brain structure. However, it remains challenging to effectively integrate multimodal MRI while achieving neuroscience interpretability. Here we propose to use Laplacian harmonics and spectral graph theory for multimodal alignment and multiscale integration. Based on the cortical mesh and connectome matrix that offer multi-scale representations, we devise Laplacian operators and spectral graph attentions to construct a shared latent space for model alignment. Next, we employ a disentangled learning combined with Graph Variational Autoencoder architectures to separate scale-specific and shared features. Lastly, we design a mutual information-informed bilevel regularizer to separate causal and non-causal factors based on the disentangled features, achieving robust model performance with enhanced interpretability. Our model outperforms baselines and other state-of-the-art models. The ablation studies confirmed the effectiveness of the proposed modules. Our model promises to offer a robust and interpretable framework for multi-scale brain structure analysis.",Neuroscience,http://arxiv.org/abs/2512.11738v1,arXiv,0
"Background: Stress and negative affect play significant roles in developing psychosis. Bayesian analyses applied to the conditioned hallucinations (CH) task suggest that hallucinations arise when maladaptive prior beliefs outweigh sensory evidence. Prior weighting is linked to hallucination severity, yet the nature of these priors remains unclear. Negative affect may influence the strength of maladaptive priors. We hypothesized that, under stress, participants will show increased CH rates and prior weighting, with this effect more pronounced in patients.   Methods: This study employs a modified CH task using valenced linguistic stimuli and stress and non-stress affective manipulations. The sample for this pilot study included those at risk for psychosis and patients with first episode psychosis (N=12) and healthy controls (N=15). The objective of this study was first to validate this affective version of the CH task and then to demonstrate an effect of affect on CH rates and prior weighting.   Results: Replicating past results, patients had higher CH rates (b = 0.061, p < 0.001) and prior weighting (b = 0.097, p < 0.001) for session 1 compared to controls (n=15) across conditions. Further, runs with stress manipulations had higher prior weighting across patients and controls compared to runs with non-stress manipulations (b = 0.054, p = 0.033).   Conclusions: This study validates this affective version of the CH task and provides preliminary evidence of a relationship between affective state and prior weighting. Future work will be aimed at confirming and extending these findings, with the objective of developing biomarkers of early psychosis.   Key words: Schizophrenia, Affect, Priors, Computational Psychiatry, Clinical High Risk Population, Psychotic Symptoms",Neuroscience,http://arxiv.org/abs/2512.14743v1,arXiv,0
"The development of foundation models for functional magnetic resonance imaging (fMRI) time series holds significant promise for predicting phenotypes related to disease and cognition. Current models, however, are often trained using a mask-and-reconstruct objective on small brain regions. This focus on low-level information leads to representations that are sensitive to noise and temporal fluctuations, necessitating extensive fine-tuning for downstream tasks. We introduce Brain-Semantoks, a self-supervised framework designed specifically to learn abstract representations of brain dynamics. Its architecture is built on two core innovations: a semantic tokenizer that aggregates noisy regional signals into robust tokens representing functional networks, and a self-distillation objective that enforces representational stability across time. We show that this objective is stabilized through a novel training curriculum, ensuring the model robustly learns meaningful features from low signal-to-noise time series. We demonstrate that learned representations enable strong performance on a variety of downstream tasks even when only using a linear probe. Furthermore, we provide comprehensive scaling analyses indicating more unlabeled data reliably results in out-of-distribution performance gains without domain adaptation.",Neuroscience,http://arxiv.org/abs/2512.11582v1,arXiv,0
"We develop here a stochastic framework for modeling and segmenting transient spindle-like oscillatory bursts in electroencephalogram (EEG) signals. At the modeling level, individual spindles are represented as path realizations of a two-dimensional Ornstein{Uhlenbeck (OU) process with a stable focus, providing a low-dimensional stochastic dynamical system whose trajectories reproduce key morphological features of spindles, including their characteristic rise{decay amplitude envelopes. On the signal processing side, we propose a segmentation procedure based on Empirical Mode Decomposition (EMD) combined with the detection of a central extremum, which isolates single spindle events and yields a collection of oscillatory atoms. This construction enables a systematic statistical analysis of spindle features: we derive empirical laws for the distributions of amplitudes, inter-spindle intervals, and rise/decay durations, and show that these exhibit exponential tails consistent with the underlying OU dynamics. We further extend the model to a pair of weakly coupled OU processes with distinct natural frequencies, generating a stochastic mixture of slow, fast, and mixed spindles in random temporal order. The resulting framework provides a data-driven framework for the analysis of transient oscillations in EEG and, more generally, in nonstationary time series.",Neuroscience,http://arxiv.org/abs/2512.10844v1,arXiv,0
"Allometric scaling laws, such as Kleiber's law for metabolic rate, highlight how efficiency emerges with size across living systems. The brain, with its characteristic sublinear scaling of activity, has long posed a puzzle: why do larger brains operate with disproportionately lower firing rates? Here we show that this economy of scale is a universal outcome of avalanche dynamics. We derive analytical scaling laws directly from avalanche statistics, establishing that any system governed by critical avalanches must exhibit sublinear activity-size relations. This theoretical prediction is then verified in integrate-and-fire neuronal networks at criticality and in classical self-organized criticality models, demonstrating that the effect is not model-specific but generic. The predicted exponents align with experimental observations across mammal species, bridging dynamical criticality with the allometry of brain metabolism. Our results reveal avalanche criticality as a fundamental mechanism underlying Kleiber-like scaling in the brain.",Neuroscience,http://arxiv.org/abs/2512.10834v1,arXiv,0
"The full connectome of an adult Drosophila enables a search for novel neural structures in the insect brain. I describe a new neural structure, called a Parallel Neuron Group (PNG). Two neurons are called parallel if they share a significant number of input neurons and output neurons. Most pairs of neurons in the Drosophila brain have very small parallel match. There are about twenty larger groups of neurons for which any pair of neurons in the group has a high match. These are the parallel groups. Parallel groups contain only about 1000 out of the 65,000 neurons in the brain, and have distinctive properties. There are groups in the right mushroom bodies, the antennal lobes, the lobula, and in two central neuropils (GNG and EB). Most parallel groups do not have lateral symmetry. A group usually has one major input neuron, which inputs to all the neurons in the group, and a small number of major output neurons. The major input and output neurons are laterally asymmetric. Parallel neuron groups present puzzles, such as: what does a group do, that could not be done by one larger neuron? Do all neurons in a group fire in synchrony, or do they perform different functions? Why are they laterally asymmetric? These may merit further investigation.",Neuroscience,http://arxiv.org/abs/2512.10525v1,arXiv,0
"The efficiency of modern machine intelligence depends on high accuracy with minimal computational cost. In spiking neural networks (SNNs), synaptic delays are crucial for encoding temporal structure, yet existing models treat them as fully trainable, unconstrained parameters, leading to large memory footprints, higher computational demand, and a departure from biological plausibility. In the brain, however, delays arise from physical distances between neurons embedded in space. Building on this principle, we introduce Spatial Spiking Neural Networks (SpSNNs), a framework in which neurons learn coordinates in a finite-dimensional Euclidean space and delays emerge from inter-neuron distances. This replaces per-synapse delay learning with position learning, substantially reducing parameter count while retaining temporal expressiveness. Across the Yin-Yang and Spiking Heidelberg Digits benchmarks, SpSNNs outperform SNNs with unconstrained delays despite using far fewer parameters. Performance consistently peaks in 2D and 3D networks rather than infinite-dimensional delay spaces, revealing a geometric regularization effect. Moreover, dynamically sparsified SpSNNs maintain full accuracy even at 90% sparsity, matching standard delay-trained SNNs while using up to 18x fewer parameters. Because learned spatial layouts map naturally onto hardware geometries, SpSNNs lend themselves to efficient neuromorphic implementation. Methodologically, SpSNNs compute exact delay gradients via automatic differentiation with custom-derived rules, supporting arbitrary neuron models and architectures. Altogether, SpSNNs provide a principled platform for exploring spatial structure in temporal computation and offer a hardware-friendly substrate for scalable, energy-efficient neuromorphic intelligence.",Neuroscience,http://arxiv.org/abs/2512.10011v2,arXiv,0
"Representations pervade our daily experience, from letters representing sounds to bit strings encoding digital files. While such representations require externally defined decoders to convey meaning, conscious experience appears fundamentally different: a neural state corresponding to perceiving a red square cannot alternatively encode the experience of a green square. This intrinsic property of consciousness suggests that conscious representations must be unambiguous in a way that conventional representations are not. We formalize this intuition using information theory, defining representational ambiguity as the conditional entropy H(I|R) over possible interpretations I given a representation R. Through experiments on neural networks trained to classify MNIST digits, we demonstrate that relational structures in network connectivity can unambiguously encode representational content. Using both learned decoders and direct geometric matching, we achieve perfect (100%) accuracy for dropout-trained networks and 38% for standard backpropagation in identifying output neuron class identity, despite identical task performance, demonstrating that representational ambiguity can arise orthogonally to behavioral accuracy. We further show that spatial position information of input neurons can be decoded from network connectivity with R2 up to 0.844. These results provide a quantitative method for measuring representational ambiguity in neural systems and demonstrate that neural networks can exhibit the low-ambiguity representations posited as necessary (though not sufficient) by theoretical accounts of consciousness.",Neuroscience,http://arxiv.org/abs/2512.11000v1,arXiv,0
"Reinforcement learning (RL) enables adaptive behavior across species via reward prediction errors (RPEs), but the neural origins of species-specific adaptability remain unknown. Integrating RL modeling, transcriptomics, and neuroimaging during reversal learning, we discovered convergent RPE signatures - shared monoaminergic/synaptic gene upregulation and neuroanatomical representations, yet humans outperformed macaques behaviorally. Single-trial decoding showed RPEs guided choices similarly in both species, but humans disproportionately recruited dorsal anterior cingulate (dACC) and dorsolateral prefrontal cortex (dlPFC). Cross-species alignment uncovered that macaque prefrontal circuits encode human-like optimal RPEs yet fail to translate them into action. Adaptability scaled not with RPE encoding fidelity, but with the areal extent of dACC/dlPFC recruitment governing RPE-to-action transformation. These findings resolve an evolutionary puzzle: behavioral performance gaps arise from executive cortical readout efficiency, not encoding capacity.",Neuroscience,http://arxiv.org/abs/2512.09761v2,arXiv,0
"Neural decoding, a critical component of Brain-Computer Interface (BCI), has recently attracted increasing research interest. Previous research has focused on leveraging signal processing and deep learning methods to enhance neural decoding performance. However, the in-depth exploration of model architectures remains underexplored, despite its proven effectiveness in other tasks such as energy forecasting and image classification. In this study, we propose NeuroSketch, an effective framework for neural decoding via systematic architecture optimization. Starting with the basic architecture study, we find that CNN-2D outperforms other architectures in neural decoding tasks and explore its effectiveness from temporal and spatial perspectives. Building on this, we optimize the architecture from macro- to micro-level, achieving improvements in performance at each step. The exploration process and model validations take over 5,000 experiments spanning three distinct modalities (visual, auditory, and speech), three types of brain signals (EEG, SEEG, and ECoG), and eight diverse decoding tasks. Experimental results indicate that NeuroSketch achieves state-of-the-art (SOTA) performance across all evaluated datasets, positioning it as a powerful tool for neural decoding. Our code and scripts are available at https://github.com/Galaxy-Dawn/NeuroSketch.",Neuroscience,http://arxiv.org/abs/2512.09524v1,arXiv,0
"Diverse scientific and engineering research areas deal with discrete, time-stamped changes in large systems of interacting delay differential equations. Simulating such complex systems at scale on high-performance computing clusters demands efficient management of communication and memory. Inspired by the human cerebral cortex -- a sparsely connected network of $\mathcal{O}(10^{10})$ neurons, each forming $\mathcal{O}(10^{3})$--$\mathcal{O}(10^{4})$ synapses and communicating via short electrical pulses called spikes -- we study the simulation of large-scale spiking neural networks for computational neuroscience research. This work presents a novel network construction method for multi-GPU clusters and upcoming exascale supercomputers using the Message Passing Interface (MPI), where each process builds its local connectivity and prepares the data structures for efficient spike exchange across the cluster during state propagation. We demonstrate scaling performance of two cortical models using point-to-point and collective communication, respectively.",Neuroscience,http://arxiv.org/abs/2512.09502v1,arXiv,0
"Biological neural networks learn complex behaviors from sparse, delayed feedback using local synaptic plasticity, yet the mechanisms enabling structured credit assignment remain elusive. In contrast, artificial recurrent networks solving similar tasks typically rely on biologically implausible global learning rules or hand-crafted local updates. The space of local plasticity rules capable of supporting learning from delayed reinforcement remains largely unexplored. Here, we present a meta-learning framework that discovers local learning rules for structured credit assignment in recurrent networks trained with sparse feedback. Our approach interleaves local neo-Hebbian-like updates during task execution with an outer loop that optimizes plasticity parameters via \textbf{tangent-propagation through learning}. The resulting three-factor learning rules enable long-timescale credit assignment using only local information and delayed rewards, offering new insights into biologically grounded mechanisms for learning in recurrent circuits.",Neuroscience,http://arxiv.org/abs/2512.09366v2,arXiv,0
"Influential models of primate visual cortex describe two functionally distinct pathways: a ventral pathway for object recognition and the dorsal pathway for spatial and action processing. However, recent human and non-human primate research suggests the existence of a third visual pathway projecting from early visual cortex through the motion-selective area V5/MT into the superior temporal sulcus (STS). Here we integrate anatomical, neuroimaging, and neuropsychological evidence demonstrating that this pathway specializes in processing dynamic social cues such as facial expressions, eye gaze, and body movements. This third pathway supports social perception by computing the actions and intentions of other people. These findings enhance our understanding of visual cortical organization and highlight the STS's critical role in social cognition, suggesting that visual processing encompasses a dedicated neural circuit for interpreting socially relevant motion and behavior.",Neuroscience,http://arxiv.org/abs/2512.09351v1,arXiv,0
"Neurons, as eukaryotic cells, have powerful internal computation capabilities. One neuron can have many distinct states, and brains can use this capability. Processes of neuron growth and maintenance use chemical signalling between cell bodies and synapses, ferrying chemical messengers over microtubules and actin fibres within cells. These processes are computations which, while slower than neural electrical signalling, could allow any neuron to change its state over intervals of seconds or minutes. Based on its state, a single neuron can selectively de-activate some of its synapses, sculpting a dynamic neural net from the static neural connections of the brain. Without this dynamic selection, the static neural networks in brains are too amorphous and dilute to do the computations of neural cognitive models. The use of multi-state neurons in animal brains is illustrated in hierarchical Bayesian object recognition. Multi-state neurons may support a design which is more efficient than two-state neurons, and scales better as object complexity increases. Brains could have evolved to use multi-state neurons. Multi-state neurons could be used in artificial neural networks, to use a kind of non-Hebbian learning which is faster and more focused and controllable than traditional neural net learning. This possibility has not yet been explored in computational models.",Neuroscience,http://arxiv.org/abs/2512.08815v1,arXiv,0
"Background: Adolescence is a critical period of brain maturation and heightened vulnerability to cognitive and mental health disorders. Sleep plays a vital role in neurodevelopment, yet the mechanisms linking insufficient sleep to adverse brain and behavioral outcomes remain unclear. The glymphatic system (GS), a brain-wide clearance pathway, may provide a key mechanistic link. Methods: Participants from the Adolescent Brain Cognitive Development (ABCD) Study (n =6,800; age ~ 11 years) were categorized into sleep-sufficient (>=9 h/night) and sleep-insufficient (<9 h/night) groups. Linear models tested associations among sleep, PVS burden, brain volumes, and behavioral outcomes. Mediation analyses evaluated whether PVS burden explained sleep-related effects. Results: Adolescents with insufficient sleep exhibited significantly greater PVS burden, reduced cortical, subcortical, and white matter volumes, poorer cognitive performance across multiple domains (largest effect in crystallized intelligence), and elevated psychopathology (largest effect in general problems). Sleep duration and quality were strongly associated with PVS burden. Mediation analyses revealed that PVS burden partially mediated sleep effects on cognition and mental health, with indirect proportions up to 10.9%. Sequential models suggested a pathway from sleep -> PVS -> brain volume -> behavior as the most plausible route. Conclusions: Insufficient sleep during adolescence is linked to glymphatic dysfunction, reflected by increased PVS burden, which partially accounts for adverse effects on brain structure, cognition, and mental health. These findings highlight the GS as a potential mechanistic pathway and imaging biomarker, underscoring the importance of promoting adequate sleep to support neurodevelopment and mental health.",Neuroscience,http://arxiv.org/abs/2512.08704v1,arXiv,0
"Accurately predicting individual neurons' responses and spatial functional properties in complex visual tasks remains a key challenge in understanding neural computation. Existing whole-brain connectome models of Drosophila often rely on parameter assumptions or deep learning approaches, yet remain limited in their ability to reliably predict dynamic neuronal responses. We introduce a Multi-Path Aggregation (MPA) framework, based on neural network steady-state theory, to build a whole-brain Visual Function Profiles (VFP) of Drosophila neurons and predict their responses under diverse visual tasks. Unlike conventional methods relying on redundant parameters, MPA combines visual input features with the whole-brain connectome topology. It uses adjacency matrix powers and finite-path optimization to efficiently predict neuronal function, including ON/OFF polarity, direction selectivity, and responses to complex visual stimuli. Our model achieves a Pearson correlation of 0.84+/-0.12 for ON/OFF responses, outperforming existing methods (0.33+/-0.59), and accurately captures neuron functional properties, including luminance and direction preferences, while allowing single-neuron or population-level blockade simulations. Replacing CNN modules with VFP-derived Lobula Columnar(LC) population responses in a Drosophila simulation enables successful navigation and obstacle avoidance, demonstrating the model's effectiveness in guiding embodied behavior. This study establishes a ""connectome-functional profile-behavior"" framework, offering a whole-brain quantitative tool to study Drosophila visual computation and a neuron-level guide for brain-inspired intelligence.",Neuroscience,http://arxiv.org/abs/2512.06934v1,arXiv,0
"In the process of evolution, the brain has achieved such perfection that artificial intelligence systems do not have and which needs its own mathematics. The concept of cognitome, introduced by the academician K.V. Anokhin, as the cognitive structure of the mind -- a high-order structure of the brain and a neural hypernetwork, is considered as the basis for modeling. Consciousness then is a special form of dynamics in this hypernetwork -- a large-scale integration of its cognitive elements. The cognitome, in turn, consists of interconnected COGs (cognitive groups of neurons) of two types -- functional systems and cellular ensembles. K.V. Anokhin sees the task of the fundamental theory of the brain and mind in describing these structures, their origin, functions and processes in them. The paper presents mathematical models of these structures based on new mathematical results, as well as models of different cognitive processes in terms of these models. In addition, it is shown that these models can be derived based on a fairly general principle of the brain works: \textit{the brain discovers all possible causal relationships in the external world and draws all possible conclusions from them}. Based on these results, the paper presents models of: ``natural"" classification; theory of functional brain systems by P.K. Anokhin; prototypical theory of categorization by E. Roche; theory of causal models by Bob Rehter; theory of consciousness as integrated information by G. Tononi.",Neuroscience,http://arxiv.org/abs/2512.10988v1,arXiv,0
"The human visual system exhibits non-uniform spatial resolution across the visual field, which is characterized by the cortical magnification factor (CMF) that reflects its anatomical basis. However, current approaches for quantifying CMF using retinotopic maps derived from BOLD functional magnetic resonance imaging (fMRI) are limited by the inherent low signal-to-noise ratio of fMRI data and inaccuracies in the topological relationships of the retinotopic maps. In this study, we introduced a new pipeline to quantify planar CMF from retinotopic maps generated from the population receptive field (pRF) model. The pipeline projected the 3D pRF solutions onto a 2D planar disk, using optimal transport (OT) to preserve local cortical surface areas, and applied topological smoothing to ensure that the resulting retinotopic maps maintain their topology. We then estimated 2D CMF maps from the projected retinotopic maps on the planar disk using the 1-ring patch method. Applying this pipeline to the Human Connectome Project (HCP) 7T dataset, we revealed previously unobserved CMF patterns across the visual field and demonstrated individual differences among the 181 subjects. The pipeline was further validated on the New York University (NYU) 3T dataset, showing reliable and repeatable results. Our study provided new analytical methods and offered novel insights into visual processing.",Neuroscience,http://arxiv.org/abs/2512.06492v1,arXiv,0
"In the last decade, there have been major advances in clusterless decoding algorithms for neural data analysis. These algorithms use the theory of marked point processes to describe the joint activity of many neurons simultaneously, without the need for spike sorting. In this study, we examine information-theoretic metrics to analyze the information extracted from each observed spike under such clusterless models. In an analysis of spatial coding in the rat hippocampus, we compared the entropy reduction between spike-sorted and clusterless models for both individual spikes observed in isolation and when the prior information from all previously observed spikes is accounted for. Our analysis demonstrates that low-amplitude spikes, which are difficult to cluster and often left out of spike sorting, provide reduced information compared to sortable, high-amplitude spikes when considered in isolation, but the two provide similar levels of information when considering all the prior information available from past spiking. These findings demonstrate the value of combining information measures with state-space modeling and yield new insights into the underlying mechanisms of neural computation.",Neuroscience,http://arxiv.org/abs/2512.06280v1,arXiv,0
"Early forecasting of individual cognitive decline in Alzheimer's disease (AD) is central to disease evaluation and management. Despite advances, it is as of yet challenging for existing methodological frameworks to integrate multimodal data for longitudinal personalized forecasting while maintaining interpretability. To address this gap, we present the Neural Koopman Machine (NKM), a new machine learning architecture inspired by dynamical systems and attention mechanisms, designed to forecast multiple cognitive scores simultaneously using multimodal genetic, neuroimaging, proteomic, and demographic data. NKM integrates analytical ($Î±$) and biological ($Î²$) knowledge to guide feature grouping and control the hierarchical attention mechanisms to extract relevant patterns. By implementing Fusion Group-Aware Hierarchical Attention within the Koopman operator framework, NKM transforms complex nonlinear trajectories into interpretable linear representations. To demonstrate NKM's efficacy, we applied it to study the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. Our results suggest that NKM consistently outperforms both traditional machine learning methods and deep learning models in forecasting trajectories of cognitive decline. Specifically, NKM (1) forecasts changes of multiple cognitive scores simultaneously, (2) quantifies differential biomarker contributions to predicting distinctive cognitive scores, and (3) identifies brain regions most predictive of cognitive deterioration. Together, NKM advances personalized, interpretable forecasting of future cognitive decline in AD using past multimodal data through an explainable, explicit system and reveals potential multimodal biological underpinnings of AD progression.",Neuroscience,http://arxiv.org/abs/2512.06134v1,arXiv,0
"A few million words suffice for children to acquire language. Yet, the brain mechanisms underlying this unique ability remain poorly understood. To address this issue, we investigate neural activity recorded from over 7,400 electrodes implanted in the brains of 46 children, teenagers, and adults for epilepsy monitoring, as they listened to an audiobook version of ""The Little Prince"". We then train neural encoding and decoding models using representations, derived either from linguistic theory or from large language models, to map the location, dynamics and development of the language hierarchy in the brain. We find that a broad range of linguistic features is robustly represented across the cortex, even in 2-5-year-olds. Crucially, these representations evolve with age: while fast phonetic features are already present in the superior temporal gyrus of the youngest individuals, slower word-level representations only emerge in the associative cortices of older individuals. Remarkably, this neuro-developmental trajectory is spontaneously captured by large language models: with training, these AI models learned representations that can only be identified in the adult human brain. Together, these findings reveal the maturation of language representations in the developing brain and show that modern AI systems provide a promising tool to model the neural bases of language acquisition.",Neuroscience,http://arxiv.org/abs/2512.05718v1,arXiv,0
"The existence of 'what' and 'where' pathways of information processing in the brain was proposed almost 30 years ago, but there is still a lack of a clear mathematical model that could show how these pathways work together. We propose a biologically inspired mathematical model that uses this idea to identify and separate the self from the environment and then build and use a self-model for better predictions. This is a model of neocortical columns governed by the basal ganglia to make predictions and choose the next action, where some columns act as 'what' columns and others act as 'where' columns. Based on this model, we present a reinforcement learning agent that learns purposeful behavior in a virtual environment. We evaluate the agent on the Atari games Pong and Breakout, where it successfully learns to play. We conclude that the ability to separate the self from the environment gives advantages to the agent and therefore such a model could appear in living organisms during evolution. We propose Self-Awareness Principle 1: the ability to separate the self from the world is a necessary but insufficient condition for self-awareness.",Neuroscience,http://arxiv.org/abs/2512.10985v1,arXiv,0
"Art has long played a profound role in shaping human emotion, cognition, and behavior. While visual arts such as painting and architecture have been studied through eye tracking, revealing distinct gaze patterns between experts and novices, analogous methods for auditory art forms remain underdeveloped. Music, despite being a pervasive component of modern life and culture, still lacks objective tools to quantify listeners' attention and perceptual focus during natural listening experiences. To our knowledge, this is the first attempt to decode selective attention to musical elements using naturalistic, studio-produced songs and a lightweight consumer-grade EEG device with only four electrodes. By analyzing neural responses during real world like music listening, we test whether decoding is feasible under conditions that minimize participant burden and preserve the authenticity of the musical experience. Our contributions are fourfold: (i) decoding music attention in real studio-produced songs, (ii) demonstrating feasibility with a four-channel consumer EEG, (iii) providing insights for music attention decoding, and (iv) demonstrating improved model ability over prior work. Our findings suggest that musical attention can be decoded not only for novel songs but also across new subjects, showing performance improvements compared to existing approaches under our tested conditions. These findings show that consumer-grade devices can reliably capture signals, and that neural decoding in music could be feasible in real-world settings. This paves the way for applications in education, personalized music technologies, and therapeutic interventions.",Neuroscience,http://arxiv.org/abs/2512.05528v1,arXiv,0
"EEG recordings are inherently contaminated by artifacts such as ocular, muscular, and environmental noise, which obscure neural activity and complicate preprocessing. Artifact classification offers advantages in stability and transparency, providing a viable alternative to ICA-based methods that enable flexible use alongside human inspections and across various applications. However, artifact classification is limited by its training data as it requires extensive manual labeling, which cannot fully cover the diversity of real-world EEG. Semi-synthetic data (SSD) methods have been proposed to address this limitation, but prior approaches typically injected single artifact types using ICA components or required separately recorded artifact signals, reducing both the realism of the generated data and the applicability of the method. To overcome these issues, we introduce SSDLabeler, a framework that generates realistic, annotated SSDs by decomposing real EEG with ICA, epoch-level artifact verification using RMS and PSD criteria, and reinjecting multiple artifact types into clean data. When applied to train a multi-label artifact classifier, it improved accuracy on raw EEG across diverse conditions compared to prior SSD and raw EEG training, establishing a scalable foundation for artifact handling that captures the co-occurrence and complexity of real EEG.",Neuroscience,http://arxiv.org/abs/2512.05500v1,arXiv,0
"We propose Symmetry-Loss, a brain-inspired algorithmic principle that enforces invariance and equivariance through a differentiable constraint derived from environmental symmetries. The framework models learning as the iterative refinement of an effective symmetry group, paralleling developmental processes in which cortical representations align with the world's structure. By minimizing structural surprise, i.e. deviations from symmetry consistency, Symmetry-Loss operationalizes a Free-Energy--like objective for representation learning. This formulation bridges predictive-coding and group-theoretic perspectives, showing how efficient, stable, and compositional representations can emerge from symmetry-based self-organization. The result is a general computational mechanism linking developmental learning in the brain with principled representation learning in artificial systems.",Neuroscience,http://arxiv.org/abs/2512.10984v2,arXiv,0
"Energy-based models have become a central paradigm for understanding computation and stability in both theoretical neuroscience and machine learning. However, the energetic framework typically relies on symmetry in synaptic or weight matrices - a constraint that excludes biologically realistic systems such as excitatory-inhibitory (E-I) networks. When symmetry is relaxed, the classical notion of a global energy landscape fails, leaving the dynamics of asymmetric neural systems conceptually unanchored. In this work, we extend the energetic framework to asymmetric firing rate networks, revealing an underlying game-theoretic structure for the neural dynamics in which each neuron is an agent that seeks to minimize its own energy. In addition, we exploit rigorous stability principles from network theory to study regulation and balancing of neural activity in E-I networks. We combine the novel game-energetic interpretation and the stability results to revisit standard frameworks in theoretical neuroscience, such as the Wilson-Cowan and lateral inhibition models. These insights allow us to study cortical columns of lateral inhibition microcircuits as contrast enhancer - with the ability to selectively sharpen subtle differences in the environment through hierarchical excitation-inhibition interplay. Our results bridge energetic and game-theoretic views of neural computation, offering a pathway toward the systematic engineering of biologically grounded, dynamically stable neural architectures.",Neuroscience,http://arxiv.org/abs/2512.05252v1,arXiv,0
"Serotonin (5-hydroxytryptamine) is a major neurotransmitter whose release from densely distributed serotonergic varicosities shapes plasticity and network integration throughout the brain, yet its extracellular dynamics remain poorly understood due to the sub-micrometer and millisecond scales involved. We develop a mathematical framework that captures the coupled reaction-diffusion processes governing serotonin signaling in realistic tissue microenvironments. Formulating a two-dimensional compartmental-reaction diffusion system, we use strong localized perturbation theory to derive an asymptotically equivalent set of nonlinear integro-ODEs that preserve diffusive coupling while enabling efficient computation. We analyze period-averaged steady states, establish bounds using Jensen's inequality, obtain closed-form spike maxima and minima, and implement a fast marching-scheme solver based on sum-of-exponentials kernels. These mathematical results provide quantitative insight into how firing frequency, varicosity geometry, and uptake kinetics shape extracellular serotonin. The model reveals that varicosities form diffusively coupled microdomains capable of generating spatial ""serotonin reservoirs,"" clarifies aspects of local versus volume transmission, and yields predictions relevant to interpreting high-resolution serotonin imaging and the actions of selective serotonin-reuptake inhibitors.",Neuroscience,http://arxiv.org/abs/2512.10983v1,arXiv,0
"Brain dynamics dominate every level of neural organization -- from single-neuron spiking to the macroscopic waves captured by fMRI, MEG, and EEG -- yet the mathematical tools used to interrogate those dynamics remain scattered across a patchwork of traditions. Neural mass models (NMMs) (aggregate neural models) provide one of the most popular gateways into this landscape, but their sheer variety -- spanning lumped parameter models, firing-rate equations, and multi-layer generators -- demands a unifying framework that situates diverse architectures along a continuum of abstraction and biological detail. Here, we start from the idea that oscillations originate from a simple push-pull interaction between two or more neural populations. We build from the undamped harmonic oscillator and, guided by a simple push-pull motif between excitatory and inhibitory populations, climb a systematic ladder of detail. Each rung is presented first in isolation, next under forcing, and then within a coupled network, reflecting the progression from single-node to whole-brain modeling. By transforming a repertoire of disparate formalisms into a navigable ladder, we hope to turn NMM choice from a subjective act into a principled design decision, helping both theorists and experimentalists translate between scales, modalities, and interventions. In doing so, we offer a \emph{Rosetta Stone} for brain oscillation models -- one that lets the field speak a common dynamical language while preserving the dialectical richness that fuels discovery.",Neuroscience,http://arxiv.org/abs/2512.10982v1,arXiv,0
"Advances in large-scale neural recordings have expanded our ability to describe the activity of distributed brain circuits. However, understanding how neural population dynamics differ across regions and behavioral contexts remains challenging. Here, we surveyed neuronal population dynamics across multiple mouse brain areas (visual cortex, hippocampus, thalamus, and midbrain) using spike data from local ensembles. Two complementary measures were used to characterize these dynamics: the coefficient of variation (CV), a classical indicator of spike-time variability, and statistical complexity, an information-theoretic quantifier of organizational structure. To probe stimulus-dependent activity, we segmented and concatenated recordings from behavioral experiments into distinct time series corresponding to natural image presentations, blank screens during visual task, and spontaneous activity. While the CV failed to discriminate between these conditions, statistical complexity revealed clear, stimulus-specific motifs in population activity. These results indicate that information-theoretic measures can uncover structured, stimulus-dependent patterns in neural population dynamics that remain unobserved in traditional variability metrics.",Neuroscience,http://arxiv.org/abs/2512.05007v1,arXiv,0
"This study investigates the causal neural dynamics by which affordance representations influence action language comprehension. In this study, 18 participants observed stimuli displayed in two conditions during the experiment: text-only (e.g., `Hit with a hammer') and video+text (visual clips with matching phrases). EEG data were recorded from 32 channels and analyzed for event-related potentials and source localization using LORETA, which identified four left-hemisphere regions of interest: the Lateral Occipital Cortex (LOC), Posterior Superior Temporal Gyrus (pSTG), Ventral Premotor Cortex (PMv), and Inferior Parietal Lobule (IPL). A space of dynamic causal modeling (DCM) was constructed with driving inputs to LOC and pSTG, and multiple connectivity configurations were tested. Bayesian Model Selection revealed a dominant model in which PMv causally influenced IPL and pSTG, reflecting a feedforward architecture from affordance-related motor regions to semantic hubs. Bayesian Model Averaging further confirmed strong endogenous connections from LOC to PMv and IPL, and significant modulation from PMv to IPL. These findings provide direct evidence that affordance processing in premotor regions drives action language understanding by engaging downstream parietal and temporal areas. The results support grounded cognition theories and offer a mechanistic account of how sensorimotor information contributes to linguistic comprehension.",Neuroscience,http://arxiv.org/abs/2512.04989v1,arXiv,0
"Alzheimer's disease (AD) persists as a paramount challenge in neurological research, characterized by the pathological hallmarks of amyloid-beta (Abeta) plaques and neurofibrillary tangles composed of hyperphosphorylated tau. This review synthesizes the evolving understanding of AD pathogenesis, moving beyond the linear amyloid cascade hypothesis to conceptualize the disease as a cross-talk of intricately interacting pathologies, encompassing Abeta, tau, and neuroinflammation. This evolving pathophysiological understanding parallels a transformation in diagnostic paradigms, where biomarker-based strategies -- such as the AT(N) framework -- enable early disease detection during preclinical or prodromal stages. Within this new landscape, while anti-Abeta monoclonal antibodies (e.g., lecanemab, donanemab) represent a breakthrough as the first disease-modifying therapies, their modest efficacy underscores the limitation of single-target approaches. Therefore, this review explores the compelling rationale for combination therapies that simultaneously target Abeta pathology, aberrant tau, and neuroinflammation. Looking forward, we emphasize emerging technological platforms -- such as gene editing and biophysical neuromodulation -- n advancing precision medicine. Ultimately, the integration of early biomarker detection, multi-target therapeutic strategies, and AI-driven patient stratification charts a promising roadmap toward fundamentally altering the trajectory of AD. The future of AD management will be defined by preemptive, biomarker-guided, and personalized combination interventions.",Neuroscience,http://arxiv.org/abs/2512.10981v1,arXiv,0
"The vision of a universal AI tutor has remained elusive, despite decades of effort. Could LLMs be the game-changer? We overview novel issues arising from developing a nationwide AI tutor. We highlight the practical questions that point to specific gaps in our scientific understanding of the learning process.",Neuroscience,http://arxiv.org/abs/2512.04869v1,arXiv,0
"Discovering the neural mechanisms underpinning cognition is one of the grand challenges of neuroscience. However, previous approaches for building models of RNN dynamics that explain behaviour required iterative refinement of architectures and/or optimisation objectives, resulting in a piecemeal, and mostly heuristic, human-in-the-loop process. Here, we offer an alternative approach that automates the discovery of viable RNN mechanisms by explicitly training RNNs to reproduce behaviour, including the same characteristic errors and suboptimalities, that humans and animals produce in a cognitive task. Achieving this required two main innovations. First, as the amount of behavioural data that can be collected in experiments is often too limited to train RNNs, we use a non-parametric generative model of behavioural responses to produce surrogate data for training RNNs. Second, to capture all relevant statistical aspects of the data, we developed a novel diffusion model-based approach for training RNNs. To showcase the potential of our approach, we chose a visual working memory task as our test-bed, as behaviour in this task is well known to produce response distributions that are patently multimodal (due to swap errors). The resulting network dynamics correctly qualitative features of macaque neural data. Importantly, these results were not possible to obtain with more traditional approaches, i.e., when only a limited set of behavioural signatures (rather than the full richness of behavioural response distributions) were fitted, or when RNNs were trained for task optimality (instead of reproducing behaviour). Our approach also yields novel predictions about the mechanism of swap errors, which can be readily tested in experiments. These results suggest that fitting RNNs to rich patterns of behaviour provides a powerful way to automatically discover mechanisms of important cognitive functions.",Neuroscience,http://arxiv.org/abs/2512.04808v1,arXiv,0
"Rhythmic fluctuations in acoustic energy and accompanying neuronal excitations in cortical oscillations are characteristic of human speech, yet whether a corresponding rhythmicity inheres in the articulatory movements that generate speech remains unclear. The received understanding of speech movements as discrete, goal-oriented actions struggles to make contact with the rhythmicity findings. In this work, we demonstrate that an unintuitive -- but no less principled than the conventional -- representation for discrete movements reveals a pervasive limit cycle organization and unlocks the recovery of previously inaccessible rhythmic structure underlying the motor activity of speech. These results help resolve a time-honored tension between the ubiquity of biological rhythmicity and discreteness in speech, the quintessential human higher function, by revealing a rhythmic organization at the most fundamental level of individual articulatory actions.",Neuroscience,http://arxiv.org/abs/2512.04642v1,arXiv,0
"Analysing how neural networks represent data features in their activations can help interpret how they perform tasks. Hence, a long line of work has focused on mathematically characterising the geometry of such ""neural representations."" In parallel, machine learning has seen a surge of interest in understanding how dynamical systems perform computations on time-varying input data. Yet, the link between computation-through-dynamics and representational geometry remains poorly understood. Here, we hypothesise that recurrent neural networks (RNNs) perform computations by dynamically warping their representations of task variables. To test this hypothesis, we develop a Riemannian geometric framework that enables the derivation of the manifold topology and geometry of a dynamical system from the manifold of its inputs. By characterising the time-varying geometry of RNNs, we show that dynamic warping is a fundamental feature of their computations.",Neuroscience,http://arxiv.org/abs/2512.04310v1,arXiv,0
"A combinatorial neural code is a subset of the power set $2^{[n]}$ on $[n]=\{1,\dots, n\}$, in which each $1\leq i\leq n$ represents a neuron and each element (codeword) represents the co-firing event of some neurons. Consider a space $X\subseteq\mathbb{R}^d$, simulating an animal's environment, and a collection $\mathcal{U}=\{U_1,\dots,U_n\}$ of open subsets of $X$. Each $U_i\subseteq X$ simulates a place field which is a specific region where a place cell $i$ is active. Then, the code of $\mathcal{U}$ in $X$ is defined as $\text{code}(\mathcal{U},X)=\left\{Ï\subseteq[n]\bigg|\bigcap_{i\inÏ} U_i\setminus\bigcup_{j\notinÏ}U_j\neq\varnothing\right\}$. If a neural code $\mathcal{C}=\text{code}(\mathcal{U},X)$ for some $X$ and $\mathcal{U}$, we say $\mathcal{C}$ has a realization of open subsets of some space $X$. Although every combinatorial neural code obviously has a realization by some open subsets, determining whether it has a realization by some open convex subsets remains unsolved. Many studies attempted to tackle this decision problem, but only partial results were achieved. In fact, a previous study showed that the decision problem of convex neural codes is NP-hard. Furthermore, the authors of this study conjectured that every convex neural code can be realized as a minor of a neural code arising from a representable oriented matroid, which can lead to an equivalence between convex and polytope convex neural codes. Even though this conjecture has been confirmed in dimension two, its validity in higher dimensions is still unknown. To advance the investigation of this conjecture, we provide a complete characterization of the covering relations within the poset $\mathbf{P_{Code}}$ of neural codes.",Neuroscience,http://arxiv.org/abs/2512.04241v1,arXiv,0
"Understanding the dynamics of large-scale brain models remains a central challenge due to the inherent complexity of these systems. In this work, we explore the emergence of complex spatiotemporal patterns in a large scale-brain model composed of 90 interconnected brain regions coupled through empirically derived anatomical connectivity. An important aspect of our formulation is that the local dynamics of each brain region are described by a next-generation neural mass model, which explicitly captures the macroscopic gamma activity of coupled excitatory and inhibitory neural populations (PING mechanism). We first identify the system's homogeneous states-both resting and oscillatory-and analyze their stability under uniform perturbations. Then, we determine the stability against non-uniform perturbations by obtaining dispersion relations for the perturbation growth rate. This analysis enables us to link unstable directions of the homogeneous solutions to the emergence of rich spatiotemporal patterns, that we characterize by means of Lyapunov exponents and frequency spectrum analysis. Our results show that, compared to previous studies with classical neural mass models, next-generation neural mass models provide a broader dynamical repertoire, both within homogeneous states and in the heterogeneous regime. Additionally, we identify a key role for anatomical connectivity in cross-frequency coupling, allowing for the emergence of gamma oscillations with amplitude modulated by slower rhythms. These findings suggest that such models are not only more biophysically grounded but also particularly well-suited to capture the full complexity of large-scale brain dynamics. Overall, our study advances the analytical understanding of emerging spatiotemporal patterns in whole-brain models.",Neuroscience,http://arxiv.org/abs/2512.03907v1,arXiv,0
"Sleep disorders have emerged as a critical global health issue, highlighting the urgent need for effective and widely accessible intervention technologies. Non-invasive brain stimulation has garnered attention as it enables direct or indirect modulation of neural activity, thereby promoting sleep enhancement in a safe and unobtrusive manner. This class of approaches is collectively referred to as sleep modulation. To date, the majority of sleep modulation research relies on open-loop paradigms with empirically determined parameters, while achieving individual adaptation and modulation accuracy remains a distant objective. The paradigm-specific constraints inherent to open-loop designs represent a major obstacle to clinical translation and large-scale deployment in home environments. In this paper, we delineate fundamental paradigms of sleep modulation, critically examine the intrinsic limitations of open-loop approaches, and formally conceptualize sleep closed-loop modulation. We further provide a comprehensive synthesis of prior studies involving five commonly employed modulation techniques, evaluating their potential integration within a closed-loop framework. Finally, we identify three primary challenges in constructing an effective sleep closed-loop modulation system: sensor solution selection, monitoring model design, and modulation strategy design, while also proposing potential solutions. Collectively, this work aims to advance the paradigm shift of sleep modulation from open-loop toward closed-loop systems.",Neuroscience,http://arxiv.org/abs/2512.03784v1,arXiv,0
"Large language models (LLMs) have achieved state-of-the-art performance in a variety of tasks, but remain largely opaque in terms of their internal mechanisms. Understanding these mechanisms is crucial to improve their reasoning abilities. Drawing inspiration from the interplay between neural processes and human cognition, we propose a novel interpretability framework to systematically analyze the roles and behaviors of attention heads, which are key components of LLMs. We introduce CogQA, a dataset that decomposes complex questions into step-by-step subquestions with a chain-of-thought design, each associated with specific cognitive functions such as retrieval or logical reasoning. By applying a multi-class probing method, we identify the attention heads responsible for these functions. Our analysis across multiple LLM families reveals that attention heads exhibit functional specialization, characterized as cognitive heads. These cognitive heads exhibit several key properties: they are universally sparse, vary in number and distribution across different cognitive functions, and display interactive and hierarchical structures. We further show that cognitive heads play a vital role in reasoning tasks - removing them leads to performance degradation, while augmenting them enhances reasoning accuracy. These insights offer a deeper understanding of LLM reasoning and suggest important implications for model design, training, and fine-tuning strategies.",Neuroscience,http://arxiv.org/abs/2512.10978v1,arXiv,0
"A prominent hypothesis in neuroscience proposes that brains achieve optimal performance by operating near a critical point. However, this framework, which often assumes a universal critical point, fails to account for the extensive individual variability observed in neural dynamics and cognitive functions. These variabilities are not noise but rather an inherent manifestation of a fundamental systems-biology principle: the necessary trade-off between robustness and flexibility in human populations. Here, we propose that the Griffiths phase (GP), an extended critical regime synergically induced by two kinds of heterogeneities in brain network region and connectivity, offers a unified framework for brain criticality that better reconciles robustness and flexibility and accounts for individual variability. Using Human Connectome Project data and whole-brain modeling, we demonstrated that the synergic interplay between structural network modularity and regional heterogeneity in local excitability yields biologically viable GP featured with widely extended global excitability ranges, with an embedded optimal point that balances global/local information transmission. Crucially, an individua's position within the GP gives rise to unique global network dynamics, which in turn confer a distinctive cognitive profile via flexible configuration of functional connectivity for segregation, integration, and balance between them. These results establish GP as an evolved adaptive mechanism resolving the robustness-flexibility trade-off, fulfilling diverse cognitive demands through individualized criticality landscapes, providing a new framework of brain criticality.",Neuroscience,http://arxiv.org/abs/2512.03409v1,arXiv,0
"Biological intelligence emerges from substrates that are slow, noisy, and energetically constrained, yet it performs rapid and coherent inference in open-ended environments. Classical computational theories, built around vector-space transformations and instantaneous error minimization, struggle to reconcile the slow timescale of synaptic plasticity with the fast timescale of perceptual synthesis. We propose a unifying framework based on algebraic topology, the Homological Brain, in which neural computation is understood as the construction and navigation of topological structure. Central to this view is the Parity Principle, a homological partition between even-dimensional scaffolds encoding stable content ($Î¦$) and odd-dimensional flows encoding dynamic context ($Î¨$). Transient contextual flows are resolved through a three-stage topological trinity transformation: Search (open-chain exploration), Closure (topological cycle formation), and Condensation (collapse of validated flows into new scaffold). This process converts high-complexity recursive search (formally modeled by Savitch's Theorem in NPSPACE) into low-complexity navigation over a learned manifold (analogous to memoized Dynamic Programming in P). In this framework, topological condensation is the mechanism that transforms a ``search problem'' into a ``navigation task'', allowing the brain to amortize past inference and achieve rapid perceptual integration. This perspective unifies the Wake-Sleep cycle, episodic-to-semantic consolidation, and dual-process theories (System 1-vs-System 2), revealing the brain as a homology engine that minimizes topological complexity to transmute high-entropy sensory flux into low-entropy, invariant cognitive structure.",Neuroscience,http://arxiv.org/abs/2512.10976v1,arXiv,0
"Active inference proposes expected free energy as an objective for planning and decision-making to adequately balance exploitative and explorative drives in learning agents. The exploitative drive, or what an agent wants to achieve, is formalised as the Kullback-Leibler divergence between a variational probability distribution, updated at each inference step, and a preference probability distribution that indicates what states or observations are more likely for the agent, hence determining the agent's goal in a certain environment. In the literature, the questions of how the preference distribution should be specified and of how a certain specification impacts inference and learning in an active inference agent have been given hardly any attention. In this work, we consider four possible ways of defining the preference distribution, either providing the agents with hard or soft goals and either involving or not goal shaping (i.e., intermediate goals). We compare the performances of four agents, each given one of the possible preference distributions, in a grid world navigation task. Our results show that goal shaping enables the best performance overall (i.e., it promotes exploitation) while sacrificing learning about the environment's transition dynamics (i.e., it hampers exploration).",Neuroscience,http://arxiv.org/abs/2512.03293v1,arXiv,0
"Robust decoding and classification of brain patterns measured with electroencephalography (EEG) remains a major challenge for real-world (i.e. outside scientific lab and medical facilities) brain-computer interface (BCI) applications due to well documented inter- and intra-participant variability. Here, we present a large-scale benchmark evaluating over 340,000+ unique combinations of spatial and nonlinear EEG classification. Our methodological pipeline consists in combinations of Common Spatial Patterns (CSP), Riemannian geometry, functional connectivity, and fractal- or entropy-based features across three open-access EEG datasets. Unlike prior studies, our analysis operates at the per-participant level and across multiple frequency bands (8-15 Hz and 8-30 Hz), enabling direct assessment of both group-level performance and individual variability. Covariance tangent space projection (cov-tgsp) and CSP consistently achieved the highest average classification accuracies. However, their effectiveness was strongly dataset-dependent, and marked participant-level differences persisted, particularly in the most heterogeneous of the datasets. Importantly, nonlinear methods outperformed spatial approaches for specific individuals, underscoring the need for personalized pipeline selection. Our findings highlight that no universal 'one-size-fits-all' method can optimally decode EEG motor imagery patterns across all users or datasets. Future work will require adaptive, multimodal, and possibly novel approaches to fully address neurophysiological variability in practical BCI applications where the system can automatically adapt to what makes each user unique.",Neuroscience,http://arxiv.org/abs/2512.02978v1,arXiv,0
"Large language models (LLMs) excel at explicit reasoning, but their implicit computational strategies remain underexplored. Decades of psychophysics research show that humans intuitively process and integrate noisy signals using near-optimal Bayesian strategies in perceptual tasks. We ask whether LLMs exhibit similar behaviour and perform optimal multimodal integration without explicit training or instruction. Adopting the psychophysics paradigm, we infer computational principles of LLMs from systematic behavioural studies. We introduce a behavioural benchmark - BayesBench: four magnitude estimation tasks (length, location, distance, and duration) over text and image, inspired by classic psychophysics, and evaluate a diverse set of nine LLMs alongside human judgments for calibration. Through controlled ablations of noise, context, and instruction prompts, we measure performance, behaviour and efficiency in multimodal cue-combination. Beyond accuracy and efficiency metrics, we introduce a Bayesian Consistency Score that detects Bayes-consistent behavioural shifts even when accuracy saturates. Our results show that while capable models often adapt in Bayes-consistent ways, accuracy does not guarantee robustness. Notably, GPT-5 Mini achieves perfect text accuracy but fails to integrate visual cues efficiently. This reveals a critical dissociation between capability and strategy, suggesting accuracy-centric benchmarks may over-index on performance while missing brittle uncertainty handling. These findings reveal emergent principled handling of uncertainty and highlight the correlation between accuracy and Bayesian tendencies. We release our psychophysics benchmark and consistency metric (https://bayes-bench.github.io) as evaluation tools and to inform future multimodal architecture designs.",Neuroscience,http://arxiv.org/abs/2512.02719v1,arXiv,0
"Higher-order information theory has become a rapidly growing toolkit in computational neuroscience, motivated by the idea that multivariate dependencies can reveal aspects of neural computation and communication that are invisible to pairwise analyses. Yet functional interpretations of synergy and redundancy often outpace principled arguments for how statistical quantities map onto mechanistic cognitive processes. Here we review the main families of higher-order measures with the explicit goal of translating mathematical properties into defensible mechanistic inferences. First, we systematize Shannon-based multivariate metrics and demonstrate that higher-order dependence is parsimoniously characterized by two largely independent axes: interaction strength and redundancy-synergy balance. We argue that balanced layering of synergistic integration and redundant broadcasting optimizes multiscale complexity, formalizing a computation-communication tradeoff. We then examine the partial information decomposition and outline pragmatic considerations for its deployment in neural data. Equipped with the relevant mathematical essentials, we connect redundancy-synergy balance to cognitive function by progressively embedding their mathematical properties in real-world constraints, starting with small synthetic systems before gradually building up to neuroimaging. We close by identifying key future directions for mechanistic insight: cross-scale bridging, intervention-based validation, and thermodynamically grounded unification of information dynamics.",Neuroscience,http://arxiv.org/abs/2512.02671v1,arXiv,0
"Studying learning-related plasticity is central to understanding the acquisition of complex skills, for example learning to master a musical instrument. Over the past three decades, conventional group-based functional magnetic resonance imaging (fMRI) studies have advanced our understanding of how humans' neural representations change during skill acquisition. However, group-based fMRI studies average across heterogeneous learners and often rely on coarse pre- versus post-training comparisons, limiting the spatial and temporal precision with which neural changes can be estimated. Here, we outline an individual-specific precision approach that tracks neural changes within individuals by collecting high-quality neuroimaging data frequently over the course of training, mapping brain function in each person's own anatomical space, and gathering detailed behavioral measures of learning, allowing neural trajectories to be directly linked to individual learning progress. Complementing fMRI with mobile neuroimaging methods, such as functional near-infrared spectroscopy (fNIRS), will enable researchers to track plasticity during naturalistic practice and across extended time scales. This multi-modal approach will enhance sensitivity to individual learning trajectories and will offer more nuanced insights into how neural representations change with training. We also discuss how findings can be generalized beyond individuals, including through statistical methods based on replication in additional individuals. Together, this approach allows researchers to design highly informative longitudinal training studies that advance a mechanistic, personalized account of skill learning in the human brain.",Neuroscience,http://arxiv.org/abs/2512.02503v1,arXiv,0
"Recent advances in general-purpose AI systems with attention-based transformers offer a potential window into how the neocortex and cerebellum, despite their relatively uniform circuit architectures, give rise to diverse functions and, ultimately, to human intelligence. This Perspective provides a cross-domain comparison between the brain and AI that goes beyond the traditional focus on visual processing, adopting the emerging perspecive of world-model-based computation. Here, we identify shared computational mechanisms in the attention-based neocortex and the non-attentional cerebellum: both predict future world events from past inputs and construct internal world models through prediction-error learning. These predictive world models are repurposed for seemingly distinct functions -- understanding in sensory processing and generation in motor processing -- enabling the brain to achieve multi-domain capabilities and human-like adaptive intelligence. Notably, attention-based AI has independently converged on a similar learning paradigm and world-model-based computation. We conclude that these shared mechanisms in both biological and artificial systems constitute a core computational foundation for realizing diverse functions including high-level intelligence, despite their relatively uniform circuit structures. Our theoretical insights bridge neuroscience and AI, advancing our understanding of the computational essence of intelligence.",Neuroscience,http://arxiv.org/abs/2512.02419v1,arXiv,0
"Quantifying the neural signatures of consciousness remains a major challenge in neuroscience and AI. Although many theories link consciousness to rich, multiscale, and flexible neural organisation, robust quantitative measures are still lacking. This paper presents a theory-neutral framework that characterises consciousness-related dynamics through three properties: hierarchical integration (H), cross-frequency complexity (D), and metastability (M). Candidate subsystems are identified using predictive information, temporal complexity, and state-space exploration to distinguish structured from unstructured activity. We provide mathematical definitions for all components and implement the framework in a generative model of synthetic EEG, simulating nine brain states ranging from psychedelic and wakeful to dreaming, non-REM sleep, minimally conscious, anaesthetised, and seizure-like regimes. Across single trials and Monte Carlo ensembles, the composite index reliably separates high-consciousness from impaired or non-conscious states. We further validate the framework using real EEG from the Sleep-EDF dataset alongside matched synthetic EEG designed to reproduce state-dependent oscillatory structure. Across Wake, N2, and REM sleep, synthetic data recapitulate the empirical ordering and magnitude of the index, indicating that the index captures stable and biologically meaningful distinctions. This approach provides a principled and empirically grounded tool for quantifying consciousness-related neural organisation with potential applications to both biological and artificial systems.",Neuroscience,http://arxiv.org/abs/2512.10972v1,arXiv,0
"Neurons in cortical areas often integrate signals from different origins. In the primary visual cortex (V1), neural responses are modulated by non-visual context such as the animal's position. However, the spatial profile of these position signals across the environment remains unknown. Here, we propose a new framework to disentangle visual and spatial contributions in virtual reality. This method relies on two principles: 1) a virtual corridor design that decorrelates vision and space through targeted cue repetitions and manipulations and 2) a Generalized Linear Model (GLM) that explicitly estimates visual contributions in retinotopic rather than environmental coordinates. In simulations, we demonstrate that this framework is highly specific (recovering spatial modulation only when present) and effectively captures the profile and weight of spatial gain fields across the environment. When applied to V1 recordings from mice navigating the virtual corridor, the model isolated significant spatial components in a substantial fraction of V1 neurons. The recovered spatial components exhibited heterogeneous, often multi-peaked, profiles. Application of this framework to large-scale recordings may provide a robust approach to characterize the nature of spatial signals modulating sensory processing across brain areas.",Neuroscience,http://arxiv.org/abs/2512.01962v1,arXiv,0
"Recent studies suggest that the representations learned by large language models (LLMs) are partially aligned to those of the human brain. However, whether and why this alignment score arises from a similar sequence of computations remains elusive. In this study, we explore this question by examining temporally-resolved brain signals of participants listening to 10 hours of an audiobook. We study these neural dynamics jointly with a benchmark encompassing 22 LLMs varying in size and architecture type. Our analyses confirm that LLMs and the brain generate representations in a similar order: specifically, activations in the initial layers of LLMs tend to best align with early brain responses, while the deeper layers of LLMs tend to best align with later brain responses. This brain-LLM alignment is consistent across transformers and recurrent architectures. However, its emergence depends on both model size and context length. Overall, this study sheds light on the sequential nature of computations and the factors underlying the partial convergence between biological and artificial neural networks.",Neuroscience,http://arxiv.org/abs/2512.01591v1,arXiv,0
"High-definition transcranial direct current stimulation (HD-tDCS) dosing in children remains largely empirical, relying on one-size-fits-all protocols despite rapid developmental changes in head anatomy and tissue properties that strongly modulate how currents reach the developing brain. Using 70 pediatric head models and commonly used cortical targets, our forward simulations find that standard montages produce marked age-dependent reductions in target electric-field intensity and systematic sex differences linked to tissue-volume covariation, underscoring the profound limitations of conventional uniform montages. To overcome these limitations, we introduce a developmentally informed, dual-objective optimization framework designed to generate personalized Pareto fronts summarizing the trade-off between electric-field intensity and focality. From these optimized solutions, we derive two practical dosing prescriptions: a dose-consistency strategy that, for the first time, enforces fixed target intensity across individuals to implicitly mitigate demographic effects, and a target-engagement strategy that maximizes target intensity under safety limits. Both strategies remain robust to large conductivity variations, and we further show that dense HD-tDCS solutions admit sparse equivalents without performance loss under the target-engagement strategy. We also find that tissue conductivity sensitivity is depth-dependent, with Pareto-front distributions for superficial cortical targets most influenced by gray matter, scalp, and bone conductivities, and those for a deep target predominantly shaped by gray and white matter conductivities. Together, these results establish a principled framework for pediatric HD-tDCS planning that explicitly accounts for developmental anatomy and physiological uncertainty, enabling reliable and individualized neuromodulation dosing in pediatric populations.",Neuroscience,http://arxiv.org/abs/2512.01406v1,arXiv,0
"Neurons process information in ways that depend on their cell type, connectivity, and the brain region in which they are embedded. However, inferring these factors from neural activity remains a significant challenge. To build general-purpose representations that allow for resolving information about a neuron's identity, we introduce NuCLR, a self-supervised framework that aims to learn representations of neural activity that allow for differentiating one neuron from the rest. NuCLR brings together views of the same neuron observed at different times and across different stimuli and uses a contrastive objective to pull these representations together. To capture population context without assuming any fixed neuron ordering, we build a spatiotemporal transformer that integrates activity in a permutation-equivariant manner. Across multiple electrophysiology and calcium imaging datasets, a linear decoding evaluation on top of NuCLR representations achieves a new state-of-the-art for both cell type and brain region decoding tasks, and demonstrates strong zero-shot generalization to unseen animals. We present the first systematic scaling analysis for neuron-level representation learning, showing that increasing the number of animals used during pretraining consistently improves downstream performance. The learned representations are also label-efficient, requiring only a small fraction of labeled samples to achieve competitive performance. These results highlight how large, diverse neural datasets enable models to recover information about neuron identity that generalize across animals. Code is available at https://github.com/nerdslab/nuclr.",Neuroscience,http://arxiv.org/abs/2512.01199v1,arXiv,0
"The Machine Consciousness Hypothesis states that consciousness is a substrate-free functional property of computational systems capable of second-order perception. I propose a research program to investigate this idea in silico by studying how collective self-models (coherent, self-referential representations) emerge from distributed learning systems embedded within universal self-organizing environments. The theory outlined here starts from the supposition that consciousness is an emergent property of collective intelligence systems undergoing synchronization of prediction through communication. It is not an epiphenomenon of individual modeling but a property of the language that a system evolves to internally describe itself. For a model of base reality, I begin with a minimal but general computational world: a cellular automaton, which exhibits both computational irreducibility and local reducibility. On top of this computational substrate, I introduce a network of local, predictive, representational (neural) models capable of communication and adaptation. I use this layered model to study how collective intelligence gives rise to self-representation as a direct consequence of inter-agent alignment. I suggest that consciousness does not emerge from modeling per se, but from communication. It arises from the noisy, lossy exchange of predictive messages between groups of local observers describing persistent patterns in the underlying computational substrate (base reality). It is through this representational dialogue that a shared model arises, aligning many partial views of the world. The broader goal is to develop empirically testable theories of machine consciousness, by studying how internal self-models may form in distributed systems without centralized control.",Neuroscience,http://arxiv.org/abs/2512.01081v1,arXiv,0
"We propose that consciousness arises from a single control agent, the Modeler-schema. It monitors the brain's Modeler as that system constructs and updates the internal World Model. As part of that monitoring, the Modeler-schema generates experience by applying a qualia-based consistency check to the Modeler's output. The Human Agent comprises three cooperating agents: Modeler, Controller, and Targeter, each paired with an associated regulatory ""schema"" agent. We also describe fast-Modelers and fast-Controllers; evolutionary shortcuts whose rapid actions will precede awareness. Our core prediction is that the Modeler-schema performs a qualia-based consistency check during saccades and issues a bottom-up target when a discrepancy is found. To test this prediction, we propose a saccadic change-detection experiment that distinguishes Modeler-generated from Modeler-schema-generated targets. Locating qualia in the Modeler-schema ties experience to the regulation and refinement of internal representations, clarifies how awareness arises from model control, and suggests a path toward empirical falsification, thereby offering a concrete, testable proposal toward solving the Hard Problem of consciousness.",Neuroscience,http://arxiv.org/abs/2512.01073v1,arXiv,0
"We investigate a conductance-based neuron model to explore how voltage-gated ion channel isoforms influence action-potential generation. The model combines a six-state Markov representation of NaV channels with a first-order KV3.1 model, allowing us to vary maximal sodium and potassium conductances and compare nine NaV isoforms. Using bifurcation theory and local stability analysis, we map regions of stable limit cycles and visualize excitability landscapes via heatmap-based diagrams. These analyses show that isoforms NaV1.3, NaV1.4 and NaV1.6 support broad excitable regimes, while isoforms NaV1.7 and NaV1.9 exhibit minimal oscillatory behavior. Our findings provide insights into the role of channel heterogeneity in neuronal dynamics and may help to guide the design of synthetic excitable systems by narrowing the parameter space needed for robust action-potential trains.",Neuroscience,http://arxiv.org/abs/2512.01058v2,arXiv,0
"In reservoir computing, the coupling strength of the initial untrained recurrent neural network (the reservoir) is an important hyperparameter that can be varied for accurate training. A common heuristic is to set this parameter near the ``edge of chaos"", where the untrained reservoir is near the transition to chaotic dynamics, and the chaos can be ``tamed"". Here, we investigate how the overall connectivity strength should be varied in threshold power-law recurrent neural networks, where the firing rate is 0 below some threshold of the current and is a power function of the current above this threshold. These networks have been previously shown to exhibit chaotic solutions for very small coupling strengths, which may imply that the chaos cannot be tamed at all. We show that for reservoirs constructed with threshold power-law transfer functions, if the reservoir can be trained for one single positive value of the initial reservoir coupling strength, then there exist networks with identical accuracy for all positive coupling strengths, implying that the chaotic dynamics can always be tamed or never be tamed. This is a direct consequence of the coupling strength of threshold power-law RNNs acting as a scale parameter that does not qualitatively influence the dynamics of the system, but only scales all system solutions in magnitude. This is independent of the power of the transfer function, with the exception of Rectified Linear Unit (ReLU) networks. This is in contrast with conventional RNNs/reservoirs employing sigmoidal firing rates, where the strength of the recurrent coupling in the initial reservoir determines the performance on different tasks during training and also influences the network dynamics explicitly.",Neuroscience,http://arxiv.org/abs/2512.01006v1,arXiv,0
"Many systems of interest exhibit nested emergent layers with their own rules and regularities, and our knowledge about them seems naturally organised around these levels. This paper proposes that this type of hierarchical emergence arises as a result of underlying symmetries. By combining principles from information theory, group theory, and statistical mechanics, one finds that dynamical processes that are equivariant with respect to a symmetry group give rise to emergent macroscopic levels organised into a hierarchy determined by the subgroups of the symmetry. The same symmetries happen to also shape Bayesian beliefs, yielding hierarchies of abstract belief states that can be updated autonomously at different levels of resolution. These results are illustrated in Hopfield networks and Ehrenfest diffusion, showing that familiar macroscopic quantities emerge naturally from their symmetries. Together, these results suggest that symmetries provide a fundamental mechanism for emergence and support a structural correspondence between objective and epistemic processes, making feasible inferential problems that would otherwise be computationally intractable.",Neuroscience,http://arxiv.org/abs/2512.00984v1,arXiv,0
"Early detection of malignant lung nodules is critical, but its dependence on size and growth in screening inherently delays diagnosis. We present an AI system that redefines lung cancer screening by performing both detection and malignancy diagnosis directly at the nodule level on low-dose CT scans. To address limitations in dataset scale and explainability, we designed an ensemble of shallow deep learning and feature-based specialized models. Trained and evaluated on 25,709 scans with 69,449 annotated nodules, the system outperforms radiologists, Lung-RADS, and leading AI models (Sybil, Brock, Google, Kaggle). It achieves an area under the receiver operating characteristic curve (AUC) of 0.98 internally and 0.945 on an independent cohort. With 0.5 false positives per scan at 99.3\% sensitivity, it addresses key barriers to AI adoption. Critically, it outperforms radiologists across all nodule sizes and stages, excelling in stage 1 cancers, and all growth-based metrics, including the least accurate: Volume-Doubling Time. It also surpasses radiologists by up to one year in diagnosing indeterminate and slow-growing nodules.",Neuroscience,http://arxiv.org/abs/2512.00281v1,arXiv,0
"Many tasks require mapping continuous input data (e.g. images) to discrete task outputs (e.g. class labels). Yet, how neural networks learn to perform such discrete computations on continuous data manifolds remains poorly understood. Here, we show that signatures of such computations emerge in the representational geometry of neural networks as they learn. By analysing the Riemannian pullback metric across layers of a neural network, we find that network computation can be decomposed into two functions: discretising continuous input features and performing logical operations on these discretised variables. Furthermore, we demonstrate how different learning regimes (rich vs. lazy) have contrasting metric and curvature structures, affecting the ability of the networks to generalise to unseen inputs. Overall, our work provides a geometric framework for understanding how neural networks learn to perform discrete computations on continuous manifolds.",Neuroscience,http://arxiv.org/abs/2512.00196v1,arXiv,0
"Contemporary ML separates the static structure of parameters from the dynamic flow of inference, yielding systems that lack the sample efficiency and thermodynamic frugality of biological cognition. In this theoretical work, we propose \textbf{Memory-Amortized Inference (MAI)}, a formal framework rooted in algebraic topology that unifies learning and memory as phase transitions of a single geometric substrate. Central to our theory is the \textbf{Homological Parity Principle}, which posits a fundamental dichotomy: even-dimensional homology ($H_{even}$) physically instantiates stable \textbf{Content} (stable scaffolds or ``what''), while odd-dimensional homology ($H_{odd}$) instantiates dynamic \textbf{Context} (dynamic flows or ``where''). We derive the logical flow of MAI as a topological trinity transformation: \textbf{Search $\to$ Closure $\to$ Structure}. Specifically, we demonstrate that cognition operates by converting high-complexity recursive search (modeled by \textit{Savitch's Theorem} in NPSPACE) into low-complexity lookup (modeled by \textit{Dynamic Programming} in P) via the mechanism of \textbf{Topological Cycle Closure}. We further show that this consolidation process is governed by a topological generalization of the Wake-Sleep algorithm, functioning as a coordinate descent that alternates between optimizing the $H_{odd}$ flow (inference/wake) and condensing persistent cycles into the $H_{even}$ scaffold (learning/sleep). This framework offers a rigorous explanation for the emergence of fast-thinking (intuition) from slow-thinking (reasoning) and provides a blueprint for post-Turing architectures that compute via topological resonance.",Neuroscience,http://arxiv.org/abs/2512.05990v1,arXiv,0
"Computation fundamentally separates time from space: nondeterministic search is exponential in time but polynomially simulable in space (Savitch's Theorem). We propose that the brain physically instantiates a biological variant of this theorem through Memory-Amortized Inference (MAI), creating a geometry of certainty from the chaos of exploration. We formalize the cortical algorithm as a recursive topological transformation of flow into scaffold:$H_{odd}^{(k)} \xrightarrow{\text{Condense}} H_{even}^{(k+1)}$, where a stable, high-frequency cycle ($Î²_1$) at level $k$ is collapsed into a static atomic unit ($Î²_0$) at level $k+1$. Through this Topological Trinity (Search $\to$ Closure $\to$ Condensation), the system amortizes the thermodynamic cost of inference. By reducing complex homological loops into zero-dimensional defects (memory granules), the cortex converts high-entropy parallel search into low-entropy serial navigation. This mechanism builds a ``Tower of Scaffolds'' that achieves structural parity with the environment, allowing linear cortical growth to yield exponential representational reach. However, this efficiency imposes a strict limit: the same metric contraction that enables \emph{generalization} (valid manifold folding) inevitably risks \emph{hallucination} (homological collapse). We conclude that intelligence is the art of navigating this trade-off, where the ``Geometry of Certainty'' is defined by the precise threshold between necessary abstraction and topological error.",Neuroscience,http://arxiv.org/abs/2512.00140v1,arXiv,0
"Starting from the foundational axiomatization of the perceptual color space initiated by SchrÃ¶dinger in 1920 and eventually refined by Resnikoff in 1974, Berthier, Provenzi and their collaborators have recently proposed a reformulation of perceptual color attributes within the framework of quantum information. Their work is based on the Jordan algebra formalism of quantum theories and, more specifically, on a quantum system described by a spin factor over the field of real numbers. This theoretical framework is not that of ordinary quantum mechanics, mainly because it requires dealing with rebits, whereas the latter uses qubits. The aim of this paper is to show that this difference in no way hinders the implementation of experimental protocols for testing the validity of the predictions of the color perception model. In particular, we show how to compute the quantum information based perceptual attributes of perceived colors in terms of qubit density matrices.",Neuroscience,http://arxiv.org/abs/2512.00132v1,arXiv,0
"Generating whole-brain 4D fMRI sequences conditioned on cognitive tasks remains challenging due to the high-dimensional, heterogeneous BOLD dynamics across subjects/acquisitions and the lack of neuroscience-grounded validation. We introduce the first diffusion transformer for voxelwise 4D fMRI conditional generation, combining 3D VQ-GAN latent compression with a CNN-Transformer backbone and strong task conditioning via AdaLN-Zero and cross-attention. On HCP task fMRI, our model reproduces task-evoked activation maps, preserves the inter-task representational structure observed in real data (RSA), achieves perfect condition specificity, and aligns ROI time-courses with canonical hemodynamic responses. Performance improves predictably with scale, reaching task-evoked map correlation of 0.83 and RSA of 0.98, consistently surpassing a U-Net baseline on all metrics. By coupling latent diffusion with a scalable backbone and strong conditioning, this work establishes a practical path to conditional 4D fMRI synthesis, paving the way for future applications such as virtual experiments, cross-site harmonization, and principled augmentation for downstream neuroimaging models.",Neuroscience,http://arxiv.org/abs/2511.22870v1,arXiv,0
"We analyze continuous Hopfield associative memories augmented by additional, rapid short-term associative synaptic plasticity. Through the cavity method, we determine the boundary between the retrieval and forgetting, or spin-glass phase, of the network as a function of the fraction of stored memories and the neuronal gain. We find that short-term synaptic plasticity yields marginal improvements in critical memory capacity. However, through dynamical mean field theory, backed by extensive numerical simulations, we find that short-term synaptic plasticity has a dramatic impact on memory retrieval above the critical capacity. When short-term synaptic plasticity is turned on, the combined neuronal and synaptic dynamics descends a high-dimensional energy landscape over both neurons and synapses. The energy landscape over neurons alone is thus dynamic, and is lowered in the vicinity of recent neuronal patterns visited by the network, just like the surface of a trampoline is lowered in the vicinity of regions recently visited by a heavy ball. This trampoline-like reactivity of the neuronal energy landscape to short-term plasticity in synapses can lead to the recall of stored memories that would otherwise have been forgotten. This occurs because the dynamics without short-term plasticity transiently moves towards a stored memory before departing away from it. Thus short-term plasticity, operating during the transient, lowers the energy in the vicinity of the stored memory, eventually trapping the combined neuronal and synaptic dynamics at a fixed point close to the stored memory. In this manner, short-term plasticity enables the recall of memories that would otherwise be forgotten, by trapping transients that would otherwise escape. We furthermore find an optimal time constant for short-term synaptic plasticity, matched to the transient dynamics, to empower recall of forgotten memories.",Neuroscience,http://arxiv.org/abs/2511.22848v1,arXiv,0
"To understand how neural systems process information, it is often essential to compare one circuit with another, one brain with another, or data with a model. Traditional similarity measures ignore the dynamical processes underlying neural representations. Dynamical similarity methods offer a framework to compare the temporal structure of dynamical systems by embedding their (possibly) nonlinear dynamics into a globally linear space and there computing conjugacy metrics. However, identifying the best embedding and computing these metrics can be computationally slow. Here we introduce fast Dynamical Similarity Analysis (fastDSA), which is computationally far more efficient than previous methods while maintaining their accuracy and robustness. FastDSA introduces two key components that boost efficiency: (1) automatic selection of the effective model order of the Hankel (delay) embedding from the data via a data-driven singular-value threshold that identifies the informative subspace and discards noise to lower computational cost without sacrificing signal, and (2) a novel optimization procedure and objective, which replaces the slow exact orthogonality constraint in finding a minimal distance between dynamics matrices with a lightweight process to keep the search close to the space of orthogonal transformations. We demonstrate that fastDSA is at least an order of magnitude faster than the previous methods. Furthermore, we demonstrate that fastDSA has the properties of its ancestor, including its invariances and sensitivities to system dynamics. FastDSA, therefore, provides a computationally efficient and accurate method for dynamical similarity analysis.",Neuroscience,http://arxiv.org/abs/2511.22828v1,arXiv,0
"This short note comments on \citet{Aerts2024Origin}, which proposes that ranked word frequencies in texts should be read through the lens of Bose--Einstein (BE) statistics and even used to illuminate the origin of quantum statistics in physics. The core message here is modest: the paper offers an interesting analogy and an eye-catching fit, but several key steps mix physical claims with definitions and curve-fitting choices. We highlight three such points: (i) a normalization issue that is presented as ""bosonic enhancement"", (ii) an identification of rank with energy that makes the BE fit only weakly diagnostic of an underlying mechanism, and (iii) a baseline comparison that is too weak to support an ontological conclusion. We also briefly flag a few additional concerns (interpretation drift, parameter semantics, and reproducibility).",Neuroscience,http://arxiv.org/abs/2512.07881v1,arXiv,0
"In real life, psychological and physiological states rarely change along a single dimension. Through self-tracking and discussions with clinicians, I have come to recognise with increasing clarity that sleep patterns, autonomic arousal, bodily sensations, and cognitive load are in constant interaction. Existing models often fail to capture this complexity. Many theoretical frameworks continue to analyse these elements in isolation, making it difficult to explain sudden changes reported by individuals,such as abrupt spikes in anxiety, sudden drops in dissociation, or even moments of heightened alertness. The mathematical modelling employed herein does not replace clinical or subjective narratives, but rather provides a structural framework for these rapid transitions and elucidates why bodily-driven and cognitively-driven changes manifest differently. The objective is to build a conceptual bridge between physiological signals and lived experience, laying the groundwork for dynamic modelling and future case analyses.",Neuroscience,http://arxiv.org/abs/2512.00109v1,arXiv,0
"Area V4 is a mid-level stage of the macaque ventral visual stream, known to encode intermediate visual features such as color, curvature, corners, texture, three-dimensional (3D) solids, and local form. Classical neurophysiological studies have typically examined these dimensions in isolation, contrasting V4 selectivity for shape versus texture, 3D solid surfaces versus two-dimensional (2D) flat patterns, or object form versus texture. Yet how these tunings relate to one another within individual neurons, and how they are jointly organized across the cortical surface, remain unknown. For instance, does a neuron selective for 2D contour-defined shape prefer 3D solid surfaces or 2D flat surfaces? How are preferences for such heterogeneous attributes arranged in a common topographic map? To address these questions, we leverage V4 ""digital twins"" -- deep neural network models fitted to large-scale, wide-field calcium imaging data comprising tens of thousands of natural images. These digital twins allow us to systematically probe not only the stimulus dimensions explored in earlier studies, but also new, multidimensional stimulus sets that reveal additional aspects of the V4 code. In this study, we find that neural pixels preferring 2D contour-defined shapes also tend to prefer 3D surface shape defined by shading or texture gradients and by object form. In contrast, pixels preferring 2D texture tend to prefer flat surfaces defined by uniform texture or reflectance. We propose that this division of labor suggests that V4 may decompose the encoding of geometrical shape and surface appearance of visual stimuli into distinct populations of neurons, organized as interleaved clusters in the V4 topographic map.",Neuroscience,http://arxiv.org/abs/2511.22050v1,arXiv,0
"Non-invasive Brain-Computer Interfaces (BCIs) based on Code-Modulated Visual Evoked Potentials (C-VEPs) require highly robust decoding methods to address temporal variability and session-dependent noise in EEG signals. This study proposes and evaluates several deep learning architectures, including convolutional neural networks (CNNs) for 63-bit m-sequence reconstruction and classification, and Siamese networks for similarity-based decoding, alongside canonical correlation analysis (CCA) baselines. EEG data were recorded from 13 healthy adults under single-target flicker stimulation. The proposed deep models significantly outperformed traditional approaches, with distance-based decoding using Earth Mover's Distance (EMD) and constrained EMD showing greater robustness to latency variations than Euclidean and Mahalanobis metrics. Temporal data augmentation with small shifts further improved generalization across sessions. Among all models, the multi-class Siamese network achieved the best overall performance with an average accuracy of 96.89%, demonstrating the potential of data-driven deep architectures for reliable, single-trial C-VEP decoding in adaptive non-invasive BCI systems.",Neuroscience,http://arxiv.org/abs/2511.21940v1,arXiv,0
"Foundation models have shown remarkable success in fitting biological visual systems; however, their black-box nature inherently limits their utility for understanding brain function. Here, we peek inside a SOTA foundation model of neural activity (Wang et al., 2025) as a physiologist might, characterizing each 'neuron' based on its temporal response properties to parametric stimuli. We analyze how different stimuli are represented in neural activity space by building decoding manifolds, and we analyze how different neurons are represented in stimulus-response space by building neural encoding manifolds. We find that the different processing stages of the model (i.e., the feedforward encoder, recurrent, and readout modules) each exhibit qualitatively different representational structures in these manifolds. The recurrent module shows a jump in capabilities over the encoder module by 'pushing apart' the representations of different temporal stimulus patterns; while the readout module achieves biological fidelity by using numerous specialized feature maps rather than biologically plausible mechanisms. Overall, we present this work as a study of the inner workings of a prominent neural foundation model, gaining insights into the biological relevance of its internals through the novel analysis of its neurons' joint temporal response patterns.",Neuroscience,http://arxiv.org/abs/2512.07869v1,arXiv,0
"The brain has evolved to effectively control the body, and in order to understand the relationship we need to model the sensorimotor transformations underlying embodied control. As part of a coordinated effort, we are developing a general-purpose platform for behavior-driven simulation modeling high fidelity behavioral dynamics, biomechanics, and neural circuit architectures underlying embodied control. We present a pipeline for taking kinematics data from the neuroscience lab and creating a pipeline for recapitulating those natural movements in a biomechanical model. We implement a imitation learning framework to perform a dexterous forelimb reaching task with a musculoskeletal model in a simulated physics environment. The mouse arm model is currently training at faster than 1 million training steps per second due to GPU acceleration with JAX and Mujoco-MJX. We present results that indicate that adding naturalistic constraints on energy and velocity lead to simulated musculoskeletal activity that better predict real EMG signals. This work provides evidence to suggest that energy and control constraints are critical to modeling musculoskeletal motor control.",Neuroscience,http://arxiv.org/abs/2511.21848v1,arXiv,0
"Cognitive impairment in multiple sclerosis (MS) is driven by both focal inflammation and compartmentalized neurodegeneration, yet the relative effect of lesion-independent thalamic atrophy on information processing speed (IPS) remains unclear. This retrospective cohort study included 100 participants with MS. Automatic segmentation techniques quantified lesion load and delineated 26 thalamic regions of interest (ROIs). Linear models compared associations between ROI volumes and Symbol Digit Modalities Test (SDMT) performance in lesion-adjusted and unadjusted models. Twenty-one of 26 ROIs showed significant SDMT associations before lesion adjustment; twelve remained significant after adjustment. Lesion-independent associations were observed in the global thalamus, sensory relay nuclei (ventral posterolateral, medial and lateral geniculate), and associative hubs (pulvinar and mediodorsal-parafascicular complex). These intrinsically vulnerable nuclei exhibited significantly lower lesion-mediated effects (13.4%) than those losing significance after adjustment (34.2%, p < 0.001). Our findings suggest that IPS impairment reflects heterogenous contributions from both primary and secondary degeneration, with nucleus-specific phenotyping potentially informing identification of higher risk individuals.",Neuroscience,http://arxiv.org/abs/2511.21677v1,arXiv,0
"Despite remarkable technological advances, AI systems may still benefit from biological principles, such as recurrent connectivity and energy-efficient mechanisms. Drawing inspiration from the brain, we present a biologically plausible extension of the eligibility propagation (e-prop) learning rule for recurrent spiking networks. By translating the time-driven update scheme into an event-driven one, we integrate the learning rule into a simulation platform for large-scale spiking neural networks and demonstrate its applicability to tasks such as neuromorphic MNIST. We extend the model with prominent biological features such as continuous dynamics and weight updates, strict locality, and sparse connectivity. Our results show that biologically grounded constraints can inform the design of computationally efficient AI algorithms, offering scalability to millions of neurons without compromising learning performance. This work bridges machine learning and computational neuroscience, paving the way for sustainable, biologically inspired AI systems while advancing our understanding of brain-like learning.",Neuroscience,http://arxiv.org/abs/2511.21674v1,arXiv,0
"How does the brain know what is out there and what is not? Living organisms cannot rely solely on sensory signals for perception because they are noisy and ambiguous. To transform sensory signals into stable percepts, the brain uses its prior knowledge or beliefs. Current theories describe perceptual beliefs as probability distributions over the features of the stimuli, summarised by their mean and variance. Beliefs are updated by feature prediction errors: the mismatch between expected and observed feature values. This framework explains how the brain encodes unexpected changes in stimulus features (e.g., higher or lower pitch, stronger or weaker motion). How the brain updates beliefs about a stimulus' presence or absence is, however, unclear.   We propose that the detection of absence relies on a distinct form of prediction error dedicated to reducing the beliefs on stimulus occurrence. We call this signal absence prediction error. Using the human auditory system as a model for sensory processing, we developed a paradigm designed to test this hypothesis. fMRI results showed that absence prediction error is encoded in the auditory thalamus and cortex, indicating that absence is explicitly represented in subcortical sensory pathways. Moreover, while feature prediction error is already encoded in the auditory midbrain, absence prediction error was not, implying that absence-related error signals are supported by a different circuit.   These results identify a neural mechanism for the detection of sensory absence. Such mechanisms may be disrupted in conditions such as psychosis, where predictions about absence and presence are impaired.",Neuroscience,http://arxiv.org/abs/2511.21605v1,arXiv,0
"Human skin acts as a dynamic biomechanical interface that conveys critical physiological and behavioural information through spatiotemporally distributed deformations. Due to the limited capabilities of current sensing technologies, the spatiotemporal diversity of its mechanical cues has remained underutilised to date, preventing these mechanisms from being used to capture and decode the full spectrum of underlying physiological states. In this work, we define this heterogeneous set of mechanical signals as mechanodermal activity (MDA) and introduce the biomimetic metamaterial-based interface (BMMI), an engineered auxetic metamaterial substrate that reproduces the microrelief and mechanoreceptor architecture of natural skin. The BMMI allows selective capture of diverse MDA signals from adjacent skin regions with simultaneous signal amplification and noise suppression, and permits straightforward modulation to accommodate various scenarios. Combined with bespoke algorithms, the wireless BMMI device decodes MDA accurately and robustly for multimodal communication interfaces, unleashing applications in healthcare monitoring and human-machine interaction.",Neuroscience,http://arxiv.org/abs/2512.00090v1,arXiv,0
"Criticality describes a regime between order and chaos that supports flexible yet stable information processing. Here we examine whether neural dynamics can be volitionally shifted toward criticality through the self-regulation of attention. We examined ten experienced practitioners of meditation during a 10-day retreat, comparing refined states of meditative absorption, called the jhanas, to regular mindfulness of breathing. We collected electroencephalography (EEG) and physiological data during these practices and quantified the signal's dynamical properties using Lempel-Ziv complexity, signal entropy, chaoticity and long-range temporal correlations. In addition, we estimated perturbational sensitivity using a global auditory oddball mismatch negativity (MMN) during meditation. Relative to mindfulness, jhana was associated with pronounced self-reported sensory fading, slower respiration, higher neural signal diversity across multiple measures, reduced chaoticity, and enhanced MMN amplitude over frontocentral sites. Spectral analyses showed a flatter aperiodic one over f component and a frequency-specific reorganization of long-range temporal correlations. Together, increased diversity with reduced chaoticity and heightened deviance detection indicate a shift toward a metastable, near-critical regime during jhana. We propose an overlap of the phenomenology of jhana with minimal phenomenal experiences in terms of progressive attenuation of sensory content with preserved tonic alertness. Accordingly, our findings suggest that criticality is a candidate neurophysiological marker of the absorptive, minimal-content dimension of the minimal phenomenal experience.",Neuroscience,http://arxiv.org/abs/2511.20990v1,arXiv,0
"Medically uncontrolled epileptic seizures affect nearly 15 million people worldwide, resulting in enormous economic and psychological burdens. Treatment of medically refractory epilepsy is essential for patients to achieve remission, improve psychological functioning, and enhance social and vocational outcomes. Here, we show a state-of-the-art method that stabilizes fractional dynamical networks modeled from intracranial EEG data, effectively suppressing seizure activity in 34 out of 35 total spontaneous episodes from patients at the University of Pennsylvania and the Mayo Clinic. We perform a multi-scale analysis and show that the fractal behavior and stability properties of these data distinguish between four epileptic states: interictal, pre-ictal, ictal, and post-ictal. Furthermore, the simulated controlled signals exhibit substantial amplitude reduction ($49\%$ average). These findings highlight the potential of fractional dynamics to characterize seizure-related brain states and demonstrate its capability to suppress epileptic activity.",Neuroscience,http://arxiv.org/abs/2511.20950v3,arXiv,0
"Brain-computer interfaces (BCIs) are evolving from research prototypes into clinical, assistive, and performance enhancement technologies. Despite the rapid rise and promise of implantable technologies, there is a need for better and more capable wearable and non-invasive approaches whilst also minimising hardware requirements. We present a non-invasive BCI for mind-drawing that iteratively infers a subject's internal visual intent by adaptively presenting visual stimuli (probes) on a screen encoded at different flicker-frequencies and analyses the steady-state visual evoked potentials (SSVEPs). A Gabor-inspired or machine-learned policies dynamically update the spatial placement of the visual probes on the screen to explore the image space and reconstruct simple imagined shapes within approximately two minutes or less using just single-channel EEG data. Additionally, by leveraging stable diffusion models, reconstructed mental images can be transformed into realistic and detailed visual representations. Whilst we expect that similar results might be achievable with e.g. eye-tracking techniques, our work shows that symbiotic human-AI interaction can significantly increase BCI bit-rates by more than a factor 5x, providing a platform for future development of AI-augmented BCI.",Neuroscience,http://arxiv.org/abs/2511.20835v1,arXiv,0
"The primary output of the nervous system is movement and behavior. While recent advances have democratized pose tracking during complex behavior, kinematic trajectories alone provide only indirect access to the underlying control processes. Here we present MIMIC-MJX, a framework for learning biologically-plausible neural control policies from kinematics. MIMIC-MJX models the generative process of motor control by training neural controllers that learn to actuate biomechanically-realistic body models in physics simulation to reproduce real kinematic trajectories. We demonstrate that our implementation is accurate, fast, data-efficient, and generalizable to diverse animal body models. Policies trained with MIMIC-MJX can be utilized to both analyze neural control strategies and simulate behavioral experiments, illustrating its potential as an integrative modeling framework for neuroscience.",Neuroscience,http://arxiv.org/abs/2511.20532v2,arXiv,0
"Traumatic Brain Injury (TBI) results from an impact or concussion to the head with the injury being specifically characterized through pathological degradation at various biological length scales. Following injury, various mechanical modeling techniques have been proposed in the literature that seek to quantify neuronal-scale to tissue-scale metrics of brain damage. Broadly, the two categories of degradation encompass physiological deterioration of neurons and upregulation of chemical entities such as neurotransmitters which causes initiation of downstream pathophysiological effects. Despite the many contributing pathways, in this work, we delineate and model a potential glia-initiated injury pathway that leads to secondary injury. The goal of this work is to demonstrate a continuum framework which models the multiphysics of mechano-chemical interactions underlying TBI. Using a coupled PDE (partial differential equation) formulation and FEM (finite element method) discretization, the framework highlights evolution of field variables which spatio-temporally resolve mechanical metrics and chemical species across neuronal clusters. The modeling domain encompasses microglia, neurons and the extracellular matrix. The continuum framework used to model the mechano-chemical interactions assumes a three dimensional viscoelastic network to capture the mechanical response underlying proteins constituting the neuron microstructure and advection-diffusion equations modeling spatio-temporal evolution of chemical species. We use this framework to numerically estimate key concentrations of chemical species produced by the strain field. In this work, we identify key biomarkers within the labyrinth of molecular pathways and build a framework that captures the core mechano-chemical interactions. This framework is an attempt to quantify secondary injury and thus assist in developing targeted TBI treatments.",Neuroscience,http://arxiv.org/abs/2511.20392v1,arXiv,0
"Scalable assessments of mental illness remain a critical roadblock toward accessible and equitable care. Here, we show that everyday human-computer interactions encode mental health with state-of-the-art biomarker precision. We introduce MAILA, a MAchine-learning framework for Inferring Latent mental states from digital Activity. We trained MAILA on 20,000 cursor and touchscreen recordings labelled with 1.3 million mental-health self-reports collected from 9,000 participants. The dataset includes 2,000 individuals assessed longitudinally, 1,500 diagnosed with depression, and 500 with obsessive-compulsive disorder. MAILA tracks dynamic mental states along three orthogonal dimensions, identifies individuals living with mental illness, and achieves near-ceiling accuracy when predicting group-level mental health. By extracting non-verbal signatures of psychological function that have so far remained untapped, MAILA represents a key step toward scalable digital phenotyping and foundation models for mental health.",Neuroscience,http://arxiv.org/abs/2511.20179v3,arXiv,0
"Large multi-modal models (LMMs) show increasing performance in realistic visual tasks for images and, more recently, for videos. For example, given a video sequence, such models are able to describe in detail objects, the surroundings and dynamic actions. In this study, we explored the extent to which these models ground their semantic understanding in the actual visual input. Specifically, given sequences of hands interacting with objects, we asked models when and where the interaction begins or ends. For this purpose, we introduce a first of its kind, large-scale dataset with more than 20K annotated interactions on videos from the Something-Something-V2 dataset. 250 AMTurk human annotators labeled core interaction events, particularly when and where objects and agents become attached ('contact') or detached ('release'). We asked two LMMs (Qwen-2.5VL and GPT-4o) to locate these events in short videos, each with a single event. The results show that although the models can reliably name the target objects, identify the action and provide coherent reasoning, they consistently fail to identify the frame where the interaction begins or ends and cannot localize the event within the scene. Our findings suggest that in struggling to pinpoint the moment and location of physical contact that defines the interaction, the models lack the perceptual grounding required for deeper understanding of dynamic scenes.",Neuroscience,http://arxiv.org/abs/2511.20162v1,arXiv,0
"The study of cortical dynamics during different states such as decision making, sleep and movement, is an important topic in Neuroscience. Modelling efforts aim to relate the neural rhythms present in cortical recordings to the underlying dynamics responsible for their emergence. We present an effort to characterize the neural activity from the cortex of a mouse during natural sleep, captured through local field potential measurements. Our approach relies on using a discretized Wilson--Cowan Amari neural field model for neural activity, along with a data assimilation method that allows the Bayesian joint estimation of the state and parameters. We demonstrate the feasibility of our approach on synthetic measurements before applying it to a dataset available in literature. Our findings suggest the potential of our approach to characterize the stimulus received by the cortex from other brain regions, while simultaneously inferring a state that aligns with the observed signal.",Neuroscience,http://arxiv.org/abs/2512.07842v1,arXiv,0
"Neuroeconomics promises to ground welfare analysis in neural and computational evidence about how people value outcomes, learn from experience and exercise self-control. At the same time, policy and commercial actors increasingly invoke neural data to justify paternalistic regulation, ""brain-based"" interventions and new welfare measures. This paper asks under what conditions neural data can legitimately inform welfare judgements for policy rather than merely describing behaviour. I develop a non-empirical, model-based framework that links three levels: neural signals, computational decision models and normative welfare criteria. Within an actor-critic reinforcement-learning model, I formalise the inference path from neural activity to latent values and prediction errors and then to welfare claims. I show that neural evidence constrains welfare judgements only when the neural-computational mapping is well validated, the decision model identifies ""true"" interests versus context-dependent mistakes, and the welfare criterion is explicitly specified and defended. Applying the framework to addiction, neuromarketing and environmental policy, I derive a Neuroeconomic Welfare Inference Checklist for regulators and for designers of NeuroAI systems. The analysis treats brains and artificial agents as value-learning systems while showing that internal reward signals, whether biological or artificial, are computational quantities and cannot be treated as welfare measures without an explicit normative model.",Neuroscience,http://arxiv.org/abs/2511.19548v1,arXiv,0
"Stroke is a leading cause of disability and death worldwide, with ischemic strokes accounting for nearly 80% of cases. Fewer than 5% of patients receive the sole validated pharmacotherapy, intravenous thrombolysis, highlighting the urgent need for novel therapies. Within this landscape, the exploration of natural molecules emerges as a promising avenue, particularly as a means to address limitations associated with conventional drugs. Nutraceuticals, bioactive compounds derived from food sources, offer a compelling prospect for health and wellness. The term nutraceutical reflects their dual potential in nutrition and pharmacotherapy, emphasizing their relevance to both disease prevention and treatment. Interestingly, many were initially recognized as ''natural preconditioners'', substances that prime the body for protection against stress or damage. In fact, numerous nutraceuticals have been shown to activate protective pathways similar to those triggered by preconditioning across various organs. Among nutraceuticals, omega-3 polyunsaturated fatty acids sourced from plants or fish, along with polyphenols, have emerged as particularly promising. Their consumption has been associated with a reduced risk of ischemic stroke, supported by numerous preclinical studies demonstrating their beneficial effects on cellular components within the neurovascular unit. This review explores the shared protective mechanisms of various nutraceuticals against key drivers of ischemic injury, including excitotoxicity, oxidative stress, apoptosis, and inflammation. By delineating these actions, the review highlights the potential of nutraceuticals as brain preconditioners that enhance neuroprotection, thereby mitigating the impact of cerebral ischemia in both preventive and therapeutic contexts.",Neuroscience,http://arxiv.org/abs/2511.18853v1,arXiv,0
"Sleep disorder is a serious global public health issue, with cognitive-emotional dysfunction being a core symptom. The analysis of multimodal MRI data provides an effective method for detecting sleep deprivation-induced neural network abnormalities. The structure-function coupling (SC-FC) integrates functional connectivity with white matter structural information, which can enable comprehensive detection of brain network abnormalities and offer quantitative measures of sleep deprivation-induced neural damage. This study integrates diffusion tensor imaging (DTI) and resting-state fMRI (rs-fMRI) to systematically investigate brain network reorganization and their relationship with emotional functions in partial sleep deprivation (PSD). Our methodology employed DTI to construct structural connectivity (SC) networks and rs-fMRI to establish functional connectivity (FC) networks, then construct SC-FC coupling model . The experiment included 16 healthy controls (HC) and 20 PSD patients, with comprehensive whole-brain and nodal-level SC-FC analyses performed. The results show that (1) severe FC disruptions in PSD patients involving the limbic system, default mode network, sensorimotor network, and visual networks; (2) altered SC in default mode, sensorimotor, visual, language, and auditory networks; (3) significant SC-FC decoupling in these networks; and (4) strong correlations between these neural changes and clinical measures (KSQ and HADS scores). The SC-FC coupling approach achieved comprehensive detection of PSD-related network abnormalities. Compared to single-modal approaches, this integrated SC-FC analysis provides more comprehensive biomarkers for sleep-related emotional dysregulation. This innovative multimodal neuroimaging approach elucidates the neural mechanisms of SC-FC imbalance induced by PSD, establishing novel biomarkers for sleep-mediated emotional dysregulation.",Neuroscience,http://arxiv.org/abs/2512.00063v1,arXiv,0
"Understanding how glioblastoma (GBM) emerges from initially healthy glial tissue requires models that integrate bioelectrical, metabolic, and multicellular dynamics. This work introduces an ASAL-inspired agent-based framework that simulates bioelectric state transitions in glial cells as a function of mitochondrial efficiency (Meff), ion-channel conductances, gap-junction coupling, and ROS dynamics. Using a 64x64 multicellular grid over 60,000 simulation steps, we show that reducing Meff below a critical threshold (~0.6) drives sustained depolarization, ATP collapse, and elevated ROS, reproducing key electrophysiological signatures associated with GBM. We further apply evolutionary optimization (genetic algorithms and MAP-Elites) to explore resilience, parameter sensitivity, and the emergence of tumor-like attractors. Early evolutionary runs converge toward depolarized, ROS-dominated regimes characterized by weakened electrical coupling and altered ionic transport. These results highlight mitochondrial dysfunction and disrupted bioelectric signaling as sufficient drivers of malignant-like transitions and provide a computational basis for probing the bioelectrical origins of oncogenesis.",Neuroscience,http://arxiv.org/abs/2511.19520v1,arXiv,0
"Psychedelics, such as psilocybin, reorganise large-scale brain connectivity, yet how these changes are reflected across electrophysiological (electroencephalogram, EEG) and haemodynamic (functional magnetic resonance imaging, fMRI) networks remains unclear. We present Brain-MGF, a multimodal graph fusion network for joint EEG-fMRI connectivity analysis. For each modality, we construct graphs with partial-correlation edges and Pearson-profile node features, and learn subject-level embeddings via graph convolution. An adaptive softmax gate then fuses modalities with sample-specific weights to capture context-dependent contributions. Using the world's largest single-site psilocybin dataset, PsiConnect, Brain-MGF distinguishes psilocybin from no-psilocybin conditions in meditation and rest. Fusion improves over unimodal and non-adaptive variants, achieving 74.0% accuracy and 76.5% F1 score on meditation, and 76.0% accuracy with 85.8% ROC-AUC on rest. UMAP visualisations reveal clearer class separation for fused embeddings. These results indicate that adaptive graph fusion effectively integrates complementary EEG-fMRI information, providing an interpretable framework for characterising psilocybin-induced alterations in large-scale neural organisation.",Neuroscience,http://arxiv.org/abs/2511.18325v1,arXiv,0
"Neural decoding from electroencephalography (EEG) remains fundamentally limited by poor generalization to unseen subjects, driven by high inter-subject variability and the lack of large-scale datasets to model it effectively. Existing methods often rely on synthetic subject generation or simplistic data augmentation, but these strategies fail to scale or generalize reliably. We introduce \textit{MultiDiffNet}, a diffusion-based framework that bypasses generative augmentation entirely by learning a compact latent space optimized for multiple objectives. We decode directly from this space and achieve state-of-the-art generalization across various neural decoding tasks using subject and session disjoint evaluation. We also curate and release a unified benchmark suite spanning four EEG decoding tasks of increasing complexity (SSVEP, Motor Imagery, P300, and Imagined Speech) and an evaluation protocol that addresses inconsistent split practices in prior EEG research. Finally, we develop a statistical reporting framework tailored for low-trial EEG settings. Our work provides a reproducible and open-source foundation for subject-agnostic EEG decoding in real-world BCI systems.",Neuroscience,http://arxiv.org/abs/2511.18294v1,arXiv,0
"We propose a framework for constructing combinatorial complexes (CCs) from fMRI time series data that captures both pairwise and higher-order neural interactions through information-theoretic measures, bridging topological deep learning and network neuroscience. Current graph-based representations of brain networks systematically miss the higher-order dependencies that characterize neural complexity, where information processing often involves synergistic interactions that cannot be decomposed into pairwise relationships. Unlike topological lifting approaches that map relational structures into higher-order domains, our method directly constructs CCs from statistical dependencies in the data. Our CCs generalize graphs by incorporating higher-order cells that represent collective dependencies among brain regions, naturally accommodating the multi-scale, hierarchical nature of neural processing. The framework constructs data-driven combinatorial complexes using O-information and S-information measures computed from fMRI signals, preserving both pairwise connections and higher-order cells (e.g., triplets, quadruplets) based on synergistic dependencies. Using NetSim simulations as a controlled proof-of-concept dataset, we demonstrate our CC construction pipeline and show how both pairwise and higher-order dependencies in neural time series can be quantified and represented within a unified structure. This work provides a framework for brain network representation that preserves fundamental higher-order structure invisible to traditional graph methods, and enables the application of topological deep learning (TDL) architectures to neural data.",Neuroscience,http://arxiv.org/abs/2511.20692v1,arXiv,0
"Finding food is a fundamental activity for survival of all living organisms. Free-ranging dogs have been known to use their olfaction to assess the quality and type of available food but their use of visual ability in foraging is not well-documented. In the current study, we seek to remedy that by testing free-ranging dogs in a food-based choice test. We tested whether the dogs implemented hierarchical or synergistic usage of cues while finding food. We found limited prioritization of olfactory cues over visual cues in dichromatic choice tests but in phases with similar perceptual elements, the sensory choice was not clear. Furthermore, free-ranging dogs display a dynamic decision-making in unpredictable urban environments adopting a good-enough strategy during foraging. They prefer speed over accuracy, settling for intermediate quality food if their preferred food item is not available. These dogs also displayed left-bias during food choice. In multi-sensorial, natural setting multiple modulators like environmental noise, risk, and internal perceptual elements apart from food cues seem to be affecting the decision-making in dogs.",Neuroscience,http://arxiv.org/abs/2512.00058v1,arXiv,0
"Current models treat physiological signals as noise corrupting neural computation. Previously, we showed that removing these ""artifacts"" eliminates 70% of predictive correlation, suggesting body signals functionally drive cognition. Here, we investigate the mechanism using high-density EEG (64 channels, 10 subjects, 500+ trials) during P300 target recognition.   Phase Slope Index revealed zero-lag synchrony (PSI=0.000044, p=0.061) with high coherence (0.316, p<0.0001). Ridge-regularized Granger causality showed massive bidirectional coupling (F=100.53 brain-to-body, F=62.76 body-to-brain) peaking simultaneously at 78.1ms, consistent with mutually coupled resonance pairs.   Time-resolved entropy analysis (200ms windows, 25ms steps) revealed triphasic dynamics: (1) constraint accumulation (0-78ms) building causal drive without entropy change (delta-S=-0.002 bits, p=0.75); (2) supercritical transition (100-600ms) triggering state expansion (58% directional increase, binomial p=0.002); (3) sustained metastability. Critically, transition magnitude was uncorrelated with resonance strength (r=-0.044, p=0.327), indicating binary threshold dynamics.   Understanding emerges through a thermodynamic sequence: brain-body resonance acts as a discrete gate triggering non-linear information integration. This architecture may fundamentally distinguish biological from artificial intelligence.   Keywords: embodied cognition, phase transitions, Granger causality, thermodynamics, neuromorphic computing, resonance dynamics, EEG artifacts",Neuroscience,http://arxiv.org/abs/2511.18057v1,arXiv,0
"Large language models (LLMs) increasingly mediate human decision-making and behaviour. Ensuring LLM processing of moral meaning therefore has become a critical challenge. Current approaches rely predominantly on bottom-up methods such as fine-tuning and reinforcement learning from human feedback. We propose a fundamentally different approach: embedding moral meaning processing directly into the architectural mechanisms and frameworks of transformer-based models through top-down design principles. We first sketch a framework that conceptualizes attention as a dynamic interface mediating between structure and processing, contrasting with existing linear attention frameworks in psychology. We start from established biological-artificial attention analogies in neural architecture design to improve cognitive processing. We extend this analysis to moral processing, using Iris Murdoch's theory of loving attention (sustained, just observation that enables moral transformation by reseeing others with clarity and compassion) to philosophically discuss functional analogies between human and LLM moral processing. We formulate and evaluate potentially promising technical operationalizations to embed morality in LLM architectures and frameworks. We acknowledge the limitations of our exploration and give three key contributions. (1) We conceptualize attention as a dynamic system mechanism mediating between structure and processing. (2) Drawing on the Murdoch notion of loving attention, we outline technical pathways for embedding morality in LLMs, through modified training objectives, runtime weight adjustments, and architectural refinements to attention. (3) We argue that integrating morality into architectures and frameworks complements external, constraint-based methods. We conclude with a call for collaboration between transformer designers and philosophers engaged in AI ethics.",Neuroscience,http://arxiv.org/abs/2511.20689v1,arXiv,0
"Accurate simulations of electric fields (E-fields) in brain stimulation depend on tissue conductivity representations that link macroscopic assumptions with underlying microscopic tissue structure. Mesoscale conductivity variations can produce meaningful changes in E-fields and neural activation thresholds but remain largely absent from standard macroscopic models. Recent microscopic models have suggested substantial local E-field perturbations and could, in principle, inform mesoscale conductivity. However, the quantitative validity of microscopic models is limited by fixation-related tissue distortion and incomplete extracellular-space reconstruction. We outline approaches that bridge macro- and microscales to derive consistent mesoscale conductivity distributions, providing a foundation for accurate multiscale models of E-fields and neural activation in brain stimulation.",Neuroscience,http://arxiv.org/abs/2511.16465v4,arXiv,0
"The big strides seen in generative AI are not based on somewhat obscure algorithms, but due to clearly defined generative principles. The resulting concrete implementations have proven themselves in large numbers of applications. We suggest that it is imperative to thoroughly investigate which of these generative principles may be operative also in the brain, and hence relevant for cognitive neuroscience. In addition, ML research led to a range of interesting characterizations of neural information processing systems. We discuss five examples, the shortcomings of world modelling, the generation of thought processes, attention, neural scaling laws, and quantization, that illustrate how much neuroscience could potentially learn from ML research.",Neuroscience,http://arxiv.org/abs/2511.16432v1,arXiv,0
"Structural connectomes are detailed graphs that map how different brain regions are physically connected, offering critical insight into aging, cognition, and neurodegenerative diseases. However, these connectomes are high-dimensional and densely interconnected, which makes them difficult to interpret and analyze at scale. While low-dimensional spaces like PCA and autoencoders are often used to capture major sources of variation, their latent spaces are generally continuous and cannot fully reflect the mixed nature of variability in connectomes, which include both continuous (e.g., connectivity strength) and discrete factors (e.g., imaging site). Motivated by this, we propose a variational autoencoder (VAE) with a hybrid latent space that jointly models the discrete and continuous components. We analyze a large dataset of 5,761 connectomes from six Alzheimer's disease studies with ten acquisition protocols. Each connectome represents a single scan from a unique subject (3579 females, 2182 males), aged 22 to 102, with 4338 cognitively normal, 809 with mild cognitive impairment (MCI), and 614 with Alzheimer's disease (AD). Each connectome contains 121 brain regions defined by the BrainCOLOR atlas. We train our hybrid VAE in an unsupervised way and characterize what each latent component captures. We find that the discrete space is particularly effective at capturing subtle site-related differences, achieving an Adjusted Rand Index (ARI) of 0.65 with site labels, significantly outperforming PCA and a standard VAE followed by clustering (p < 0.05). These results demonstrate that the hybrid latent space can disentangle distinct sources of variability in connectomes in an unsupervised manner, offering potential for large-scale connectome analysis.",Neuroscience,http://arxiv.org/abs/2512.02032v1,arXiv,0
"In neurosciences, the brain processes information via the firing patterns of connected neurons operating across a spectrum of frequencies. To better understand the effects of these frequencies in the neuron dynamics, we have simulated a neuronal network of Izhikevich neurons to examine the interaction between frequency allocation and intermittent phase synchronization dynamics. As the synchronized population of neurons passes through a bifurcation, an additional frequency mode emerges, enabling a match in the mean frequency while retaining distinct most probable frequencies among neurons. Subsequently, the network intermittently transits between two patterns, one partially synchronized and the other unsynchronized. Through our analysis, we demonstrate that the frequency changes on the network lead to characteristic transition times between synchronization states. Moreover, these transitions adhere to beat frequency statistics when the neurons' frequencies differ by multiples of a frequency gap. Finally, our results can improve the performance in predicting transitions on problems where the beat frequency strongly influences the dynamics.",Neuroscience,http://arxiv.org/abs/2511.15985v1,arXiv,0
"In soccer penalty kicks, goalkeepers that orient their arms upward compared to downward can be misperceived as being taller - effectively recreating the Muller-Lyer illusion. The present study elaborates on previous research surrounding a potential illusion-induced bias in penalty kicks. Participants were exposed to goalkeeper configurations within a virtual goal including arms-parallel, arms-down, arms-out and arms-up. They separately judged the perceived size of the goalkeeper, and executed penalty kicks. The perceived size was near fully consistent with the intended illusion. Meanwhile, the penalty kicks indicated wider a horizontal position following arms-out, and lower vertical position following arms-up. Likewise, there was no relation between the biases expressed in perception and action. While goalkeepers can elicit a perceptual illusion, this does not extend to influencing the penalty kick itself. Instead, other contextual cues appeared more relevant including the proximity between the goalkeeper and goalposts, and with it, the available space in the goal.",Neuroscience,http://arxiv.org/abs/2511.15338v1,arXiv,0
"There is now ample evidence that Motor Imagery (MI) contributes to improve motor performance. Previous studies provided evidence that its effectiveness remains dependent upon specific guidelines and recommendations. The body posture, as well as the context in which MI is performed, are notably critical and should be carefully considered. The present study in young tennis players (n=18) was designed to compare the effectiveness of performing MI of the serve while adopting a loose grip (congruent MI) or holding tightly and squeezing hard the racket (incongruent MI). Data revealed that both MI conditions contributed to enhance the number of successful serves (p<0.001) and the technical quality of the serve (p<0.001). Interestingly, comparing mean serve accuracy scores showed that performance gains were significantly higher in the loose MI group than in the tight MI group (p<0.02). These findings confirm the critical importance of the congruence between the content of the mental representation and the features of the corresponding actual movement. Overall, the present study further highlights the effectiveness of the loose grip while mentally rehearsing the serve, and might thus contribute to update and adjust specific MI guidelines and recommendations.",Neuroscience,http://arxiv.org/abs/2511.15298v1,arXiv,0
"In the context of spiking neural networks, temporal coding of signals is increasingly preferred over the rate coding hypothesis due to its advantages in processing speed and energy efficiency. In temporal coding, synaptic delays are crucial for processing signals with precise spike timings, known as spiking motifs. Synaptic delays are however bounded in the brain and can thus be shorter than the duration of a motif. This prevents the use of motif recognition methods that consist of setting heterogeneous delays to synchronize the input spikes on a single output neuron acting as a coincidence detector. To address this issue, we developed a method to detect motifs of arbitrary length using a sequence of output neurons connected to input neurons by bounded synaptic delays. Each output neuron is associated with a sub-motif of bounded duration. A motif is recognized if all sub-motifs are sequentially detected by the output neurons. We simulated this network using leaky integrate-and-fire neurons and tested it on the Spiking Heidelberg Digits (SHD) database, that is, on audio data converted to spikes via a cochlear model, as well as on random simultaneous motifs. The results demonstrate that the network can effectively recognize motifs of arbitrary length extracted from the SHD database. Our method features a correct detection rate of about 60% in presence of ten simultaneous motifs from the SHD dataset and up to 80% for five motifs, showing the robustness of the network to noise. Results on random overlapping patterns show that the recognition of a single motif overlapping with other motifs is most effective for a large number of input neurons and sparser motifs. Our method provides a foundation for more general models for the storage and retrieval of neural information of arbitrary temporal lengths.",Neuroscience,http://arxiv.org/abs/2511.15296v1,arXiv,0
"Complex spatial connectivity patterns, such as interictal suppression and ictal propagation, complicate accurate drug-resistant epilepsy (DRE) seizure detection using stereotactic electroencephalography (SEEG) and traditional machine learning methods. Two critical challenges remain:(1)a low signal-to-noise ratio in functional connectivity estimates, making it difficult to learn seizure-related interactions; and (2)expert labels for spatial pathological connectivity patterns are difficult to obtain, meanwhile lacking the patterns' representation to improve seizure detection. To address these issues, we propose a novel node-graph dual contrastive learning framework, Seizure-NGCLNet, to learn SEEG interictal suppression and ictal propagation patterns for detecting DRE seizures with high precision. First, an adaptive graph augmentation strategy guided by centrality metrics is developed to generate seizure-related brain networks. Second, a dual-contrastive learning approach is integrated, combining global graph-level contrast with local node-graph contrast, to encode both spatial structural and semantic epileptogenic features. Third, the pretrained embeddings are fine-tuned via a top-k localized graph attention network to perform the final classification. Extensive experiments on a large-scale public SEEG dataset from 33 DRE patients demonstrate that Seizure-NGCLNet achieves state-of-the-art performance, with an average accuracy of 95.93%, sensitivity of 96.25%, and specificity of 94.12%. Visualizations confirm that the learned embeddings clearly separate ictal from interictal states, reflecting suppression and propagation patterns that correspond to the clinical mechanisms. These results highlight Seizure-NGCLNet's ability to learn interpretable spatial pathological patterns, enhancing both seizure detection and seizure onset zone localization.",Neuroscience,http://arxiv.org/abs/2512.02028v1,arXiv,0
"Working memory requires the brain to maintain information from the recent past to guide ongoing behavior. Neurons can contribute to this capacity by slowly integrating their inputs over time, creating persistent activity that outlasts the original stimulus. However, when these slowly integrating neurons are organized hierarchically, they introduce cumulative delays that create a fundamental challenge for learning: teaching signals that indicate whether behavior was correct or incorrect arrive out-of-sync with the neural activity they are meant to instruct. Here, we demonstrate that neurons enhanced with an adaptive current can compensate for these delays by responding to external stimuli prospectively -- effectively predicting future inputs to synchronize with them. First, we show that such prospective neurons enable teaching signal synchronization across a range of learning algorithms that propagate error signals through hierarchical networks. Second, we demonstrate that this successfully guides learning in slowly integrating neurons, enabling the formation and retrieval of memories over extended timescales. We support our findings with a mathematical analysis of the prospective coding mechanism and learning experiments on motor control tasks. Together, our results reveal how neural adaptation could solve a critical timing problem and enable efficient learning in dynamic environments.",Neuroscience,http://arxiv.org/abs/2511.14917v1,arXiv,0
"Empirical evidence of scaling behaviors in neuronal avalanches suggests that neuronal populations in the brain operate near criticality. Departure from scaling in neuronal avalanches has been used as a measure of distance to criticality and linked to brain disorders. A distinct line of evidence for brain criticality has come from thermodynamic signatures in maximum entropy (ME) models. Both of these approaches have been widely applied to the analysis of neuronal data. However, the relationship between deviations from avalanche criticality and thermodynamics of ME models of neuronal populations remains poorly understood. To address this question, we study spontaneous activity of organotypic rat cortex slice cultures in physiological and drug-induced hypo- or hyper-excitable conditions, which are classified as critical, subcritical and supercritical based on avalanche dynamics. We find that ME models inferred from critical cultures show signatures of criticality in thermodynamic quantities, e.g. specific heat. However, such signatures are also present, and equally strong, in models inferred from supercritical cultures -- despite their altered dynamics and poor functional performance. On the contrary, ME models inferred from subcritical cultures do not show thermodynamic hints of criticality. Importantly, we confirm these results using an interpretable neural network model that can be tuned to and away from avalanche criticality. Our findings indicate that maximum entropy models correctly distinguish subcritical from critical/supercritical systems. However, they may not be able to discriminate between avalanche criticality and supercriticality, although they may still capture a number of important features from neuronal data.",Neuroscience,http://arxiv.org/abs/2511.14872v2,arXiv,0
"Decoded Neurofeedback (DecNef) is a flourishing non-invasive approach to brain modulation with wide-ranging applications in neuromedicine and cognitive neuroscience. However, progress in DecNef research remains constrained by subject-dependent learning variability, reliance on indirect measures to quantify progress, and the high cost and time demands of experimentation.   We present DecNefLab, a modular and interpretable simulation framework that formalizes DecNef as a machine learning problem. Beyond providing a virtual laboratory, DecNefLab enables researchers to model, analyze and understand neurofeedback dynamics. Using latent variable generative models as simulated participants, DecNefLab allows direct observation of internal cognitive states and systematic evaluation of how different protocol designs and subject characteristics influence learning.   We demonstrate how this approach can (i) reproduce empirical phenomena of DecNef learning, (ii) identify conditions under which DecNef feedback fails to induce learning, and (iii) guide the design of more robust and reliable DecNef protocols in silico before human implementation.   In summary, DecNefLab bridges computational modeling and cognitive neuroscience, offering a principled foundation for methodological innovation, robust protocol design, and ultimately, a deeper understanding of DecNef-based brain modulation.",Neuroscience,http://arxiv.org/abs/2511.14555v2,arXiv,0
"In this work, the effects of dopamine neurotransmitter within the Cortico-Striatal-Thalamo-Cortical (CSTC) loop have been investigated. Simulations confirmed dopamine facilitates movement via thalamic disinhibition. Analysis of its impact on the signal-to-noise ratio (SNR) revealed a complex, region-specific outcome: SNR increased in some regions (e.g., D2 Striatum: 3.41 dB to 6.25 dB), decreased in others (e.g., Thalamus VL: 6.24 dB to 3.93 dB), and remained stable elsewhere (e.g., M1: 3.16 dB to 3.13 dB). This heterogeneity stems from dopamine increasing the excitability of D1-receptor-expressing neurons, which amplifies channel conductance noise and reduces SNR in specific circuits. Thus, dopamine acts not as a uniform signal enhancer, but as a complex modulator that critically balances facilitation and noise within the CSTC loop.",Neuroscience,http://arxiv.org/abs/2511.14466v2,arXiv,0
"Adult language learning varies greatly among individuals. Traditionally associated with frontotemporal language regions, this variability is increasingly seen as stemming from distributed brain networks. However, the role of these networks and their topological organization in explaining these differences remains unclear. We hypothesize that graph-theory-based network analysis of intrinsic multimodal connectivities across multiple networks explains overall and component-specific variations in language learning. We tested this in 101 healthy adults who underwent resting-state fMRI, structural MRI, and diffusion tensor imaging before seven days of six artificial language training tasks. We identified one dominant general learning component shared across tasks and five task-specific ones. Cross-validated predictive models used multimodal multi-network graph-theoretic metrics to predict final learning outcomes (LO) and rates (LR). We significantly predicted the LO and LR of the general component, which were primarily contributed by dorsal attention and frontoparietal networks. Nodal local efficiency was the most consistent predictor, with additional contributions from node clustering coefficient and network centrality for LR, highlighting local robustness, mesoscale network segregation, and global influence in explaining individual differences. Only task-specific word learning LO was predictable, relying on default mode and frontoparietal hubs with high betweenness centrality and efficiency. These findings demonstrate that intrinsic network topologies underlie differences in language learning success, supporting a multiple-systems hypothesis in which attentional-control networks interact with default and subcortical systems to shape learning trajectories. This advances mechanistic understanding and paves the way for personalized language education.",Neuroscience,http://arxiv.org/abs/2511.14453v1,arXiv,0
"Long COVID ""brain fog"" is a common and debilitating subjective syndrome often associated with persistent cognitive impairment after COVID-19 infection. Here we identify a specific regional brain dysfunction that mediates this cognitive impairment and provide evidence that targeted neuromodulation improves this deficit. In 120 patients with long COVID brain fog, we found an aberrant perceptual processing pattern. Patients with more severe brain fog committed significantly more false alarms (impulsive responses to non-signals) despite preserved overall accuracy. Both high-density (128-channel) EEG and structural MRI analyses provided converging evidence of a right inferior insula deficit, characterized by a blunted neural monitoring signal and cortical atrophy. We confirmed this deficit in a separate 796-participant UK Biobank longitudinal COVID re-imaging cohort, where COVID-19 survivors also showed selective impairment on a perceptual processing task and corresponding longitudinal atrophy of the right inferior insula compared with healthy controls. Finally, in a proof-of-principle randomized, sham-controlled trial (n = 40), a non-invasive, excitatory theta-burst ultrasound stimulation protocol targeting the right inferior insula rescued the perceptual deficit by reducing false alarms. These findings provide evidence of a causal role for right inferior insula dysfunction in long COVID-related perceptual impairment and show that modulation of this region can rescue the deficit, establishing it as a novel therapeutic target for long COVID cognitive impairment.",Neuroscience,http://arxiv.org/abs/2511.14188v3,arXiv,0
"The collective frequency that emerges from synchronized neuronal populations--the network resonance--shows a systematic relationship with brain size: whole-brain's large networks oscillate slowly, whereas finer parcellations of fixed volume exhibit faster rhythms. This resonance-size scaling has been reported in delayed neural mass models and human neuroimaging, yet the physical mechanism remained unresolved. Here we show that size-dependent resonance follows directly from propagation delays in delay-coupled phase oscillators. Starting from a Kuramoto model with heterogeneous delays, we linearize around the near-synchronous solution and obtain a closed-form approximation linking the resonance $Î©$ to the mean delay and the effective coupling field. The analysis predicts a generic scaling law: $Î©\approx (\sum_j c_{ij} Ï)^{-1}$, so resonance is delay-limited and therefore depends systematically on geometric size or parcellation density. We evaluate four growth scenarios--expanding geometry, fixed-volume parcellation, constant geometry, and an unphysical reference case--and show that only geometry-consistent scaling satisfies the analytical prediction. Numerical simulations with heterogeneous delays validate the law and quantify its error as a function of delay dispersion. These results identify a minimal physical mechanism for size-dependent cortical resonance and provide an analytical framework that unifies numeric simulation outputs.",Neuroscience,http://arxiv.org/abs/2511.14065v1,arXiv,0
"Human emotions are difficult to convey through words and are often abstracted in the process; however, electroencephalogram (EEG) signals can offer a more direct lens into emotional brain activity. Recent studies show that deep learning models can process these signals to perform emotion recognition with high accuracy. However, many existing approaches overlook the dynamic interplay between distinct brain regions, which can be crucial to understanding how emotions unfold and evolve over time, potentially aiding in more accurate emotion recognition. To address this, we propose RBTransformer, a Transformer-based neural network architecture that models inter-cortical neural dynamics of the brain in latent space to better capture structured neural interactions for effective EEG-based emotion recognition. First, the EEG signals are converted into Band Differential Entropy (BDE) tokens, which are then passed through Electrode Identity embeddings to retain spatial provenance. These tokens are processed through successive inter-cortical multi-head attention blocks that construct an electrode x electrode attention matrix, allowing the model to learn the inter-cortical neural dependencies. The resulting features are then passed through a classification head to obtain the final prediction. We conducted extensive experiments, specifically under subject-dependent settings, on the SEED, DEAP, and DREAMER datasets, over all three dimensions, Valence, Arousal, and Dominance (for DEAP and DREAMER), under both binary and multi-class classification settings. The results demonstrate that the proposed RBTransformer outperforms all previous state-of-the-art methods across all three datasets, over all three dimensions under both classification settings. The source code is available at: https://github.com/nnilayy/RBTransformer.",Neuroscience,http://arxiv.org/abs/2511.13954v1,arXiv,0
"Low-rank recurrent neural networks (lrRNNs) are a class of models that uncover low-dimensional latent dynamics underlying neural population activity. Although their functional connectivity is low-rank, it lacks disentanglement interpretations, making it difficult to assign distinct computational roles to different latent dimensions. To address this, we propose the Disentangled Recurrent Neural Network (DisRNN), a generative lrRNN framework that assumes group-wise independence among latent dynamics while allowing flexible within-group entanglement. These independent latent groups allow latent dynamics to evolve separately, but are internally rich for complex computation. We reformulate the lrRNN under a variational autoencoder (VAE) framework, enabling us to introduce a partial correlation penalty that encourages disentanglement between groups of latent dimensions. Experiments on synthetic, monkey M1, and mouse voltage imaging data show that DisRNN consistently improves the disentanglement and interpretability of learned neural latent trajectories in low-dimensional space and low-rank connectivity over baseline lrRNNs that do not encourage partial disentanglement.",Neuroscience,http://arxiv.org/abs/2511.13899v1,arXiv,0
"Interoception and exteroception provide continuous feedback about the body and the environment, yet how they are dynamically integrated within a unified predictive coding framework has remained under-specified. This paper develops and empirically validates an integrative predictive coding model that treats interoceptive and exteroceptive inference as parallel hierarchical systems exchanging precision-weighted prediction errors. Within this framework, arbitration between the two streams is governed by relative precision weights (w) and integrated within the anterior insula (AIC) and anterior cingulate cortex (ACC). Computational simulations of the model reproduced biologically plausible dynamics: prediction errors decayed exponentially while arbitration weights self-normalized toward equilibrium (w = 0.5), demonstrating stable convergence and coherent integration. Simulated anxiety and PTSD profiles, characterized respectively by interoceptive and exteroceptive overweighting, yielded rigid, self-sustaining imbalances (w to 1 or w to 0) and slowed recalibration. Empirical application of the arbitration equation to published EEG-fMRI datasets further validated the model. The framework contributes a unifying account of how dysregulated precision weighting may underlie anxiety (overweighted interoception) and PTSD (underweighted interoception). Building on this validation, a proposed experimental paradigm is outlined to test the model's predictions in humans. It examines recalibration across anxiety, neutral, and PTSD groups following targeted interoceptive or exteroceptive therapies. Key predictions include identifiable neural markers of coherence, modulation of heartbeat-evoked potentials by vagal stimulation, and precision-sensitive behavioral signatures in interoceptive-exteroceptive congruency tasks.",Neuroscience,http://arxiv.org/abs/2511.13668v1,arXiv,0
"Scenes are complex, yet structured collections of parts, including objects and surfaces, that exhibit spatial and semantic relations to one another. An effective visual system therefore needs unified scene representations that relate scene parts to their location and their co-occurrence. We hypothesize that this structure can be learned self-supervised from natural experience by exploiting the temporal regularities of active vision: each fixation reveals a locally-detailed glimpse that is statistically related to the previous one via co-occurrence and saccade-conditioned spatial regularities. We instantiate this idea with Glimpse Prediction Networks (GPNs) -- recurrent models trained to predict the feature embedding of the next glimpse along human-like scanpaths over natural scenes. GPNs successfully learn co-occurrence structure and, when given relative saccade location vectors, show sensitivity to spatial arrangement. Furthermore, recurrent variants of GPNs were able to integrate information across glimpses into a unified scene representation. Notably, these scene representations align strongly with human fMRI responses during natural-scene viewing across mid/high-level visual cortex. Critically, GPNs outperform architecture- and dataset-matched controls trained with explicit semantic objectives, and match or exceed strong modern vision baselines, leaving little unique variance for those alternatives. These results establish next-glimpse prediction during active vision as a biologically plausible, self-supervised route to brain-aligned scene representations learned from natural visual experience.",Neuroscience,http://arxiv.org/abs/2511.12715v1,arXiv,0
"Many properties of perceptual decision making are well-modeled by deep neural networks. However, such architectures typically treat decisions as instantaneous readouts, overlooking the temporal dynamics of the decision process. We present an image-computable model of perceptual decision making in which choices and response times arise from efficient sensory encoding and Bayesian decoding of neural spiking activity. We use a Poisson variational autoencoder to learn unsupervised representations of visual stimuli in a population of rate-coded neurons, modeled as independent homogeneous Poisson processes. A task-optimized decoder then continually infers an approximate posterior over actions conditioned on incoming spiking activity. Combining these components with an entropy-based stopping rule yields a principled and image-computable model of perceptual decisions capable of generating trial-by-trial patterns of choices and response times. Applied to MNIST digit classification, the model reproduces key empirical signatures of perceptual decision making, including stochastic variability, right-skewed response time distributions, logarithmic scaling of response times with the number of alternatives (Hick's law), and speed-accuracy trade-offs.",Neuroscience,http://arxiv.org/abs/2511.11480v2,arXiv,0
"A universal theorem of sensory information, analogous to the second law of thermodynamics, is derived. Beginning from a minimal description of a sensory neuron, a state-space representation of firing rate emerges naturally from Shannon's measure of information. A special case of this formulation predicts a previously unknown inequality governing sensory adaptation, which was confirmed across different modalities, species, and experimental conditions. Further analysis shows that the firing rate behaves like a state function in thermodynamics, leading to an entropy production equation from which a general law follows: any closed cycle of stimulation yields a non-negative net gain of sensory information.",Neuroscience,http://arxiv.org/abs/2511.11463v1,arXiv,0
"The Adaptive Exponential Integrate-and-Fire (AdEx) model is a simplified framework that effectively characterizes neuronal electrical activity. The aim of this paper is to employ phase plane analysis to systematically investigate diverse firing patterns generated by the AdEx model under varying parametric conditions. We first introduce the fundamental equations and parameter configurations of the AdEx model to numerically simulate the six representative firing patterns in the AdEx model. And then we use phase plane analysis to explore the dynamic mechanism of these firing patterns under different input currents and parametric conditions. Our findings demonstrate that the AdEx model can simulate multiple firing patterns, including Tonic Spiking, Adapting, Initial Bursting, Busting, Transient Spiking and Delayed Spiking firing patterns. These results not only advance the understanding of complex electrophysiological phenomena in neurons but also provide theoretical foundations for applications in many fields like neuromorphic computing and brain-computer interfaces.",Neuroscience,http://arxiv.org/abs/2511.20670v1,arXiv,0
"Brain-computer interface (BCI) speech decoding has emerged as a promising tool for assisting individuals with speech impairments. In this context, the integration of electroencephalography (EEG) and electromyography (EMG) signals offers strong potential for enhancing decoding performance. Mandarin tone classification presents particular challenges, as tonal variations convey distinct meanings even when phonemes remain identical. In this study, we propose a novel cross-subject multimodal BCI decoding framework that fuses EEG and EMG signals to classify four Mandarin tones under both audible and silent speech conditions. Inspired by the cooperative mechanisms of neural and muscular systems in speech production, our neural decoding architecture combines spatial-temporal feature extraction branches with a cross-attention fusion mechanism, enabling informative interaction between modalities. We further incorporate domain-adversarial training to improve cross-subject generalization. We collected 4,800 EEG trials and 4,800 EMG trials from 10 participants using only twenty EEG and five EMG channels, demonstrating the feasibility of minimal-channel decoding. Despite employing lightweight modules, our model outperforms state-of-the-art baselines across all conditions, achieving average classification accuracies of 87.83% for audible speech and 88.08% for silent speech. In cross-subject evaluations, it still maintains strong performance with accuracies of 83.27% and 85.10% for audible and silent speech, respectively. We further conduct ablation studies to validate the effectiveness of each component. Our findings suggest that tone-level decoding with minimal EEG-EMG channels is feasible and potentially generalizable across subjects, contributing to the development of practical BCI applications.",Neuroscience,http://arxiv.org/abs/2511.10935v1,arXiv,0
"Collective behavior pervades biological systems, from flocks of birds to neural assemblies and human societies. Yet, how such collectives acquire functional properties -- such as joint agency or knowledge -- that transcend those of their individual components remains an open question. Here, we combine active inference and information-theoretic analyses to explore how a minimal system of interacting agents can give rise to joint agency and collective knowledge. We model flocking dynamics using multiple active inference agents, each minimizing its own free energy while coupling reciprocally with its neighbors. We show that as agents self-organize, their interactions define higher-order statistical boundaries (Markov blankets) enclosing a ``flock'' that can be treated as an emergent agent with its own sensory, active, and internal states. When exposed to external perturbations (a ``predator''), the flock exhibits faster, coordinated responses than individual agents, reflecting collective sensitivity to environmental change. Crucially, analyses of synergistic information reveal that the flock encodes information about the predator's location that is not accessible to every individual bird, demonstrating implicit collective knowledge. Together, these results show how informational coupling among active inference agents can generate new levels of autonomy and inference, providing a framework for understanding the emergence of (implicit) collective knowledge and joint agency.",Neuroscience,http://arxiv.org/abs/2511.10835v1,arXiv,0
"Primates utilize distributed neural circuits to learn habits in uncertain environments, but the underlying mechanisms remain poorly understood. We propose a formal theory of network energetics explaining how brain states influence sequential behavior. We test our theory on multi-unit recordings from the caudate nucleus and cortical regions of macaques performing a motor habit task. The theory predicts the energy required to transition between brain states represented by trial-specific firing rates across channels, assuming activity spreads through effective connections. We hypothesized that habit formation would correlate with lower control energy. Consistent with this, we observed smaller energy requirements for transitions between similar saccade patterns and those of intermediate complexity, and sessions exploiting fewer patterns. Simulations ruled out confounds from neurons' directional tuning. Finally, virtual lesioning demonstrated robustness of observed relationships between control energy and behavior. This work paves the way for examining how behavior arises from changing activity in distributed circuitry.",Neuroscience,http://arxiv.org/abs/2511.10757v1,arXiv,0
"Animals use past experiences to adapt future behavior. To enable this rapid learning, vertebrates and invertebrates have evolved analogous neural structures like the vertebrate cerebellum or insect mushroom body. A defining feature of these circuits is a large expansion layer, which re-codes sensory inputs to improve pattern separation, a prerequisite to learn non-overlapping associations between relevant sensorimotor inputs and adaptive changes in behavior. However, classical models of associative learning treat expansion layers as static, assuming that associations are learned through plasticity at the output synapses. Here, we review emerging evidence that also highlights the importance of plasticity within the expansion layer for associative learning. Because the underlying plasticity mechanisms and principles of this representation learning are only emerging, we systematically compare experimental data from two well-studied circuits for expansion coding -- the cerebellum granule layer and the mushroom body calyx. The data indicate remarkably similar interneuron circuits, dendritic morphology and plasticity mechanisms between both systems that hint at more general principles for representation learning. Moreover, the data show strong overlap with recent theoretical advances that consider interneuron circuits and dendritic computations for representation learning. However, they also hint at an interesting interaction of stimulus-induced, non-associative and reinforced, associative mechanisms of plasticity that is not well understood in current theories of representation learning. Therefore, studying expansion layer plasticity will be important to elucidate the mechanisms and full potential of representation learning for behavioral adaptation.",Neuroscience,http://arxiv.org/abs/2511.10261v2,arXiv,0
"Organisms adapt to volatile environments by integrating sensory information with internal memory, yet their information processing is constrained by resource limitations. Such limitations can fundamentally alter optimal estimation strategies in biological systems. For example, recent experiments suggest that organisms exhibit nonmonotonic phase transitions between memoryless and memory-based estimation strategies depending on sensory reliability. However, an analytical understanding of these resource-induced phase transitions is still missing. This Letter presents an analytical characterization of resource-induced phase transitions in optimal estimation strategies. Our result identifies the conditions under which resource limitations alter estimation strategies and analytically reveals the mechanism underlying the emergence of discontinuous, nonmonotonic, and scaling behaviors. These results provide a theoretical foundation for understanding how limited resources shape information processing in biological systems.",Neuroscience,http://arxiv.org/abs/2511.10184v1,arXiv,0
"Functional brain connectivity changes dynamically over time, making its representation challenging for learning on non-Euclidean data. We present a framework that encodes dynamic functional connectivity as an image representation of evolving network topology. Persistent graph homology summarizes global organization across scales, yielding Wasserstein distance-preserving embeddings stable under resolution changes. Stacking these embeddings forms a topological image that captures temporal reconfiguration of brain networks. This design enables convolutional architectures and transfer learning from pretrained foundational models to operate effectively under limited and imbalanced data. Applied to early Alzheimer's detection, the approach achieves clinically meaningful accuracy, establishing a principled foundation for imaging dynamic brain topology.",Neuroscience,http://arxiv.org/abs/2511.09949v1,arXiv,0
"Neurodevelopmental disorders such as Fragile X Syndrome (FXS) and Autism Spectrum Disorder (ASD) are characterized by disrupted cortical oscillatory activity, particularly in the alpha and gamma frequency bands. These abnormalities are linked to deficits in attention, sensory processing, and cognitive function. In this work, we present an adaptive machine learning-based brain-computer interface (BCI) system designed to modulate neural oscillations through frequency-specific auditory stimulation to enhance cognitive readiness in individuals with FXS. EEG data were recorded from 38 participants using a 128-channel system under a stimulation paradigm consisting of a 30-second baseline (no stimulus) followed by 60-second auditory entrainment episodes at 7Hz, 9Hz, 11Hz, and 13Hz. A comprehensive analysis of power spectral features (Alpha, Gamma, Delta, Theta, Beta) and cross-frequency coupling metrics (Alpha-Gamma, Alpha-Beta, etc.) was conducted. The results identified Peak Alpha Power, Peak Gamma Power, and Alpha Power per second per channel as the most discriminative biomarkers. The 13Hz stimulation condition consistently elicited a significant increase in Alpha activity and suppression of Gamma activity, aligning with our optimization objective. A supervised machine learning framework was developed to predict EEG responses and dynamically adjust stimulation parameters, enabling real-time, subject-specific adaptation. This work establishes a novel EEG-driven optimization framework for cognitive neuromodulation, providing a foundational model for next-generation AI-integrated BCI systems aimed at personalized neurorehabilitation in FXS and related disorders.",Neuroscience,http://arxiv.org/abs/2511.09765v1,arXiv,0
"Complex systems produce high-dimensional signals that lack macroscopic variables analogous to entropy, temperature, or free energy. This work introduces a thermoinformational formulation that derives entropy, internal energy, temperature, and Helmholtz free energy directly from empirical microstate distributions of arbitrary datasets. The approach provides a data-driven description of how a system reorganizes, exchanges information, and moves between stable and unstable states. Applied to dual-EEG recordings from mother-infant dyads performing the A-not-B task, the formulation captures increases in informational heat during switches and errors, and reveals that correct choices arise from more stable, low-temperature states. In an independent optogenetic dam-pup experiment, the same variables separate stimulation conditions and trace coherent trajectories in thermodynamic state space. Across both human and rodent systems, this thermoinformational formulation yields compact and physically interpretable macroscopic variables that generalize across species, modalities, and experimental paradigms.",Neuroscience,http://arxiv.org/abs/2511.09506v2,arXiv,0
"Predictive coding is a framework for understanding the formation of low-dimensional internal representations mirroring the environment's latent structure. The conditions under which such representations emerge remain unclear. In this work, we investigate how the prediction horizon and network depth shape the solutions of predictive coding tasks. Using a minimal abstract setting inspired by prior work, we show empirically and theoretically that sufficiently deep networks trained with multi-step prediction horizons consistently recover the underlying latent structure, a phenomenon explained through the Ordinary Least Squares estimator structure and biases in learning dynamics. We then extend these insights to nonlinear networks and complex datasets, including piecewise linear functions, MNIST, multiple latent states and higher dimensional state geometries. Our results provide a principled understanding of when and why predictive coding induces structured representations, bridging the gap between empirical observations and theoretical foundations.",Neuroscience,http://arxiv.org/abs/2511.09290v1,arXiv,0
"Characterizing the brain dynamics during different cortical states can reveal valuable information about its patterns across various cognitive processes. In particular, studying the differences between awake and sleep stages can shed light on the understanding of brain processes essential for physical and mental well-being, such as memory consolidation, information processing, and fatigue recovery. Alterations in these patterns may indicate disorders and pathologies such as obstructive sleep apnea, narcolepsy, as well as Alzheimer's and Parkinson's diseases. Here, we analyze time series obtained from intracranial recordings of 106 patients, covering four sleep stages: Wake, N2, N3, and REM. Intracranial electroencephalography (iEEG), which can include electrocorticography (ECoG) and depth recordings, represents the state-of-the-art measurements of brain activity, offering unparalleled spatial and temporal resolution for investigating neural dynamics. We characterize the signals using Bandt and Pompe symbolic methodology to calculate the Weighted Permutation Entropy (WPE) and the Statistical Complexity Measure (SCM) based on the Jensen and Shannon disequilibrium. By mapping the data onto the complexity-entropy plane, we observe that each stage occupies a distinct region, revealing its own dynamic signature. We show that our empirical results can be reproduced by a whole-brain computational model, in which each cortical region is described by a mean-field formulation based on networks of Adaptive Exponential Integrate-and-Fire (AdEx) neurons, adjusting the adaptation parameter to simulate the different sleep stages. Finally, we show that a classification approach using Support Vector Machine (SVM) provides high accuracy in distinguishing between cortical states.",Neuroscience,http://arxiv.org/abs/2511.09243v1,arXiv,0
"Alzheimer's disease (AD) is characterized by the progressive spread of pathology across brain networks, yet forecasting this cascade at the individual level remains challenging. We present a personalized graph-based dynamical model that captures the spatiotemporal evolution of cortical atrophy from longitudinal MRI and PET data. The approach constructs individualized brain graphs and learns the dynamics driving regional neurodegeneration. Applied to 1,891 participants from the Alzheimer's Disease Neuroimaging Initiative, the model accurately predicts key AD biomarkers -- including amyloid-beta, tau, neurodegeneration, and cognition -- outperforming clinical and neuroimaging benchmarks. Patient-specific parameters reveal distinct progression subtypes and anticipate future cognitive decline more effectively than standard biomarkers. Sensitivity analysis highlights regional drivers of disease spread, reproducing known temporolimbic and frontal vulnerability patterns. This network-based digital twin framework offers a quantitative, personalized paradigm for AD trajectory prediction, with implications for patient stratification, clinical trial design, and targeted therapeutic development.",Neuroscience,http://arxiv.org/abs/2511.08847v1,arXiv,0
"The slime mould Physarum polycephalum displays adaptive transport dynamics and network formation that have inspired its use as a model of biological computation. We develop a Lagrangian formulation of Physarum's adaptive dynamics on predefined graphs, showing that steady states arise as extrema of a least-action functional balancing metabolic dissipation and transport efficiency. The organism's apparent ability to find optimal paths between nutrient sources and sinks emerges from minimizing global energy dissipation under predefined boundary conditions that specify the problem to be solved. Applied to ring, tree, and lattice geometries, the framework accurately reproduces the optimal conductance and flux configurations observed experimentally. These results show that Physarum's problem-solving on constrained topologies follows a physics-based variational principle, revealing least-action dynamics as the foundation of its adaptive organization.",Neuroscience,http://arxiv.org/abs/2511.08531v1,arXiv,0
"Weakly electric fish, like Gnathonemus petersii, use a remarkable electrical modality for active sensing and communication, but studying their rich electrosensing and electrocommunication behavior and associated neural activity in naturalistic settings remains experimentally challenging. Here, we present a novel biologically-inspired computational framework to study these behaviors, where recurrent neural network (RNN) based artificial agents trained via multi-agent reinforcement learning (MARL) learn to modulate their electric organ discharges (EODs) and movement patterns to collectively forage in virtual environments. Trained agents demonstrate several emergent features consistent with real fish collectives, including heavy tailed EOD interval distributions, environmental context dependent shifts in EOD interval distributions, and social interaction patterns like freeloading, where agents reduce their EOD rates while benefiting from neighboring agents' active sensing. A minimal two-fish assay further isolates the role of electro-communication, showing that access to conspecific EODs and relative dominance jointly shape foraging success. Notably, these behaviors emerge through evolution-inspired rewards for individual fitness and emergent inter-agent interactions, rather than through rewarding agents explicitly for social interactions. Our work has broad implications for the neuroethology of weakly electric fish, as well as other social, communicating animals in which extensive recordings from multiple individuals, and thus traditional data-driven modeling, are infeasible.",Neuroscience,http://arxiv.org/abs/2511.08436v1,arXiv,0
"Encoding the distance between locations in space is essential for accurate navigation. Grid cells, a functional class of neurons in medial entorhinal cortex, are believed to support this computation. However, existing theories of how populations of grid cells code distance rely on complex coding schemes, with assumptions that may not be met by anatomical constraints. Inspired by recent work finding grid cells to have small, but robust heterogeneity in their grid properties, we hypothesize that distance coding can be achieved by a simple de-correlation of population activity. We develop a mathematical theory for describing this de-correlation in one-dimension, showing that its predictions are consistent with simulations of noisy grid cells. Our simulations highlight a non-intuitive prediction of such a distance by de-correlation framework. Namely, that some further distances are better encoded than some nearer distances. We find evidence of this ""sweet spot"" in previously published rodent behavioral experiments and demonstrate that a decoder which estimates distance from the de-correlation of populations of simulated noisy grid cells leads to a similar pattern of errors. Finally, by simulating noisy grid cells in two-dimensions, we find that there exists a trade-off between the range of distances that can be encoded by de-correlation of population activity and the distinguishability of different distances, which is controlled by the amount of variability in grid properties. We show that the previously observed average amount of grid property variability strikes a balance between the two, enabling the encoding of distances up to several meters. Our work provides new insight on how grid cells can underlie the coding of distance, without the assumptions previously needed, and why grid cells may have small amounts of heterogeneity in their grid properties.",Neuroscience,http://arxiv.org/abs/2511.08292v1,arXiv,0
"This paper gives an in-depth theoretical analysis of the direction and speed selectivity properties of idealized models of the spatio-temporal receptive fields of simple cells and complex cells, based on the generalized Gaussian derivative model for visual receptive fields. According to this theory, the receptive fields are modelled as velocity-adapted affine Gaussian derivatives for different image velocities and different degrees of elongation. By probing such idealized receptive field models of visual neurons to moving sine waves with different angular frequencies and image velocities, we characterize the computational models to a structurally similar probing method as is used for characterizing the direction and speed selective properties of biological neurons.   By comparison to results of neurophysiological measurements of direction and speed selectivity for biological neurons in the primary visual cortex, we find that our theoretical results are qualitatively consistent with (i) velocity-tuned visual neurons that are sensitive to particular motion directions and speeds, and (ii)~different visual neurons having broader {\em vs.\/}\ sharper direction and speed selective properties. Our theoretical results in combination with results from neurophysiological characterizations of motion-sensitive visual neurons are also consistent with a previously formulated hypothesis that the simple cells in the primary visual cortex ought to be covariant under local Galilean transformations, so as to enable processing of visual stimuli with different motion directions and speeds.",Neuroscience,http://arxiv.org/abs/2511.08101v1,arXiv,0
"A recent high-profile study by Koide-Majima et al. (2024) claimed a major advance in reconstructing visual imagery from brain activity using a novel variant of a generative AI-based method. However, our independent reanalysis reveals multiple methodological concerns that raise questions about the validity of their conclusions. Specifically, our evaluation demonstrates that: (1) the reconstruction results are biased by selective reporting of only the best-performing examples at multiple levels; (2) performance is artificially inflated by circular metrics that fail to reflect perceptual accuracy; (3) fair baseline comparisons reveal no discernible advantages of the study's key innovations over existing techniques; (4) the central ""Bayesian"" sampling component is functionally inert, producing outcomes identical to the standard optimization result; and (5) even if the component were successfully implemented, the claims of Bayesian novelty are unsubstantiated, as the proposed method does not leverage the principles of a proper Bayesian framework. These systemic issues necessitate a critical reassessment of the study's contributions. This commentary dissects these deficiencies to underscore the need for greater credibility and transparency in the rapidly advancing field of brain decoding.",Neuroscience,http://arxiv.org/abs/2511.07960v1,arXiv,0
"Achieving robust generalization across individuals remains a major challenge in electroencephalogram based imagined speech decoding due to substantial variability in neural activity patterns. This study examined how training dynamics and lightweight subject specific adaptation influence cross subject performance in a neural decoding framework. A cyclic inter subject training approach, involving shorter per subject training segments and frequent alternation among subjects, led to modest yet consistent improvements in decoding performance across unseen target data. Furthermore, under the subject calibrated leave one subject out scheme, incorporating only 10 % of the target subjects data for calibration achieved an accuracy of 0.781 and an AUC of 0.801, demonstrating the effectiveness of few shot adaptation. These findings suggest that integrating cyclic training with minimal calibration provides a simple and effective strategy for developing scalable, user adaptive brain computer interface systems that balance generalization and personalization.",Neuroscience,http://arxiv.org/abs/2511.13739v1,arXiv,0
"Human cooperation depends on how accurately we infer others' motives--how much they value fairness, generosity, or self-interest from the choices they make. We model that process in binary dictator games, which isolate moral trade-offs between self and other stripped of strategic complexity. Participants observed others' allocation decisions and predicted their future behavior while playing through an exhaustive, randomized payoff space implemented on The Morality Game platform. We formalize social-preference learning as Bayesian belief updating over continuous parameters such as self-interest, altruism, envy, and guilt. The resulting Utility Bayesian Model (UBM) outperformed non-Bayesian alternatives and Bayesian models that categorize others into discrete social types. Because Bayesian updating requires a utility function in its likelihood term, we conducted the largest utility-function comparison to date--476 candidate forms differing in psychologically meaningful properties (e.g., payoff exponents, reference dependence, payoff ratios, and envy-guilt asymmetries). Exploring this joint space of payoffs and models allowed us to identify the function that unifies prior theories and generalizes across payoff conditions. Parameter estimation revealed moderate altruism, strong inequality aversion, and nonlinear payoff valuation (exponent > 1). Altruism and social-comparison motives were largely independent, revealing diverse moral phenotypes from cooperative to competitive or sadistic. Together, these findings provide a computational framework and a map of social motives, clarifying how humans learn whom to trust and offering quantitative foundations for promoting cooperation in social and artificial systems.",Neuroscience,http://arxiv.org/abs/2511.07825v1,arXiv,0
"Functional connectivity has been widely investigated to understand brain disease in clinical studies and imaging-based neuroscience, and analyzing changes in functional connectivity has proven to be valuable for understanding and computationally evaluating the effects on brain function caused by diseases or experimental stimuli. By using Mahalanobis data whitening prior to the use of dimensionality reduction algorithms, we are able to distill meaningful information from fMRI signals about subjects and the experimental stimuli used to prompt them. Furthermore, we offer an interpretation of Mahalanobis whitening as a two-stage de-individualization of data which is motivated by similarity as captured by the Bures distance, which is connected to quantum mechanics. These methods have potential to aid discoveries about the mechanisms that link brain function with cognition and behavior and may improve the accuracy and consistency of Alzheimer's diagnosis, especially in the preclinical stage of disease progression.",Neuroscience,http://arxiv.org/abs/2511.07313v1,arXiv,0
"Understanding the relationship between brain activity and behavior is a central goal of neuroscience. Despite significant advances, a fundamental dichotomy persists: neural activity manifests as both discrete spikes of individual neurons and collective waves of populations. Both neural codes correlate with behavior, yet correlation alone cannot determine whether waves exert a causal influence or merely reflect spiking dynamics without causal efficacy. According to the Causal Hierarchy Theorem, no amount of observational data--however extensive--can settle this question; causal conclusions require explicit structural assumptions or careful experiment designs that directly correspond to the causal effect of interest. We develop a formal framework that makes this limitation precise and constructive. Formalizing epiphenomenality via the invariance of interventional distributions in Structural Causal Models (SCMs), we derive a certificate of sufficiency from Pearl's do-calculus that specifies when variables can be removed from the model without loss of causal explainability and clarifies how interventions should be interpreted under different causal structures of spike-wave duality. The purpose of this work is not to resolve the spike-wave debate, but to reformulate it. We shift the problem from asking which signal matters most to asking under what conditions any signal can be shown to matter at all. This reframing distinguishes prediction from explanation and offers neuroscience a principled route for deciding when waves belong to mechanism and when they constitute a byproduct of underlying coordination",Neuroscience,http://arxiv.org/abs/2511.06602v1,arXiv,0
"Artificial Neural Networks, the building blocks of AI, were inspired by the human brain's network of neurons. Over the years, these networks have evolved to replicate the complex capabilities of the brain, allowing them to handle tasks such as image and language processing. In the realm of Large Language Models, there has been a keen interest in making the language learning process more akin to that of humans. While neuroscientific research has shown that different grammatical categories are processed by different neurons in the brain, we show that LLMs operate in a similar way. Utilizing Llama 3, we identify the most important neurons associated with the prediction of words belonging to different part-of-speech tags. Using the achieved knowledge, we train a classifier on a dataset, which shows that the activation patterns of these key neurons can reliably predict part-of-speech tags on fresh data. The results suggest the presence of a subspace in LLMs focused on capturing part-of-speech tag concepts, resembling patterns observed in lesion studies of the brain in neuroscience.",Neuroscience,http://arxiv.org/abs/2511.06519v1,arXiv,0
"Functional brain networks exhibit topological structures that reflect neural organization; however, statistical comparison of these networks is challenging for several reasons. This paper introduces a topologically invariant permutation test for detecting topological inequivalence. Under topological equivalence, topological features can be permuted separately between groups without distorting individual network structures. The test statistic uses $2$-Wasserstein distances on persistent diagrams, computed in closed form. To reduce variability in brain connectivities while preserving topology, heat kernel expansion on the Hodge Laplacian is applied with bandwidth $t$ controlling diffusion intensity. Theoretical results guarantee variance reduction through optimal Hilbert space projection. Simulations across diverse network topologies show superior performance compared to conventional two-sample tests and alternative metrics. Applied to resting-state fMRI data from the Multimodal Treatment of ADHD study, the method detects significant topological differences between cannabis users and non-users.",Neuroscience,http://arxiv.org/abs/2511.06153v2,arXiv,0
"Travelling waves of neural firing activity are observed in brain tissue as a part of various sensory, motor and cognitive processes. They represent an object of major interest in the study of excitable networks, with analysis conducted in both neural field models and spiking neuronal networks. The latter class exposes the single-neuron dynamics directly, allowing us to study the details of their influence upon network-scale behaviour. Here we present a study of a laterally-inhibited network of leaky integrate-and-fire neurons modulated by a slow voltage-gated ion channel that acts as a linear adaptation variable. As the strength of the ion channel increases, we find that its interaction with the lateral inhibition increases wave speeds. The ion channel can enable subthreshold oscillations, with the intervals between the firing events of loosely-coupled travelling wave solutions structured around the neuron's natural period. These subthreshold oscillations also enable the occurrence of codimension-2 grazing bifurcations; along with the emergence of fold bifurcations along wave solution branches, the slow ion channel introduces a variety of intermediate structures in the solution space. These point towards further investigation of the role neighbouring solution branches play in the behaviour of waves forced across bifurcations, which we illustrate with the aid of simulations using a novel root-finding algorithm designed to handle uncertainty over the existence of firing solutions.",Neuroscience,http://arxiv.org/abs/2511.05232v1,arXiv,0
"Isolated rapid eye movement sleep behavior disorder (iRBD) is a major prodromal marker of $Î±$-synucleinopathies, often preceding the clinical onset of Parkinson's disease, dementia with Lewy bodies, or multiple system atrophy. While wrist-worn actimeters hold significant potential for detecting RBD in large-scale screening efforts by capturing abnormal nocturnal movements, they become inoperable without a reliable and efficient analysis pipeline. This study presents ActiTect, a fully automated, open-source machine learning tool to identify RBD from actigraphy recordings. To ensure generalizability across heterogeneous acquisition settings, our pipeline includes robust preprocessing and automated sleep-wake detection to harmonize multi-device data and extract physiologically interpretable motion features characterizing activity patterns. Model development was conducted on a cohort of 78 individuals, yielding strong discrimination under nested cross-validation (AUROC = 0.95). Generalization was confirmed on a blinded local test set (n = 31, AUROC = 0.86) and on two independent external cohorts (n = 113, AUROC = 0.84; n = 57, AUROC = 0.94). To assess real-world robustness, leave-one-dataset-out cross-validation across the internal and external cohorts demonstrated consistent performance (AUROC range = 0.84-0.89). A complementary stability analysis showed that key predictive features remained reproducible across datasets, supporting the final pooled multi-center model as a robust pre-trained resource for broader deployment. By being open-source and easy to use, our tool promotes widespread adoption and facilitates independent validation and collaborative improvements, thereby advancing the field toward a unified and generalizable RBD detection model using wearable devices.",Neuroscience,http://arxiv.org/abs/2511.05221v2,arXiv,0
"Background and Objectives: This paper focuses on using AI to assess the cognitive function of older adults with mild cognitive impairment or mild dementia using physiological data provided by a wearable device. Cognitive screening tools are disruptive, time-consuming, and only capture brief snapshots of activity. Wearable sensors offer an attractive alternative by continuously monitoring physiological signals. This study investigated whether physiological data can accurately predict scores on established cognitive tests. Research Design and Methods: We recorded physiological signals from 23 older adults completing three NIH Toolbox Cognitive Battery tests, which assess working memory, processing speed, and attention. The Empatica EmbracePlus, a wearable device, measured blood volume pulse, skin conductance, temperature, and movement. Statistical features were extracted using wavelet-based and segmentation methods. We then applied supervised learning and validated predictions via cross-validation, hold-out testing, and bootstrapping. Results: Our models showed strong performance with Spearman's Ïof 0.73-0.82 and mean absolute errors of 0.14-0.16, significantly outperforming a naive mean predictor. Sensor roles varied: heart-related signals combined with movement and temperature best predicted working memory, movement paired with skin conductance was most informative for processing speed, and heart in tandem with skin conductance worked best for attention. Discussion and Implications: These findings suggest that wearable sensors paired with AI tools such as supervised learning and feature engineering can noninvasively track specific cognitive functions in older adults, enabling continuous monitoring. Our study demonstrates how AI can be leveraged when the data sample is small. This approach may support remote assessments and facilitate clinical interventions.",Neuroscience,http://arxiv.org/abs/2511.04983v1,arXiv,0
"Functional and structural connectivity (FC/SC) are key multimodal biomarkers for brain analysis, yet their clinical utility is hindered by costly acquisition, complex preprocessing, and frequent missing modalities. Existing foundation models either process single modalities or lack explicit mechanisms for cross-modal and cross-scale consistency. We propose BrainCSD, a hierarchical mixture-of-experts (MoE) foundation model that jointly synthesizes FC/SC biomarkers and supports downstream decoding tasks (diagnosis and prediction). BrainCSD features three neuroanatomically grounded components: (1) a ROI-specific MoE that aligns regional activations from canonical networks (e.g., DMN, FPN) with a global atlas via contrastive consistency; (2) a Encoding-Activation MOE that models dynamic cross-time/gradient dependencies in fMRI/dMRI; and (3) a network-aware refinement MoE that enforces structural priors and symmetry at individual and population levels. Evaluated on the datasets under complete and missing-modality settings, BrainCSD achieves SOTA results: 95.6\% accuracy for MCI vs. CN classification without FC, low synthesis error (FC RMSE: 0.038; SC RMSE: 0.006), brain age prediction (MAE: 4.04 years), and MMSE score estimation (MAE: 1.72 points). Code is available in \href{https://github.com/SXR3015/BrainCSD}{BrainCSD}",Neuroscience,http://arxiv.org/abs/2511.05630v1,arXiv,0
"The motor cortex (MC) is often described as an autonomous dynamical system during movement execution. In an autonomous dynamical system, flexible movement generation depends on reconfiguring the initial conditions, which then unwind along known dynamics. An open question is whether these dynamics govern MC activity during brain-machine interface (BMI) control. We investigated MC activity during BMI cursor movements of multiple durations, ranging from hundreds of milliseconds to sustained over seconds. These durations were chosen to cover the range of movement durations necessary to control modern BMIs under varying precision levels. Movements shared their MC initial condition with movements of different durations in the same direction. Long-duration movements sustained MC activity, effectively pausing the neural population dynamics until each movement goal was reached. The difference across durations in MC population dynamics may be attributed to external inputs. Our results highlight the role of sustained inputs to MC during movement.",Neuroscience,http://arxiv.org/abs/2511.04887v1,arXiv,0
"Recordings of increasingly large neural populations have revealed that the firing of individual neurons is highly coordinated. When viewed in the space of all possible patterns, the collective activity forms non-linear structures called neural manifolds. Because such structures are observed even at rest or during sleep, an important hypothesis is that activity manifolds may correspond to continuous attractors shaped by recurrent connectivity between neurons. Classical models of recurrent networks have shown that continuous attractors can be generated by specific symmetries in the connectivity. Although a variety of attractor network models have been studied, general principles linking network connectivity and the geometry of attractors remain to be formulated. Here, we address this question by using group representation theory to formalize the relationship between the symmetries in recurrent connectivity and the resulting fixed-point manifolds. We start by revisiting the classical ring model, a continuous attractor network generating a circular manifold. Interpreting its connectivity as a circular convolution, we draw a parallel with feed-forward CNNs. Building on principles of geometric deep learning, we then generalize this architecture to a broad range of symmetries using group representation theory. Specifically, we introduce a new class of equivariant RNNs, where the connectivity is based on group convolution. Using the group Fourier transform, we reduce such networks to low-rank models, giving us a low-dimensional description that can be fully analyzed to determine the symmetry, dimensionality and stability of fixed-point manifolds. Our results underline the importance of stability considerations: for a connectivity with a given symmetry, depending on parameters, several manifolds with different symmetry subgroups can coexist, some stable and others consisting of saddle points.",Neuroscience,http://arxiv.org/abs/2511.04802v2,arXiv,0
"Many models used in artificial intelligence and cognitive science rely on multi-element patterns stored in ""slots"" - dedicated storage locations - in a digital computer. As biological brains likely lack slots, we consider how they might achieve similar functional outcomes without them by building on the neurally-inspired modern Hopfield network (MHN; Krotov & Hopfield, 2021), which stores patterns in the connection weights of an individual neuron. We propose extensions of this approach to increase its biological plausibility as a model of memory and to capture an important advantage of slot-based computation in contemporary language models. For memory, neuroscience research suggests that the weights of overlapping sparse ensembles of neurons, rather than a dedicated individual neuron, are used to store a memory. We introduce the K-winner MHN, extending the approach to ensembles, and find that within a continual learning regime, the ensemble-based MHN exhibits greater retention of older memories, as measured by the graded sensitivity measure d', than a standard (one-neuron) MHN. Next, we consider the powerful use of slot-based memory in contemporary language models. These models use slots to store long sequences of past inputs and their learned encodings, supporting later predictions and allowing error signals to be transported backward in time to adjust weights underlying the learned encodings of these past inputs. Inspired by these models' successes, we show how the MHN can be extended to capture both of these important functional outcomes. Collectively, our modeling approaches constitute steps towards understanding how biologically plausible mechanisms can support computations that have enabled AI systems to capture human-like abilities that no prior models have been able to achieve.",Neuroscience,http://arxiv.org/abs/2511.04593v1,arXiv,0
"Functional brain graphs are often characterized with separate graph-theoretic or spectral descriptors, overlooking how these properties covary and partially overlap across brains and conditions. We anticipate that dense, weighted functional connectivity graphs occupy a low-dimensional latent geometry along which both topological and spectral structures display graded variations. Here, we estimated this unified graph representation and enabled generation of dense functional brain graphs through a graph transformer autoencoder with latent diffusion, with spectral geometry providing an inductive bias to guide learning. This geometry-aware latent representation, although unsupervised, meaningfully separated working-memory states and decoded visual stimuli, with performance further enhanced by incorporating neural dynamics. From the diffusion modeled distribution, we were able to sample biologically plausible and structurally grounded synthetic dense graphs.",Neuroscience,http://arxiv.org/abs/2511.04539v1,arXiv,0
"Inspired by key neuroscience principles, deep learning has driven exponential breakthroughs in developing functional models of perception and other cognitive processes. A key to this success has been the implementation of crucial features found in biological neural networks: neurons as units of information transfer, non-linear activation functions that enable general function approximation, and complex architectures vital for attentional processes. However, standard deep learning models rely on biologically implausible error propagation algorithms and struggle to accumulate knowledge incrementally. While, the precise learning rule governing synaptic plasticity in biological systems remains unknown, recent discoveries in neuroscience could fuel further progress in AI. Here I examine successful implementations of brain-inspired principles in deep learning, current limitations, and promising avenues inspired by recent advances in neuroscience, including error computation, propagation, and integration via synaptic updates in biological neural networks.",Neuroscience,http://arxiv.org/abs/2511.04455v1,arXiv,0
"We consider the problem of fitting a reinforcement learning (RL) model to some given behavioral data under a multi-armed bandit environment. These models have received much attention in recent years for characterizing human and animal decision making behavior. We provide a generic mathematical optimization problem formulation for the fitting problem of a wide range of RL models that appear frequently in scientific research applications, followed by a detailed theoretical analysis of its convexity properties. Based on the theoretical results, we introduce a novel solution method for the fitting problem of RL models based on convex relaxation and optimization. Our method is then evaluated in several simulated bandit environments to compare with some benchmark methods that appear in the literature. Numerical results indicate that our method achieves comparable performance to the state-of-the-art, while significantly reducing computation time. We also provide an open-source Python package for our proposed method to empower researchers to apply it in the analysis of their datasets directly, without prior knowledge of convex optimization.",Neuroscience,http://arxiv.org/abs/2511.04454v1,arXiv,0
"Photon scattering has traditionally limited the ability of near-infrared spectroscopy (NIRS) to extract accurate, layer-specific information from the brain. This limitation restricts its clinical utility for precise neurological monitoring. To address this, we introduce an AI-driven, high-density NIRS system optimized to provide real-time, layer-specific oxygenation data from the brain cortex, specifically targeting acute neuro-emergencies. Our system integrates high-density NIRS reflectance data with a neural network trained on MRI-based synthetic datasets. This approach achieves robust cortical oxygenation accuracy across diverse anatomical variations. In simulations, our AI-assisted NIRS demonstrated a strong correlation (R2=0.913) with actual cortical oxygenation, markedly outperforming conventional methods (R2=0.469). Furthermore, biomimetic phantom experiments confirmed its superior anatomical reliability (R2=0.986) compared to standard commercial devices (R2=0.823). In clinical validation with healthy subjects and ischemic stroke patients, the system distinguished between the two groups with an AUC of 0.943. This highlights its potential as an accessible, high-accuracy diagnostic tool for emergency and point-of-care settings. These results underscore the system's capability to advance neuro-monitoring precision through AI, enabling timely, data-driven decisions in critical care environments.",Neuroscience,http://arxiv.org/abs/2511.05612v1,arXiv,0
"Brain-computer interfaces (BCIs) allow direct communication between the brain and external devices, frequently using electroencephalography (EEG) to record neural activity. Dimensionality reduction and structured regularization are essential for effectively classifying task-related brain signals, including event-related potentials (ERPs) and motor imagery (MI) rhythms. Current tensor-based approaches, such as Tucker and PARAFAC decompositions, often lack the flexibility needed to fully capture the complexity of EEG data. This study introduces Block-Term Tensor Discriminant Analysis (BTTDA): a novel tensor-based and supervised feature extraction method designed to enhance classification accuracy by providing flexible multilinear dimensionality reduction. Extending Higher Order Discriminant Analysis (HODA), BTTDA uses a novel and interpretable forward model for HODA combined with a deflation scheme to iteratively extract discriminant block terms, improving feature representation for classification. BTTDA and a sum-of-rank-1-terms variant PARAFACDA were evaluated on publicly available ERP (second-order tensors) and MI (third-order tensors) EEG datasets from the MOABB benchmarking framework. Benchmarking revealed that BTTDA and PARAFACDA significantly outperform the traditional HODA method in ERP decoding, resulting in state-of-the art performance (ROC-AUC = 91.25%). For MI, decoding results of HODA, BTTDA and PARAFACDA were subpar, but BTTDA still significantly outperformed HODA (64.52% > 61.00%). The block-term structure of BTTDA enables interpretable and more efficient dimensionality reduction without compromising discriminative power. This offers a promising and adaptable approach for feature extraction in BCI and broader neuroimaging applications.",Neuroscience,http://arxiv.org/abs/2511.04292v1,arXiv,0
"Graph-theoretical labeling provides a rigorous mathematical framework for characterizing the structural and functional organization of complex networks. This paper investigates the application of cordial labeling and signed product cordial labeling to brain connectivity graphs, emphasizing their relevance to small-world network models in neuroscience. The cordial condition is interpreted as a measure of structural balance between excitatory and inhibitory neuronal interactions, while the signed product cordial labeling reflects the coexistence of cooperative and antagonistic neural dynamics.",Neuroscience,http://arxiv.org/abs/2511.05606v1,arXiv,0
"The presence of an expanded polyglutamine produces a toxic gain of function in huntingtin. Protein aggregation resulting from this gain of function is likely to be the cause of neuronal death. Two main mechanisms of aggregation have been proposed: hydrogen bonding by polar-zipper formation and covalent bonding by transglutaminase-catalyzed cross-linking. In cell culture models of Huntington's disease, aggregates are mostly stabilized by hydrogen bonds, but covalent bonds are also likely to occur. Nothing is known about the nature of the bonds that stabilize the aggregates in the brain of patients with Huntington's disease. It seems that the nature of the bond stabilizing the aggregates is one of the most important questions, as the answer would condition the therapeutic approach to Huntington's disease.",Neuroscience,http://arxiv.org/abs/2511.04174v1,arXiv,0
"The reductionist approach commonly employed in scientific methods presupposes that both macro and micro phenomena can be explained by micro-level laws alone. This assumption implies intra-level causal closure, rendering all macro phenomena epiphenomenal. However, the integrative nature of consciousness suggests that it is a macro phenomenon. To ensure scientific testability and reject epiphenomenalism, the reductionist assumption of intra-level causal closure must be rejected. This implies that even neural-level behavior cannot be explained by observable neural-level laws alone. Therefore, a new methodology is necessary to acknowledge the causal efficacy of macro-level phenomena. We model the brain as operating under dual laws at different levels. This model includes hypothetical macro-level psychological laws that are not determined solely by micro-level neural laws, as well as the causal effects from macro to micro levels. In this study, we propose a constructive approach that explains both mental and physical phenomena through the interaction between these two sets of laws.",Neuroscience,http://arxiv.org/abs/2511.04047v3,arXiv,0
"Humans can quickly and effortlessly extract a variety of information about others' social interactions from visual input, ranging from visuospatial cues like whether two people are facing each other to higher-level information. Yet, the computations supporting these abilities remain poorly understood, and social interaction recognition continues to challenge even the most advanced AI vision systems. Here, we hypothesized that humans rely on 3D visuospatial pose information to make social interaction judgments, which is absent in most AI vision models. To test this, we combined state-of-the-art pose and depth estimation algorithms to extract 3D joint positions of people in short video clips depicting everyday human actions and compared their ability to predict human social interaction judgments with current AI vision models. Strikingly, 3D joint positions outperformed most current AI vision models, revealing that key social information is available in explicit body position but not in the learned features of most vision models, including even the layer-wise embeddings of the pose models used to extract joint positions. To uncover the critical pose features humans use to make social judgments, we derived a compact set of 3D social pose features describing only the 3D position and direction of faces in the videos. We found that these minimal descriptors matched the predictive strength of the full set of 3D joints and significantly improved the performance of off-the-shelf AI vision models when combined with their embeddings. Moreover, the degree to which 3D social pose features were represented in each off-the-shelf AI vision model predicted the model's ability to match human social judgments. Together, our findings provide strong evidence that human social scene understanding relies on explicit representations of 3D pose and can be supported by simple, structured visuospatial primitives.",Neuroscience,http://arxiv.org/abs/2511.03988v1,arXiv,0
"We investigate and quantify the basin geometry and extreme final state uncertainty of two identical electrically asymmetrically coupled Chialvo neurons. The system's diverse behaviors are presented, along with the mathematical reasoning behind its chaotic and nonchaotic dynamics as determined by the structure of the coupled equations. The system is found to be multistable with two qualitatively different attractors. Although each neuron is individually nonchaotic, the chaotic basin takes up the vast majority of the coupled system's state space, but the nonchaotic basin stretches to infinity due to chance synchronization. The boundary between the basins is found to be fractal, leading to extreme final state sensitivity. This uncertainty and its potential effect on the synchronization of biological neurons may have significant implications for understanding human behavior and neurological disease.",Neuroscience,http://arxiv.org/abs/2511.03671v1,arXiv,0
"When people pursue rewards in stochastic environments, they often match their choice frequencies to the observed target frequencies, even when this policy is demonstrably sub-optimal. We used a ``hide and seek'' task to evaluate this behavior under conditions where pursuit (seeking) could be toggled to avoidance (hiding), while leaving the probability distribution fixed, or varying complexity by changing the number of possible choices. We developed a model for participant choice built from choice frequency histograms treated as vectors. We posited the existence of a probability antimatching strategy for avoidance (hiding) rounds, and formalized this as a vector reflection of probability matching. We found that only two basis policies: matching/antimatching and maximizing/minimizing were sufficient to account for participant choices across a range of room numbers and opponent probability distributions. This schema requires only that people have the ability to remember the relative frequency of the different outcomes. With this knowledge simple operations can construct the maximizing and minimizing policies as well as matching and antimatching strategies. A mixture of these two policies captures human choice patterns in a stochastic environment.",Neuroscience,http://arxiv.org/abs/2511.03643v2,arXiv,0
"Recent evidence suggests that beta-band activity plays a key role in decision-making. Here we review our recent work in humans and non-human primates showing that beta-band frequency shifts in frontal cortex signal categorical decision outcomes. We revisit our previous proposal suggesting that content-specific beta reflects the flexible recruiting of transient neural ensembles and update it to emphasize frequency as the relevant parameter. We argue that beta frequency shifts arise from changes in connectivity between weakly coupled oscillators and that, more than a spectral fingerprint, they reflect an active mechanism to (re)-activate behaviorally relevant communication channels in the brain.",Neuroscience,http://arxiv.org/abs/2511.03503v1,arXiv,0
"Cortical circuits exhibit high levels of response diversity, even across apparently uniform neuronal populations. While emerging data-driven approaches exploit this heterogeneity to infer effective models of cortical circuit computation (e.g. Genkin et al. Nature 2025), the power of response diversity to enable inference of mechanistic circuit models is largely unexplored. Within the landscape of cortical circuit models, spiking neuron networks in the balanced state naturally exhibit high levels of response and tuning diversity emerging from their internal dynamics. A statistical theory for this emergent tuning heterogeneity, however, has only been formulated for binary spin models (Vreeswijk & Sompolinsky, 2005). Here we present a formulation of feature-tuned balanced state networks that allows for arbitrary and diverse dynamics of postsynaptic currents and variable levels of heterogeneity in cellular excitability but nevertheless is analytically exactly tractable with respect to the emergent tuning curve heterogeneity. Using this framework, we present a case study demonstrating that, for a wide range of parameters even the population mean response is non-universal and sensitive to mechanistic circuit details. As our theory enables exactly and analytically obtaining the likelihood-function of tuning heterogeneity given circuit parameters, we argue that it forms a powerful and rigorous basis for neural circuit inference.",Neuroscience,http://arxiv.org/abs/2511.03502v1,arXiv,0
"Large-scale pre-trained models hold significant potential for learning universal EEG representations. However, most existing methods, particularly autoregressive (AR) frameworks, primarily rely on straightforward temporal sequencing of multi-channel EEG data, which fails to capture the rich physiological characteristics inherent to EEG signals. Moreover, their time-centered modeling approach also limits the effective representation of the dynamic spatial topology of brain activity. To address these challenges and fully exploit the potential of large-scale EEG models, we propose a novel Topology Hierarchical Derived Brain Autoregressive Modeling (THD-BAR) for EEG generic representations. The core innovation of THD-BAR lies in the introduction of the Brain Topology Hierarchy (BTH), which establishes a multi-scale spatial order for EEG channels. This hierarchical structure enables a redefinition of autoregressive learning as a ""next-scale-time prediction"" problem, effectively capturing both spatial and temporal dynamics. Based on BTH, we design a Topology-Hierarchical Vector Quantized-Variational Autoencoder (THVQ-VAE) for multi-scale tokenization and develop an enhanced Brain Autoregressive (BAR) module with specialized masking strategies for prediction. Through extensive large-scale pre-training on 17 datasets, followed by rigorous validation on 10 downstream datasets spanning 5 distinct tasks, THD-BAR consistently outperforms existing methods. These results highlight the superior generalization and modeling capabilities of our proposed approach.",Neuroscience,http://arxiv.org/abs/2511.13733v1,arXiv,0
"Precise, generalizable subject-agnostic seizure prediction (SASP) remains a fundamental challenge due to the intrinsic complexity and significant spectral variability of electrophysiological signals across individuals and recording modalities. We propose FAPEX, a novel architecture that introduces a learnable fractional neural frame operator (FrNFO) for adaptive time-frequency decomposition. Unlike conventional models that exhibit spectral bias toward low frequencies, our FrNFO employs fractional-order convolutions to capture both high and low-frequency dynamics, achieving approximately 10% improvement in F1-score and sensitivity over state-of-the-art baselines. The FrNFO enables the extraction of instantaneous phase and amplitude representations that are particularly informative for preictal biomarker discovery and enhance out-of-distribution generalization. FAPEX further integrates structural state-space modeling and channelwise attention, allowing it to handle heterogeneous electrode montages. Evaluated across 12 benchmarks spanning species (human, rat, dog, macaque) and modalities (Scalp-EEG, SEEG, ECoG, LFP), FAPEX consistently outperforms 23 supervised and 10 self-supervised baselines under nested cross-validation, with gains of up to 15% in sensitivity on complex cross-domain scenarios. It further demonstrates superior performance in several external validation cohorts. To our knowledge, these establish FAPEX as the first epilepsy model to show consistent superiority in SASP, offering a promising solution for discovering epileptic biomarker evidence supporting the existence of a distinct and identifiable preictal state and clinical translation.",Neuroscience,http://arxiv.org/abs/2511.03263v1,arXiv,0
"The complexity of human cognition has meant that psychology makes more use of theory and conceptual models than perhaps any other biomedical field. To enable precise quantitative study of the full breadth of phenomena in psychological and psychiatric medicine as well as cognitive aspects of AI safety, there is a need for a mathematical formulation which is both mathematically precise and equally accessible to experts from numerous fields. In this paper we formalize human psychodynamics via the diagrammatic framework of process theory, describe its key properties, and explain the links between a diagrammatic representation and central concepts in analysis of cognitive processes in contexts such as psychotherapy, neurotechnology, AI alignment, AI agent representation of individuals in autonomous negotiations, developing human-like AI systems, and other aspects of AI safety.",Neuroscience,http://arxiv.org/abs/2511.05580v1,arXiv,0
"The gut microbiota has emerged as a fundamental regulator of sleep physiology, influencing neural, endocrine, and immune pathways through the gut-microbiota-brain axis (GMBA). This bidirectional communication system modulates neurotransmitter production, circadian rhythms, and metabolic homeostasis, while disruptions in microbial composition have been linked to sleep disorders, neuroinflammation, and systemic immune dysfunction. Recent findings suggest that gut dysbiosis contributes to sleep disturbances by altering serotonin, GABA, and short-chain fatty acid (SCFA) metabolism, with implications for neurodegenerative diseases, metabolic syndromes, and mood disorders. Additionally, the gut microbiota interacts with the endocrine and immune systems, shaping inflammatory responses and stress adaptation mechanisms. This review explores the intricate connections between sleep and the gut microbiota, integrating emerging research on microbiota-targeted therapies, such as probiotics, fecal microbiota transplantation (FMT), and chrononutrition, as potential interventions to restore sleep homeostasis and improve health outcomes",Neuroscience,http://arxiv.org/abs/2511.02766v4,arXiv,0
"The human neocortex is functionally organised at its highest level along a continuous sensory-to-association (AS) hierarchy. This study characterises the AS hierarchy of patients with schizophrenia in a comparison with controls. Using a large fMRI dataset (N=355), we extracted individual AS gradients via spectral analysis of brain connectivity, quantified hierarchical specialisation by gradient spread, and related this spread with connectivity geometry. We found that schizophrenia compresses the AS hierarchy indicating reduced functional differentiation. By modelling neural timescale with the Ornstein-Uhlenbeck process, we observed that the most specialised, locally cohesive regions at the gradient extremes exhibit dynamics with a longer time constant, an effect that is attenuated in schizophrenia. To study computation, we used the gradients to regularise subject-specific recurrent neural networks (RNNs) trained on working memory tasks. Networks endowed with greater gradient spread learned more efficiently, plateaued at lower task loss, and maintained stronger alignment to the prescribed AS hierarchical geometry. Fixed point linearisation showed that high-range networks settled into more stable neural states during memory delay, evidenced by lower energy and smaller maximal Jacobian eigenvalues. This gradient-regularised RNN framework therefore links large-scale cortical architecture with fixed point stability, providing a mechanistic account of how gradient de-differentiation could destabilise neural computations in schizophrenia, convergently supported by empirical timescale flattening and model-based evidence of less stable fixed points.",Neuroscience,http://arxiv.org/abs/2511.02722v1,arXiv,0
"Predicting future brain state from a baseline magnetic resonance image (MRI) is a central challenge in neuroimaging and has important implications for studying neurodegenerative diseases such as Alzheimer's disease (AD). Most existing approaches predict future cognitive scores or clinical outcomes, such as conversion from mild cognitive impairment to dementia. Instead, here we investigate longitudinal MRI image-to-image prediction that forecasts a participant's entire brain MRI several years into the future, intrinsically modeling complex, spatially distributed neurodegenerative patterns. We implement and evaluate five deep learning architectures (UNet, U2-Net, UNETR, Time-Embedding UNet, and ODE-UNet) on two longitudinal cohorts (ADNI and AIBL). Predicted follow-up MRIs are directly compared with the actual follow-up scans using metrics that capture global similarity and local differences. The best performing models achieve high-fidelity predictions, and all models generalize well to an independent external dataset, demonstrating robust cross-cohort performance. Our results indicate that deep learning can reliably predict participant-specific brain MRI at the voxel level, offering new opportunities for individualized prognosis.",Neuroscience,http://arxiv.org/abs/2511.02558v2,arXiv,0
"Traditional neural networks, while powerful, rely on biologically implausible learning mechanisms such as global backpropagation. This paper introduces the Structurally Adaptive Predictive Inference Network (SAPIN), a novel computational model inspired by the principles of active inference and the morphological plasticity observed in biological neural cultures. SAPIN operates on a 2D grid where processing units, or cells, learn by minimizing local prediction errors. The model features two primary, concurrent learning mechanisms: a local, Hebbian-like synaptic plasticity rule based on the temporal difference between a cell's actual activation and its learned expectation, and a structural plasticity mechanism where cells physically migrate across the grid to optimize their information-receptive fields. This dual approach allows the network to learn both how to process information (synaptic weights) and also where to position its computational resources (network topology). We validated the SAPIN model on the classic Cart Pole reinforcement learning benchmark. Our results demonstrate that the architecture can successfully solve the CartPole task, achieving robust performance. The network's intrinsic drive to minimize prediction error and maintain homeostasis was sufficient to discover a stable balancing policy. We also found that while continual learning led to instability, locking the network's parameters after achieving success resulted in a stable policy. When evaluated for 100 episodes post-locking (repeated over 100 successful agents), the locked networks maintained an average 82% success rate.",Neuroscience,http://arxiv.org/abs/2511.02241v3,arXiv,0
"Cognitive control is a suite of processes that helps individuals pursue goals despite resistance or uncertainty about what to do. Although cognitive control has been extensively studied as a dynamic feedback loop of perception, valuation, and action, it remains incompletely understood as a cohesive dynamic and distributed neural process. Here, we critically examine the history of and advances in the study of cognitive control, including how metaphors and cultural norms of power, morality, and rationality are intertwined with definitions of control, to consider holistically how different models explain which brain regions act as controllers. Controllers, the source of top-down signals, are typically localized in regions whose neural activations implement elementary component processes of control, including conflict monitoring and behavioral inhibition. Top-down signals from these regions guide the activation of other task-specific regions, biasing them towards task-specific activity patterns. A relatively new approach, network control theory, has roots in dynamical systems theory and systems engineering. This approach can mathematically show that controllers are regions with strongly nested and recurrent anatomical connectivity that efficiently propagate top-down signals, and precisely estimate the amount, location, and timing of signaling required to bias global activity to task-specific patterns. The theory converges with prior evidence, offers new mathematical tools and intuitions for understanding control loops across levels of analysis, and naturally produces graded predictions of control across brain regions and modules of psychological function that have been unconsidered or marginalized. We describe how prior approaches converge and diverge, noting directions for future integration to improve understanding of how the brain instantiates cognitive control.",Neuroscience,http://arxiv.org/abs/2511.02063v1,arXiv,0
"This paper introduces variational representational similarity analysis RSA (vRSA) for electromagnetic recordings of neural responses (e.g., EEG, MEG, ECoG or LFP). Variational RSA is a Bayesian approach for testing whether the similarity of stimuli or experimental conditions is expressed in univariate or multivariate neural recordings. Extending an approach previously introduced in the context of functional MRI, vRSA decomposes the condition-by-condition data covariance matrix into hypothesised effects and observation noise, thereby casting RSA as a covariance component estimation problem. In this context, peristimulus time may be treated as an experimental factor, enabling one to test for the probability that different experimental effects are expressed in data at different times. Variational Bayesian methods are used for model estimation and model comparison, which confer a number of advantages over classical approaches, including statistically efficient hypothesis testing, quantification of uncertainty using Bayesian credible intervals and computational efficiency. After introducing the theory, we provide a worked example using openly available EEG data. Software functions implementing vRSA for the SPM software package accompany this paper, together with exemplar analysis scripts.",Neuroscience,http://arxiv.org/abs/2511.01784v1,arXiv,0
"Artificial intelligence (AI) has drawn significant inspiration from neuroscience to develop artificial neural network (ANN) models. However, these models remain constrained by the Von Neumann architecture and struggle to capture the complexity of the biological brain. Quantum computing, with its foundational principles of superposition, entanglement, and unitary evolution, offers a promising alternative approach to modeling neural dynamics. This paper explores the possibility of a neuro-quantum model of the brain by introducing a stochastic quantum approach that incorporates random fluctuations of neuronal processing within a quantum framework. We propose a mathematical formalization of stochastic quantum neural networks (QNNS), where qubits evolve according to stochastic differential equations inspired by biological neuronal processes. We also discuss challenges related to decoherence, qubit stability, and implications for AI and computational neuroscience.",Neuroscience,http://arxiv.org/abs/2511.11609v1,arXiv,0
"Current neuroimaging studies on neurodegenerative diseases and psychological risk factors have been developed predominantly in non Hispanic White cohorts, with other populations markedly underrepresented. In this work, we construct directed hyper connectomes among large scale functional brain systems based on causal influences between brain regions, and examine their links to Alzheimer Disease progression and worry levels across racial groups. By using Health and Aging Brain Study Health Disparities (HABS HD) dataset, our experimental results suggest that neglecting racial variation in brain network architecture may reduce predictive performance in both cognitive and affective phenotypes. Important shared and population-specific hyper-connectome patterns related to both AD progression and worry levels were identified. We further observed distinct closed loop directed circuits across groups, suggesting that different populations may rely on distinct feedback based network regulation strategies when supporting cognition or managing emotional states. Together, these results indicate a common backbone of network vulnerability with population-dependent variations in regulatory coordination, underscoring the importance of population-aware neuroimaging models.",Neuroscience,http://arxiv.org/abs/2511.05548v1,arXiv,0
"Several novel methods, including magnetogenetics and magnetoelectric stimulation, use high frequency alternating magnetic fields to precisely manipulate neural activity. To quantify the behavioral effects of such interventions in a freely moving mouse, we developed a dual-channel magnetic chamber, specifically designed for rate-sensitive magnetothermal-genetic stimulation, and adaptable for other uses of alternating magnetic fields. Through an optimized coil design, the system allows independent control of two spatially orthogonal uniform magnetic fields delivered at different frequencies within a 10 cm x 10 cm x 6 cm chamber. The two channels have nominal frequencies of 50 and 550 kHz with peak magnetic field strengths of 88 and 12.5 mT, achieved with resonant coil drives having peak voltages of 1.6 and 1.8 kV and currents of 1.0 and 0.26 kA, respectively. Additionally, a liquid cooling system enables magnetic field generation for second-level duration, and an observation port and camera allow video capture of the animal's behavior within the chamber. The system generates high-amplitude magnetic fields across two widely separated frequency channels with negligible interference (< 1%). Relatively uniform magnetic field distribution (+/-10% across 94% of the chamber volume) is maintained throughout the chamber, and temperature increase of the inner side of the coil enclosure during the operation is limited to < 0.35 Â°C/s to ensure in vivo safety. Using cobalt-doped and undoped iron oxide nanoparticles, we demonstrate channel-specific heating rates of 3.5 Â°C/s and 1.5 Â°C/s, respectively, validating frequency-selectivity. Both channels can run continuously for four seconds stably.",Neuroscience,http://arxiv.org/abs/2511.00745v1,arXiv,0
"Neural dynamics underlie behaviors from memory to sleep, yet identifying mechanisms for higher-order phenomena (e.g., social interaction) is experimentally challenging. Existing whole-brain models often fail to scale to single-neuron resolution, omit behavioral readouts, or rely on PCA/conv pipelines that miss long-range, non-linear interactions. We introduce a sparse-attention whole-brain foundation model (SBM) for larval zebrafish that forecasts neuron spike probabilities conditioned on sensory stimuli and links brain state to behavior. SBM factorizes attention across neurons and along time, enabling whole-brain scale and interpretability. On a held-out subject, it achieves mean absolute error <0.02 with calibrated predictions and stable autoregressive rollouts. Coupled to a permutation-invariant behavior head, SBM enables gradient-based synthesis of neural patterns that elicit target behaviors. This framework supports rapid, behavior-grounded exploration of complex neural phenomena.",Neuroscience,http://arxiv.org/abs/2510.27366v1,arXiv,0
"Connectomics - the mapping of neural connections in an organism's brain - currently requires extraordinary human effort to proofread the data collected from imaging and machine-learning assisted segmentation. With the growing excitement around using AI agents to automate important scientific tasks, we explore whether current AI systems can perform multiple tasks necessary for data proofreading. We introduce ConnectomeBench, a multimodal benchmark evaluating large language model (LLM) capabilities in three critical proofreading tasks: segment type identification, split error correction, and merge error detection. Using expert annotated data from two large open-source datasets - a cubic millimeter of mouse visual cortex and the complete Drosophila brain - we evaluate proprietary multimodal LLMs including Claude 3.7/4 Sonnet, o4-mini, GPT-4.1, GPT-4o, as well as open source models like InternVL-3 and NVLM. Our results demonstrate that current models achieve surprisingly high performance in segment identification (52-82% balanced accuracy vs. 20-25% chance) and binary/multiple choice split error correction (75-85% accuracy vs. 50% chance) while generally struggling on merge error identification tasks. Overall, while the best models still lag behind expert performance, they demonstrate promising capabilities that could eventually enable them to augment and potentially replace human proofreading in connectomics. Project page: https://github.com/jffbrwn2/ConnectomeBench and Dataset https://huggingface.co/datasets/jeffbbrown2/ConnectomeBench/tree/main",Neuroscience,http://arxiv.org/abs/2511.05542v1,arXiv,0
"We model sensory streams as observations from high-dimensional stochastic dynamical systems and conceptualize sensory neurons as self-supervised learners of compact representations of such dynamics. From prior experience, neurons learn coherent sets-regions of stimulus state space whose trajectories evolve cohesively over finite times-and assign membership indices to new stimuli. Coherent sets are identified via spectral clustering of the stochastic Koopman operator (SKO), where the sign pattern of a subdominant singular function partitions the state space into minimally coupled regions. For multivariate Ornstein-Uhlenbeck processes, this singular function reduces to a linear projection onto the dominant singular vector of the whitened state-transition matrix. Encoding this singular vector as a receptive field enables neurons to compute membership indices via the projection sign in a biologically plausible manner. Each neuron detects either a predictive coherent set (stimuli with common futures) or a retrospective coherent set (stimuli with common pasts), suggesting a functional dichotomy among neurons. Since neurons lack access to explicit dynamical equations, the requisite singular vectors must be estimated directly from data, for example, via past-future canonical correlation analysis on lag-vector representations-an approach that naturally extends to nonlinear dynamics. This framework provides a novel account of neuronal temporal filtering, the ubiquity of rectification in neural responses, and known functional dichotomies. Coherent-set clustering thus emerges as a fundamental computation underlying sensory processing and transferable to bio-inspired artificial systems.",Neuroscience,http://arxiv.org/abs/2510.26955v1,arXiv,0
"This overview of integrated information theory (IIT) emphasizes IIT's ""consciousness-first"" approach to what exists. Consciousness demonstrates to each of us that something exists--experience--and reveals its essential properties--the axioms of phenomenal existence. IIT formulates these properties operationally, yielding the postulates of physical existence. To exist intrinsically or absolutely, an entity must have cause-effect power upon itself, in a specific, unitary, definite and structured manner. IIT's explanatory identity claims that an entity's cause-effect structure accounts for all properties of an experience--essential and accidental--with no additional ingredients. These include the feeling of spatial extendedness, temporal flow, of objects binding general concepts with particular configurations of features, and of qualia such as colors and sounds. IIT's intrinsic ontology has implications for understanding meaning, perception, and free will, for assessing consciousness in patients, infants, other species, and artifacts, and for reassessing our place in nature.",Neuroscience,http://arxiv.org/abs/2510.25998v4,arXiv,0
"Reconstructing images seen by people from their fMRI brain recordings provides a non-invasive window into the human brain. Despite recent progress enabled by diffusion models, current methods often lack faithfulness to the actual seen images. We present ""Brain-IT"", a brain-inspired approach that addresses this challenge through a Brain Interaction Transformer (BIT), allowing effective interactions between clusters of functionally-similar brain-voxels. These functional-clusters are shared by all subjects, serving as building blocks for integrating information both within and across brains. All model components are shared by all clusters & subjects, allowing efficient training with a limited amount of data. To guide the image reconstruction, BIT predicts two complementary localized patch-level image features: (i)high-level semantic features which steer the diffusion model toward the correct semantic content of the image; and (ii)low-level structural features which help to initialize the diffusion process with the correct coarse layout of the image. BIT's design enables direct flow of information from brain-voxel clusters to localized image features. Through these principles, our method achieves image reconstructions from fMRI that faithfully reconstruct the seen images, and surpass current SotA approaches both visually and by standard objective metrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve results comparable to current methods trained on full 40-hour recordings.",Neuroscience,http://arxiv.org/abs/2510.25976v1,arXiv,0
"In control problems and basic scientific modeling, it is important to compare observations with dynamical simulations. For example, comparing two neural systems can shed light on the nature of emergent computations in the brain and deep neural networks. Recently, Ostrow et al. (2023) introduced Dynamical Similarity Analysis (DSA), a method to measure the similarity of two systems based on their recurrent dynamics rather than geometry or topology. However, DSA does not consider how inputs affect the dynamics, meaning that two similar systems, if driven differently, may be classified as different. Because real-world dynamical systems are rarely autonomous, it is important to account for the effects of input drive. To this end, we introduce a novel metric for comparing both intrinsic (recurrent) and input-driven dynamics, called InputDSA (iDSA). InputDSA extends the DSA framework by estimating and comparing both input and intrinsic dynamic operators using a variant of Dynamic Mode Decomposition with control (DMDc) based on subspace identification. We demonstrate that InputDSA can successfully compare partially observed, input-driven systems from noisy data. We show that when the true inputs are unknown, surrogate inputs can be substituted without a major deterioration in similarity estimates. We apply InputDSA on Recurrent Neural Networks (RNNs) trained with Deep Reinforcement Learning, identifying that high-performing networks are dynamically similar to one another, while low-performing networks are more diverse. Lastly, we apply InputDSA to neural data recorded from rats performing a cognitive task, demonstrating that it identifies a transition from input-driven evidence accumulation to intrinsically-driven decision-making. Our work demonstrates that InputDSA is a robust and efficient method for comparing intrinsic dynamics and the effect of external input on dynamical systems.",Neuroscience,http://arxiv.org/abs/2510.25943v2,arXiv,0
"Depression is a major cause of global mental illness and significantly influences suicide rates. Timely and accurate diagnosis is essential for effective intervention. Electroencephalography (EEG) provides a non-invasive and accessible method for examining cerebral activity and identifying disease-associated patterns. We propose a novel graph-based deep learning framework, named Edge-gated, axis-mixed Pooling Attention Network (ExPANet), for differentiating major depressive disorder (MDD) patients from healthy controls (HC). EEG recordings undergo preprocessing to eliminate artifacts and are segmented into short periods of activity. We extract 14 features from each segment, which include time, frequency, fractal, and complexity domains. Electrodes are represented as nodes, whereas edges are determined by the phase-locking value (PLV) to represent functional connectivity. The generated brain graphs are examined utilizing an adapted graph attention network. This architecture acquires both localized electrode characteristics and comprehensive functional connectivity patterns. The proposed framework attains superior performance relative to current EEG-based approaches across two different datasets. A fundamental advantage of our methodology is its explainability. We evaluated the significance of features, channels, and edges, in addition to intrinsic attention weights. These studies highlight features, cerebral areas, and connectivity associations that are especially relevant to MDD, many of which correspond with clinical data. Our findings demonstrate a reliable and transparent method for EEG-based screening of MDD, using deep learning with clinically relevant results.",Neuroscience,http://arxiv.org/abs/2511.05537v1,arXiv,0
"Earth's gravity has fundamentally shaped human development by guiding the brain's integration of vestibular, visual, and proprioceptive inputs into an internal model of gravity: a dynamic neural representation enabling prediction and interpretation of gravitational forces. This work presents a dual computational framework to quantitatively model these adaptations. The first component is a lightweight Multi-Layer Perceptron (MLP) that predicts g-load-dependent changes in key electroencephalographic (EEG) frequency bands, representing the brain's cortical state. The second component utilizes a suite of independent Gaussian Processes (GPs) to model the body's broader physiological state, including Heart Rate Variability (HRV), Electrodermal Activity (EDA), and motor behavior. Both models were trained on data derived from a comprehensive review of parabolic flight literature, using published findings as anchor points to construct robust, continuous functions. To complement this quantitative analysis, we simulated subjective human experience under different gravitational loads, ranging from microgravity (0g) and partial gravity (Moon 0.17g, Mars 0.38g) to hypergravity associated with spacecraft launch and re-entry (1.8g), using a large language model (Claude 3.5 Sonnet). The model was prompted with physiological parameters to generate introspective narratives of alertness and self-awareness, which closely aligned with the quantitative findings from both the EEG and physiological models. This combined framework integrates quantitative physiological modeling with generative cognitive simulation, offering a novel approach to understanding and predicting human performance in altered gravity",Neuroscience,http://arxiv.org/abs/2511.05536v1,arXiv,0
"Brain disorders are an umbrella term for a group of neurological and psychiatric conditions that have a major effect on thinking, feeling, and acting. These conditions encompass a wide range of conditions. The illnesses in question pose significant difficulties not only for individuals, but also for healthcare systems all across the world. In this study, we explore the capability of explainable machine learning for classification of people who suffer from brain disorders. This is accomplished by the utilization of brain connection map, also referred as connectome, derived from functional magnetic resonance imaging (fMRI) data. In order to analyze features that are based on the connectome, we investigated several different feature selection procedures. These strategies included the Least Absolute Shrinkage and Selection Operator (LASSO), Relief, and Analysis of Variance (ANOVA), in addition to a logistic regression (LR) classifier. First and foremost, the purpose was to evaluate and contrast the classification accuracy of different feature selection methods in terms of distinguishing healthy controls from diseased individuals. The evaluation of the stability of the traits that were chosen was the second objective. The identification of the regions of the brain that have an effect on the classification was the third main objective. When applied to the UCLA dataset, the LASSO approach, which is our most effective strategy, produced a classification accuracy of 91.85% and a stability index of 0.74, which is greater than the results obtained by other approaches: Relief and ANOVA. These methods are effective in locating trustworthy biomarkers, which adds to the development of connectome-based classification in the context of issues that impact the brain.",Neuroscience,http://arxiv.org/abs/2511.05531v1,arXiv,0
"It is known that listeners lose the ability to discriminate the direction of motion of a revolving sound (clockwise vs. counterclockwise) beyond a critical velocity (""the upper limit""), primarily due to degraded front-back discrimination. Little is known about how this ability is affected by simultaneously present distractor sounds, despite the real-life importance of tracking moving sounds in the presence of distractors. We hypothesized that the presence of a static distractor sound would impair the perception of moving target sounds and reduce the upper limit, and show that this is indeed the case. A distractor on the right was as effective as a distractor at the front in reducing the upper limit despite the importance of resolving front-back confusions. By manipulating the spectral content of both the target and distractor, we found that the upper limit was reduced if and only if the distractor spectrally overlaps with the target in the frequency range relevant for front/back discrimination; energetic masking thus explains the upper limit reduction by the distractor. We did not find any evidence for informational masking by the distractor. Our findings form the first steps towards a better understanding of the tracking of multiple sounds in the presence of distractors.",Neuroscience,http://arxiv.org/abs/2510.25119v2,arXiv,0
"The rapid aging of societies is intensifying demand for autonomous care robots; however, most existing systems are task-specific and rely on handcrafted preprocessing, limiting their ability to generalize across diverse scenarios. A prevailing theory in cognitive neuroscience proposes that the human brain operates through hierarchical predictive processing, which underlies flexible cognition and behavior by integrating multimodal sensory signals. Inspired by this principle, we introduce a hierarchical multimodal recurrent neural network grounded in predictive processing under the free-energy principle, capable of directly integrating over 30,000-dimensional visuo-proprioceptive inputs without dimensionality reduction. The model was able to learn two representative caregiving tasks, rigid-body repositioning and flexible-towel wiping, without task-specific feature engineering. We demonstrate three key properties: (i) self-organization of hierarchical latent dynamics that regulate task transitions, capture variability in uncertainty, and infer occluded states; (ii) robustness to degraded vision through visuo-proprioceptive integration; and (iii) asymmetric interference in multitask learning, where the more variable wiping task had little influence on repositioning, whereas learning the repositioning task led to a modest reduction in wiping performance, while the model maintained overall robustness. Although the evaluation was limited to simulation, these results establish predictive processing as a universal and scalable computational principle, pointing toward robust, flexible, and autonomous caregiving robots while offering theoretical insight into the human brain's ability to achieve flexible adaptation in uncertain real-world environments.",Neuroscience,http://arxiv.org/abs/2510.25053v1,arXiv,0
"In this work, we examine the conditions for the emergence of chimera-like states in Ising systems. We study an Ising chain with periodic boundaries in contact with a thermal bath at temperature T, that induces stochastic changes in spin variables. To capture the non-locality needed for chimera formation, we introduce a model setup with non-local diffusion of spin values through the whole system. More precisely, diffusion is modeled through spin-exchange interactions between units up to a distance R, using Kawasaki dynamics. This setup mimics, e.g., neural media, as the brain, in the presence of electrical (diffusive) interactions. We explored the influence of such non-local dynamics on the emergence of complex spatiotemporal synchronization patterns of activity. Depending on system parameters we report here for the first time chimera-like states in the Ising model, characterized by relatively stable moving domains of spins with different local magnetization. We analyzed the system at T=0, both analytically and via simulations and computed the system's phase diagram, revealing rich behavior: regions with only chimeras, coexistence of chimeras and stable domains, and metastable chimeras that decay into uniform stable domains. This study offers fundamental insights into how coherent and incoherent synchronization patterns can arise in complex networked systems as it is, e.g., the brain.",Neuroscience,http://arxiv.org/abs/2510.24903v1,arXiv,0
"We present a unified field-theoretic framework for the dynamics of activity and connectivity in interacting neuronal systems. Building upon previous works, where a field approach to activity--connectivity dynamics, formation of collective states and effective fields of collective states were successively introduced, the present paper synthesizes and extends these results toward a general description of multiple hierarchical collective structures. Starting with the dynamical system representing collective states in terms of connections, activity levels, and internal frequencies, we analyze its stability, emphasizing the possibility of transitions between configurations. Then, turning to the field formalism of collective states, we extend this framework to include substructures (subobjects) participating in larger assemblies while retaining intrinsic properties. We define activation classes describing compatible or independent activity patterns between objects and subobjects, and study stability conditions arising from their alignment or mismatch. The global system is described as the collection of landscapes of coexisting and interacting collective states, each characterized both by continuous (activity, frequency) and discrete (class) variables. A corresponding field formalism is developed, with an action functional incorporating both internal dynamics and interaction terms. This nonlinear field model captures cascading transitions between collective states and the formation of composite structures, providing a coherent theoretical basis for emergent neuronal assemblies and their mutual couplings.",Neuroscience,http://arxiv.org/abs/2510.24896v1,arXiv,0
"Object binding, the brain's ability to bind the many features that collectively represent an object into a coherent whole, is central to human cognition. It groups low-level perceptual features into high-level object representations, stores those objects efficiently and compositionally in memory, and supports human reasoning about individual object instances. While prior work often imposes object-centric attention (e.g., Slot Attention) explicitly to probe these benefits, it remains unclear whether this ability naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they could: recognizing which patches belong to the same object should be useful for downstream prediction and thus guide attention. Motivated by the quadratic nature of self-attention, we hypothesize that ViTs represent whether two patches belong to the same object, a property we term IsSameObject. We decode IsSameObject from patch embeddings across ViT layers using a similarity probe, which reaches over 90% accuracy. Crucially, this object-binding capability emerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker in ImageNet-supervised models, suggesting that binding is not a trivial architectural artifact, but an ability acquired through specific pretraining objectives. We further discover that IsSameObject is encoded in a low-dimensional subspace on top of object features, and that this signal actively guides attention. Ablating IsSameObject from model activations degrades downstream performance and works against the learning objective, implying that emergent object binding naturally serves the pretraining objective. Our findings challenge the view that ViTs lack object binding and highlight how symbolic knowledge of ""which parts belong together"" emerges naturally in a connectionist system.",Neuroscience,http://arxiv.org/abs/2510.24709v1,arXiv,0
"We ask where, and under what conditions, dyslexic reading costs arise in a large-scale naturalistic reading dataset. Using eye-tracking aligned to word-level features (word length, frequency, and predictability), we model how each feature influences dyslexic time costs. We find that all three features robustly change reading times in both typical and dyslexic readers, and that dyslexic readers show stronger sensitivities to each, especially predictability. Counterfactual manipulations of these features substantially narrow the dyslexic-control gap by about one third, with predictability showing the strongest effect, followed by length and frequency. These patterns align with dyslexia theories that posit heightened demands on linguistic working memory and phonological encoding, and they motivate further work on lexical complexity and parafoveal preview benefits to explain the remaining gap. In short, we quantify when extra dyslexic costs arise, how large they are, and offer actionable guidance for interventions and computational models for dyslexics.",Neuroscience,http://arxiv.org/abs/2510.24647v1,arXiv,0
"We explore whether neural networks can decode brain activity into speech by mapping EEG recordings to audio representations. Using EEG data recorded as subjects listened to natural speech, we train a model with a contrastive CLIP loss to align EEG-derived embeddings with embeddings from a pre-trained transformer-based speech model. Building on the state-of-the-art EEG decoder from Meta, we introduce three architectural modifications: (i) subject-specific attention layers (+0.15% WER improvement), (ii) personalized spatial attention (+0.45%), and (iii) a dual-path RNN with attention (-1.87%). Two of the three modifications improved performance, highlighting the promise of personalized architectures for brain-to-speech decoding and applications in brain-computer interfaces.",Neuroscience,http://arxiv.org/abs/2511.04691v1,arXiv,0
"Boundary Vector Cells (BVCs) are a class of neurons in the brains of vertebrates that encode environmental boundaries at specific distances and allocentric directions, playing a central role in forming place fields in the hippocampus. Most computational BVC models are restricted to two-dimensional (2D) environments, making them prone to spatial ambiguities in the presence of horizontal symmetries in the environment. To address this limitation, we incorporate vertical angular sensitivity into the BVC framework, thereby enabling robust boundary detection in three dimensions, and leading to significantly more accurate spatial localization in a biologically-inspired robot model.   The proposed model processes LiDAR data to capture vertical contours, thereby disambiguating locations that would be indistinguishable under a purely 2D representation. Experimental results show that in environments with minimal vertical variation, the proposed 3D model matches the performance of a 2D baseline; yet, as 3D complexity increases, it yields substantially more distinct place fields and markedly reduces spatial aliasing. These findings show that adding a vertical dimension to BVC-based localization can significantly enhance navigation and mapping in real-world 3D spaces while retaining performance parity in simpler, near-planar scenarios.",Neuroscience,http://arxiv.org/abs/2510.24029v1,arXiv,0
"A critical visual computation is to construct global scene properties from activities of early visual cortical neurons which have small receptive fields. Such a computation is enabled by contextual influences, through which a neuron's response to visual inputs is influenced by contextual inputs outside its classical receptive fields. Accordingly, neurons can signal global properties including visual saliencies and figure-ground relationships. Many believe that intracortical axons conduct signals too slowly to bring the contextual information from receptive fields of other neurons. A popular opinion is that much of the contextual influences arise from feedback from higher visual areas whose neurons have larger receptive fields. This paper re-examines pre-existing data to reveal these unexpected findings: the conduction speed of V1 intracortical axons increases approximately linearly with the conduction distance, and is sufficiently high for conveying the contextual influences. Recognizing the importance of intracortical contribution to critical visual computations should enable fresh progress in answering long-standing questions.",Neuroscience,http://arxiv.org/abs/2510.23391v2,arXiv,0
"Linearly transforming stimulus representations of deep neural networks yields high-performing models of behavioral and neural responses to complex stimuli. But does the test accuracy of such predictions identify genuine representational alignment? We addressed this question through a large-scale model-recovery study. Twenty diverse vision models were linearly aligned to 4.5 million behavioral judgments from the THINGS odd-one-out dataset and calibrated to reproduce human response variability. For each model in turn, we sampled synthetic responses from its probabilistic predictions, fitted all candidate models to the synthetic data, and tested whether the data-generating model would re-emerge as the best predictor of the simulated data. Model recovery accuracy improved with training-set size but plateaued below 80%, even at millions of simulated trials. Regression analyses linked misidentification primarily to shifts in representational geometry induced by the linear transformation, as well as to the effective dimensionality of the transformed features. These findings demonstrate that, even with massive behavioral data, overly flexible alignment metrics may fail to guide us toward artificial representations that are genuinely more human-aligned. Model comparison experiments must be designed to balance the trade-off between predictive accuracy and identifiability-ensuring that the best-fitting model is also the right one.",Neuroscience,http://arxiv.org/abs/2510.23321v1,arXiv,0
"In this study, we propose the use of persistent homology -- specifically Betti curves for brain age prediction and for distinguishing between healthy and pathological aging. The proposed framework is applied to 100 structural MRI scans from the publicly available ADNI dataset. Our results indicate that Betti curve features, particularly those from dimension-1 (connected components) and dimension-2 (1D holes), effectively capture structural brain alterations associated with aging. Furthermore, clinical features are grouped into three categories based on their correlation, or lack thereof, with (i) predicted brain age and (ii) chronological age. The findings demonstrate that this approach successfully differentiates normal from pathological aging and provides a novel framework for understanding how structural brain changes relate to cognitive impairment. The proposed method serves as a foundation for developing potential biomarkers for early detection and monitoring of cognitive decline.",Neuroscience,http://arxiv.org/abs/2511.05520v1,arXiv,0
"Neural recording implants are a crucial tool for both neuroscience research and enabling new clinical applications. The power consumption of high channel count implants is dominated by the circuits used to amplify and digitize neural signals. Since circuit designers have pushed the efficiency of these circuits close to the theoretical physical limits, reducing power further requires system level optimization. Recent advances use a strategy called channel selection, in which less important channels are turned off to save power. We demonstrate resolution reconfiguration, in which the resolution of less important channels is scaled down to save power. Our approach leverages variable importance of each channel inside machine-learning-based decoders and we trial this methodology across three applications: seizure detection, gesture recognition, and force regression. With linear decoders, resolution reconfiguration saves 8.7x, 12.8x, and 23.0x power compared to a traditional recording array for each task respectively. It further saves 1.6x, 3.4x, and 5.2x power compared to channel selection. The results demonstrate the power benefits of resolution reconfigurable front-ends and their wide applicability to neural decoding problems.",Neuroscience,http://arxiv.org/abs/2510.22924v1,arXiv,0
"Background: Upper-limb weakness and tremor (4--12 Hz) limit activities of daily living (ADL) and reduce adherence to home rehabilitation. Objective: To assess technical feasibility and clinician-relevant signals of a sensor-fused wearable targeting the triceps brachii and extensor pollicis brevis. Methods: A lightweight node integrates surface EMG (1 kHz), IMU (100--200 Hz), and flex/force sensors with on-device INT8 inference (Tiny 1D-CNN/Transformer) and a safety-bounded assist policy (angle/torque/jerk limits; stall/time-out). Healthy adults (n = 12) performed three ADL-like tasks. Primary outcomes: Tremor Index (TI), range of motion (ROM), repetitions (Reps min$^{-1}$). Secondary: EMG median-frequency slope (fatigue trend), closed-loop latency, session completion, and device-related adverse events. Analyses used subject-level paired medians with BCa 95\% CIs; exact Wilcoxon $p$-values are reported in the Results. Results: Assistance was associated with lower tremor prominence and improved task throughput: TI decreased by $-0.092$ (95\% CI [$-0.102$, $-0.079$]), ROM increased by $+12.65\%$ (95\% CI [$+8.43$, $+13.89$]), and Reps rose by $+2.99$ min$^{-1}$ (95\% CI [$+2.61$, $+3.35$]). Median on-device latency was 8.7 ms at a 100 Hz loop rate; all sessions were completed with no device-related adverse events. Conclusions: Multimodal sensing with low-latency, safety-bounded assistance produced improved movement quality (TI $\downarrow$) and throughput (ROM, Reps $\uparrow$) in a pilot technical-feasibility setting, supporting progression to IRB-approved patient studies. Trial registration: Not applicable (pilot non-clinical).",Neuroscience,http://arxiv.org/abs/2510.22913v1,arXiv,0
"Understanding how the human brain progresses from processing simple linguistic inputs to performing high-level reasoning is a fundamental challenge in neuroscience. While modern large language models (LLMs) are increasingly used to model neural responses to language, their internal representations are highly ""entangled,"" mixing information about lexicon, syntax, meaning, and reasoning. This entanglement biases conventional brain encoding analyses toward linguistically shallow features (e.g., lexicon and syntax), making it difficult to isolate the neural substrates of cognitively deeper processes. Here, we introduce a residual disentanglement method that computationally isolates these components. By first probing an LM to identify feature-specific layers, our method iteratively regresses out lower-level representations to produce four nearly orthogonal embeddings for lexicon, syntax, meaning, and, critically, reasoning. We used these disentangled embeddings to model intracranial (ECoG) brain recordings from neurosurgical patients listening to natural speech. We show that: 1) This isolated reasoning embedding exhibits unique predictive power, accounting for variance in neural activity not explained by other linguistic features and even extending to the recruitment of visual regions beyond classical language areas. 2) The neural signature for reasoning is temporally distinct, peaking later (~350-400ms) than signals related to lexicon, syntax, and meaning, consistent with its position atop a processing hierarchy. 3) Standard, non-disentangled LLM embeddings can be misleading, as their predictive success is primarily attributable to linguistically shallow features, masking the more subtle contributions of deeper cognitive processing.",Neuroscience,http://arxiv.org/abs/2510.22860v2,arXiv,0
"In threshold-linear networks (TLNs), a fixed point is called minimal if no proper subset of its support is also a fixed point. Curto et al (Advances in Applied Mathematics, 2024) conjectured that every stable fixed point of any TLN must be a minimal fixed point. We provide a counterexample to this conjecture: an explicit competitive TLN on 3 neurons that exhibits a stable fixed point whose support is not minimal (it contains the support of another stable fixed point). We prove that there is no competitive TLN on 2 neurons which contains a stable non-minimal fixed point, so our 3-neuron construction is the smallest such example. By expanding our base example, we show for any positive integers $i, j$ with $i < j-1$ that there exists a competitive TLN with stable fixed point supports $Ï\subsetneq Ï$ for which $|Ï| = i$ and $|Ï| = j$. Using a different expansion of our base example, we also show that chains of nested stable fixed points in competitive TLNs can be made arbitrarily long.",Neuroscience,http://arxiv.org/abs/2511.05517v1,arXiv,0
"Effective communication is central to achieving positive healthcare outcomes in mental health contexts, yet international students often face linguistic and cultural barriers that hinder their communication of mental distress. In this study, we evaluate the effectiveness of AI-generated images in supporting self-expression of mental distress. To achieve this, twenty Chinese international students studying at UK universities were invited to describe their personal experiences of mental distress. These descriptions were elaborated using GPT-4o with four persona-based prompt templates rooted in contemporary counselling practice to generate corresponding images. Participants then evaluated the helpfulness of generated images in facilitating the expression of their feelings based on their original descriptions. The resulting dataset comprises 100 textual descriptions of mental distress, 400 generated images, and corresponding human evaluation scores. Findings indicate that prompt design substantially affects perceived helpfulness, with the illustrator persona achieving the highest ratings. This work introduces the first publicly available text-to-image evaluation dataset with human judgment scores in the mental health domain, offering valuable resources for image evaluation, reinforcement learning with human feedback, and multi-modal research on mental health communication.",Neuroscience,http://arxiv.org/abs/2512.04087v1,arXiv,0
"Understanding how creativity is represented in the brain's intrinsic functional architecture remains a central challenge in cognitive neuroscience. While resting-state fMRI studies have revealed large-scale network correlates of creative potential, electroencephalography (EEG) offers a temporally precise and scalable approach to capture the fast oscillatory dynamics that underlie spontaneous neural organization. In this study, we used a data-driven network approach to examine whether resting-state EEG connectivity patterns differentiate individuals according to their creative abilities. Creativity was evaluated by: The Inventory of Creative Activities and Achievements (ICAA), The Divergent Association Task (DAT), The Matchstick Arithmetic Puzzles Task (MAPT) and Self-rating (SR) of creative ability in 30 healthy young adults. Graph-theoretical analyses were applied to functional connectivity matrices and clustered based on graph similarity. Two distinct participant clusters emerged, differing systematically across multiple dimensions of creativity. Cluster 1, characterized by consistently higher performance across multiple creativity variables (ICAA, DAT, MAPT and SR), showed broad alpha-band hypoconnectivity, relatively preserved left frontal connectivity and greater network modularity. Cluster 0, associated with lower creativity scores, exhibited stronger overall connectivity strength, reduced modularity and higher local clustering. These findings suggest that resting-state EEG connectivity patterns can index stable cognitive traits such as creativity. More broadly, they point to an intrinsic neural signature of adaptive brain function marked by efficient yet flexible network organization that may support creative and adaptive cognition.",Neuroscience,http://arxiv.org/abs/2510.22364v1,arXiv,0
"We present a lateral ventricular brain-computer interface (LV-BCI) that deploys an expandable, flexible electrode into the lateral ventricle through a minimally invasive external ventricular drainage pathway. Inspired by the framework of traditional Chinese lanterns, the electrode expands uniformly within the ventricle and conforms to the ependymal wall. Compared with conventional subdural ECoG electrodes, the LV-BCI shows superior signal stability and immunocompatibility. Resting-state spectral analyses revealed a maximum effective bandwidth comparable to subdural ECoG. In evoked potential tests, the LV-BCI maintained a consistently higher signal-to-noise ratio over 112 days without the decline typically associated with scarring or other immune responses. Immunohistochemistry showed only a transient, early microglial activation after implantation, returning to control levels and remaining stable through 168 days. We further designed an ""action-memory T-maze"" task and developed a microstate sequence classifier (MSSC) to predict rats' turn decisions. The LV-BCI achieved prediction accuracy up to 98%, significantly outperforming subdural ECoG, indicating enhanced access to decision-related information from deep structures such as the hippocampus. These results establish the lateral ventricle as a viable route for neural signal acquisition. Using a lantern-inspired flexible electrode, we achieve long-term stable recordings and robust memory decision decoding from within the ventricular system, opening new directions for BCI technology and systems neuroscience.",Neuroscience,http://arxiv.org/abs/2510.22262v1,arXiv,0
"Task-specific pre-training is essential when task representations diverge from generic pre-training features. Existing task-general pre-training EEG models struggle with complex tasks like emotion recognition due to mismatches between task-specific features and broad pre-training approaches. This work aims to develop a task-specific multi-dataset joint pre-training framework for cross-dataset emotion recognition, tackling problems of large inter-dataset distribution shifts, inconsistent emotion category definitions, and substantial inter-subject variability. We introduce a cross-dataset covariance alignment loss to align second-order statistical properties across datasets, enabling robust generalization without the need for extensive labels or per-subject calibration. To capture the long-term dependency and complex dynamics of EEG, we propose a hybrid encoder combining a Mamba-like linear attention channel encoder and a spatiotemporal dynamics model. Our method outperforms state-of-the-art large-scale EEG models by an average of 4.57% in AUROC for few-shot emotion recognition and 11.92% in accuracy for zero-shot generalization to a new dataset. Performance scales with the increase of datasets used in pre-training. Multi-dataset joint pre-training achieves a performance gain of 8.55% over single-dataset training. This work provides a scalable framework for task-specific pre-training and highlights its benefit in generalizable affective computing. Our code is available at https://github.com/ncclab-sustech/mdJPT_nips2025.",Neuroscience,http://arxiv.org/abs/2510.22197v1,arXiv,0
"Learning a compact representation of history is critical for planning and generalization in partially observable environments. While meta-reinforcement learning (RL) agents can attain near Bayes-optimal policies, they often fail to learn the compact, interpretable Bayes-optimal belief states. This representational inefficiency potentially limits the agent's adaptability and generalization capacity. Inspired by predictive coding in neuroscience--which suggests that the brain predicts sensory inputs as a neural implementation of Bayesian inference--and by auxiliary predictive objectives in deep RL, we investigate whether integrating self-supervised predictive coding modules into meta-RL can facilitate learning of Bayes-optimal representations. Through state machine simulation, we show that meta-RL with predictive modules consistently generates more interpretable representations that better approximate Bayes-optimal belief states compared to conventional meta-RL across a wide variety of tasks, even when both achieve optimal policies. In challenging tasks requiring active information seeking, only meta-RL with predictive modules successfully learns optimal representations and policies, whereas conventional meta-RL struggles with inadequate representation learning. Finally, we demonstrate that better representation learning leads to improved generalization. Our results strongly suggest the role of predictive learning as a guiding principle for effective representation learning in agents navigating partial observability.",Neuroscience,http://arxiv.org/abs/2510.22039v1,arXiv,0
"Wilson-Cowan and Amari-type models capture nonlinear neural population dynamics, providing a fundamental framework for modeling how sensory and other exogenous inputs shape activity in neural tissue. We study the controllability properties of Amari-type neural fields subject to piecewise/constant-in-time inputs. The model describes the time evolution of the polarization of neural tissue within a spatial continuum, with synaptic interactions represented by a convolution kernel. We study the synthesis of piecewise/constant-in-time inputs to achieve two-point boundary-type control objectives, namely, steering neural activity from an initial state to a prescribed target state. This approach is particularly relevant for predicting the emergence of paradoxical neural representations, such as discordant visual illusions that occur in response to overt sensory stimuli. We first present a control synthesis based on the Banach fixed-point theorem, which yields an iterative construction of a constant-in-time input under minimal regularity assumptions on the kernel and transfer function; however, it exhibits practical limitations, even in the linear case. To overcome these challenges, we then develop a generic synthesis framework based on the flow of neural dynamics drift, enabling explicit piecewise constant and constant-in-time inputs. Extensive numerical results in one and two spatial dimensions confirm the effectiveness of the proposed syntheses and demonstrate their superior performance compared to inputs derived from naive linearization at the initial or target states when these states are not equilibria of the drift dynamics. By providing a mathematically rigorous framework for controlling Amari-type neural fields, this work advances our understanding of nonlinear neural population control with potential applications in computational neuroscience, psychophysics, and neurostimulation.",Neuroscience,http://arxiv.org/abs/2510.22022v1,arXiv,0
"Recalling previously experienced movements is essential for a range of activities, including sports, music, and rehabilitation, yet little is known about the accuracy and decay of proprioceptive working memory. We examined how introducing a short-term memory component affected movement reproduction accuracy by comparing movement reproduction under two conditions: simultaneous reproduction (SimRep) and memorized reproduction (MemRep). In Experiment 1 (N = 191), participants felt a 5-s haptic trajectory with one hand and reproduced it with the other hand simultaneously or immediately after the template ended. Errors were greater in MemRep than SimRep (31.1 deg vs. 21.5 deg, p < 0.001). MemRep trajectories showed systematic temporal distortions: participants lagged fast movements and led slow ones (R = -0.32, p = 0.01), unlike the ~279 ms lag in SimRep. In Experiment 2 (N = 33), we varied template durations (2-8 s). Longer durations increased error for MemRep but not SimRep (p < 0.001). During MemRep, accuracy declined steadily, with replay-template correlations dropping from ~0.4 to ~0.1 over ~3 s, while SimRep correlations rose from ~0.25 to ~0.6. In ~10% of MemRep templates, participants moved in the wrong direction initially, especially for low-amplitude movements (p < 0.001). Templates with more than four movements showed element omission; after four movements had been reproduced participants ceased movement prematurely, affecting up to 40% of 8-s templates. These findings show that transferring proprioceptive experiences into working memory introduces systematic temporal and structural distortions. Accuracy decays within seconds, and memory span for movement trajectories was limited to four movements.",Neuroscience,http://arxiv.org/abs/2510.21996v1,arXiv,0
"Biological and artificial learners are inherently exposed to a stream of data and experience throughout their lifetimes and must constantly adapt to, learn from, or selectively ignore the ongoing input. Recent findings reveal that, even when the performance remains stable, the underlying neural representations can change gradually over time, a phenomenon known as representational drift. Studying the different sources of data and noise that may contribute to drift is essential for understanding lifelong learning in neural systems. However, a systematic study of drift across architectures and learning rules, and the connection to task, are missing. Here, in an online learning setup, we characterize drift as a function of data distribution, and specifically show that the learning noise induced by task-irrelevant stimuli, which the agent learns to ignore in a given context, can create long-term drift in the representation of task-relevant stimuli. Using theory and simulations, we demonstrate this phenomenon both in Hebbian-based learning -- Oja's rule and Similarity Matching -- and in stochastic gradient descent applied to autoencoders and a supervised two-layer network. We consistently observe that the drift rate increases with the variance and the dimension of the data in the task-irrelevant subspace. We further show that this yields different qualitative predictions for the geometry and dimension-dependency of drift than those arising from Gaussian synaptic noise. Overall, our study links the structure of stimuli, task, and learning rule to representational drift and could pave the way for using drift as a signal for uncovering underlying computation in the brain.",Neuroscience,http://arxiv.org/abs/2510.21588v1,arXiv,0
"Foundation models have transformed AI by reducing reliance on task-specific data through large-scale pretraining. While successful in language and vision, their adoption in EEG has lagged due to the heterogeneity of public datasets, which are collected under varying protocols, devices, and electrode configurations. Existing EEG foundation models struggle to generalize across these variations, often restricting pretraining to a single setup, resulting in suboptimal performance, in particular under linear probing. We present REVE (Representation for EEG with Versatile Embeddings), a pretrained model explicitly designed to generalize across diverse EEG signals. REVE introduces a novel 4D positional encoding scheme that enables it to process signals of arbitrary length and electrode arrangement. Using a masked autoencoding objective, we pretrain REVE on over 60,000 hours of EEG data from 92 datasets spanning 25,000 subjects, representing the largest EEG pretraining effort to date. REVE achieves state-of-the-art results on 10 downstream EEG tasks, including motor imagery classification, seizure detection, sleep staging, cognitive load estimation, and emotion recognition. With little to no fine-tuning, it demonstrates strong generalization, and nuanced spatio-temporal modeling. We release code, pretrained weights, and tutorials to support standardized EEG research and accelerate progress in clinical neuroscience.",Neuroscience,http://arxiv.org/abs/2510.21585v1,arXiv,0
"Transcranial photobiomodulation (tPBM) therapy is an emerging, non-invasive neuromodulation technique that has demonstrated considerable potential in the field of neuropsychiatric disorders. Several studies have found that pulsed wave (PW) tPBM therapy yields superior biomodulatory effects. However, its neural mechanisms are still unknown which poses a significant barrier to the development of an optimized protocol. A randomized, single-blind study including 29 participants was conducted using a crossover design, with sham and continuous wave (CW) groups as controls. The EEG microstate analysis was utilized to explore the relative variations in temporal parameters and brain functional connectivity. To further elucidate the dynamic activity patterns of microstates, a 10-repeat 10-fold cross-validation with nine machine learning algorithms and kernel Shapley additive explanations analysis was employed. Results indicated that the pulsed wave mode enhanced the global efficiency, local efficiency, and betweenness centrality of microstate C in brain functional networks as well as the mean durations parameter achieving a middle to large effect size, with superior effects compared to the sham and continuous wave groups. Furthermore, the support vector machine based on the radial basis function method with kernel Shapley additive explanations analysis demonstrated the best performance with an area under the curve (AUC) reaching 0.956, and found that the 8 of top-10 microstate features related to microstate C contributed most significantly to the PW mode. In conclusion, the EEG microstate analysis found that PW tPBM therapy modulates the microstate C-specific patterns in the human brain, suggesting that microstate dynamics may serve as a state-dependent biomarker for the optimization of tPBM protocol.",Neuroscience,http://arxiv.org/abs/2510.21265v2,arXiv,0
"A fine-grained account of functional selectivity in the cortex is essential for understanding how visual information is processed and represented in the brain. Classical studies using designed experiments have identified multiple category-selective regions; however, these approaches rely on preconceived hypotheses about categories. Subsequent data-driven discovery methods have sought to address this limitation but are often limited by simple, typically linear encoding models. We propose an in silico approach for data-driven discovery of novel category-selectivity hypotheses based on an encoder-decoder transformer model. The architecture incorporates a brain-region to image-feature cross-attention mechanism, enabling nonlinear mappings between high-dimensional deep network features and semantic patterns encoded in the brain activity. We further introduce a method to characterize the selectivity of individual parcels by leveraging diffusion-based image generative models and large-scale datasets to synthesize and select images that maximally activate each parcel. Our approach reveals regions with complex, compositional selectivity involving diverse semantic concepts, which we validate in silico both within and across subjects. Using a brain encoder as a ""digital twin"" offers a powerful, data-driven framework for generating and testing hypotheses about visual selectivity in the human brain - hypotheses that can guide future fMRI experiments. Our code is available at: https://kriegeskorte-lab.github.io/in-silico-mapping/ .",Neuroscience,http://arxiv.org/abs/2510.21142v1,arXiv,0
"As artificial intelligence (AI) advances toward superhuman capabilities, aligning these systems with human values becomes increasingly critical. Current alignment strategies rely largely on externally specified constraints that may prove insufficient against future super-intelligent AI capable of circumventing top-down controls.   This research investigates whether artificial neural networks (ANNs) can develop patterns analogous to biological mirror neurons cells that activate both when performing and observing actions, and how such patterns might contribute to intrinsic alignment in AI. Mirror neurons play a crucial role in empathy, imitation, and social cognition in humans. The study therefore asks: (1) Can simple ANNs develop mirror-neuron patterns? and (2) How might these patterns contribute to ethical and cooperative decision-making in AI systems?   Using a novel Frog and Toad game framework designed to promote cooperative behaviors, we identify conditions under which mirror-neuron patterns emerge, evaluate their influence on action circuits, introduce the Checkpoint Mirror Neuron Index (CMNI) to quantify activation strength and consistency, and propose a theoretical framework for further study.   Our findings indicate that appropriately scaled model capacities and self/other coupling foster shared neural representations in ANNs similar to biological mirror neurons. These empathy-like circuits support cooperative behavior and suggest that intrinsic motivations modeled through mirror-neuron dynamics could complement existing alignment techniques by embedding empathy-like mechanisms directly within AI architectures.",Neuroscience,http://arxiv.org/abs/2511.01885v2,arXiv,0
"The Poisson Generalized Linear Model (GLM) is a foundational tool for analyzing neural spike train data. However, standard implementations rely on discretizing spike times into binned count data, limiting temporal resolution and scalability. Here, we develop Monte Carlo (MC) methods and polynomial approximations (PA) to the continuous-time analog of these models, and show them to be advantageous over their discrete-time counterparts. Further, we propose using a set of exponentially scaled Laguerre polynomials as an orthogonal temporal basis, which improves filter identification and yields closed-form integral solutions under the polynomial approximation. Applied to both synthetic and real spike-time data from rodent hippocampus, our methods demonstrate superior accuracy and scalability compared to traditional binned GLMs, enabling functional connectivity inference in large-scale neural recordings that are temporally precise on the order of synaptic dynamical timescales and in agreement with known anatomical properties of hippocampal subregions. We provide open-source implementations of both MC and PA estimators, optimized for GPU acceleration, to facilitate adoption in the neuroscience community.",Neuroscience,http://arxiv.org/abs/2510.20966v1,arXiv,0
"The prevalence of online learning poses a vital challenge in real-time monitoring of students' concentration. Traditional methods such as questionnaire assessments require manual intervention, and webcam-based monitoring fails to provide accurate insights about learners' mental focus as it is deceived by mere screen fixation without cognitive engagement. Existing BCI-based approaches lack real-time validation and evaluation procedures. To address these limitations, a Brain-Computer Interface (BCI) system is developed using a non-invasive Electroencephalogram (EEG) headband, FocusCalm, to record brainwave activity under attentive and non-attentive states. 20 minutes of data were collected from each of 20 participants watching a pre-recorded educational video. The data validation employed a novel intra-video questionnaire assessment. Subsequently, collected signals were segmented (sliding window), filtered (Butterworth bandpass), and cleaned (removal of high-amplitude and EOG artifacts such as eye blinks). Time, frequency, wavelet, and statistical features were extracted, followed by recursive feature elimination (RFE) with support vector machines (SVMs) to classify attention and non-attention states. The leave-one-subject-out (LOSO) cross-validation accuracy was found to be 88.77%. The system provides feedback alerts upon detection of a non-attention state and maintains focus profile logs. A pilot study was conducted to evaluate the effectiveness of real-time feedback. Five participants underwent a 10-minute session comprising a 5-minute baseline phase devoid of feedback, succeeded by a 5-minute feedback phase, during which alerts were activated if participants exhibited inattention for approximately 8 consecutive seconds. A paired t-test (t = 5.73, p = 0.007) indicated a statistically significant improvement in concentration during the feedback phase.",Neuroscience,http://arxiv.org/abs/2510.20958v2,arXiv,0
"The ability to continually learn, retain and deploy skills to accomplish goals is a key feature of intelligent and efficient behavior. However, the neural mechanisms facilitating the continual learning and flexible (re-)composition of skills remain elusive. Here, we study continual learning and the compositional reuse of learned computations in recurrent neural network (RNN) models using a novel two-system approach: one system that infers what computation to perform, and one that implements how to perform it. We focus on a set of compositional cognitive tasks commonly studied in neuroscience. To construct the what system, we first show that a large family of tasks can be systematically described by a probabilistic generative model, where compositionality stems from a shared underlying vocabulary of discrete task epochs. The shared epoch structure makes these tasks inherently compositional. We first show that this compositionality can be systematically described by a probabilistic generative model. Furthermore, We develop an unsupervised online learning approach that can learn this model on a single-trial basis, building its vocabulary incrementally as it is exposed to new tasks, and inferring the latent epoch structure as a time-varying computational context within a trial. We implement the how system as an RNN whose low-rank components are composed according to the context inferred by the what system. Contextual inference facilitates the creation, learning, and reuse of low-rank RNN components as new tasks are introduced sequentially, enabling continual learning without catastrophic forgetting. Using an example task set, we demonstrate the efficacy and competitive performance of this two-system learning framework, its potential for forward and backward transfer, as well as fast compositional generalization to unseen tasks.",Neuroscience,http://arxiv.org/abs/2510.20709v1,arXiv,0
"Place cells are neurons that act as biological position sensors, associated with and firing in response to regions of an environment to situate an organism in space. These associations are recorded in (combinatorial) neural codes, motivating the following mathematical question: Which neural codes are generated by a collection of convex open sets in Euclidean space? Giusti and Itskov showed that a necessary condition for convexity is the absence of ``local obstructions."" This necessary condition is, in fact, sufficient for certain families of codes. One such family consists of all codes with up to three maximal codewords. In this article, we investigate codes with four maximal codewords, showing that for many such codes, convexity is characterized by the absence of local obstructions, whereas for other such codes, convexity is characterized by the absence of local obstructions and a second type of obstruction, a ``wheel"". Key to our analysis is a case-by-case investigation based on the nerve complex of the set of maximal codewords of a neural code. Up to symmetry, there are 20 possible nerves; and our results fully characterize convexity in 15 of the 20 cases.",Neuroscience,http://arxiv.org/abs/2510.20323v1,arXiv,0
"In cognitive science and AI, a longstanding question is whether machines learn representations that align with those of the human mind. While current models show promise, it remains an open question whether this alignment is superficial or reflects a deeper correspondence in the underlying dimensions of representation. Here we introduce a methodology to probe the internal geometry of vision-language models (VLMs) by having them generate pairwise similarity judgments for a complex set of natural objects. Using multidimensional scaling, we recover low-dimensional psychological spaces and find that their axes show a strong correspondence with the principal axes of human perceptual space. Critically, when this AI-derived representational geometry is used as the input to a classic exemplar model of categorization, it predicts human classification behavior more accurately than a space constructed from human judgments themselves. This suggests that VLMs can capture an idealized or `denoised' form of human perceptual structure. Our work provides a scalable method to overcome a measurement bottleneck in cognitive science and demonstrates that foundation models can learn a representational geometry that is functionally relevant for modeling key aspects of human cognition, such as categorization.",Neuroscience,http://arxiv.org/abs/2510.20859v1,arXiv,0
"The majority of research in both training Artificial Neural Networks (ANNs) and modeling learning in biological brains focuses on synaptic plasticity, where learning equates to changing the strength of existing connections. However, in biological brains, structural plasticity - where new connections are created and others removed - is also vital, not only for effective learning but also for recovery from damage and optimal resource usage. Inspired by structural plasticity, pruning is often used in machine learning to remove weak connections from trained models to reduce the computational requirements of inference. However, the machine learning frameworks typically used for backpropagation-based training of both ANNs and Spiking Neural Networks (SNNs) are optimized for dense connectivity, meaning that pruning does not help reduce the training costs of ever-larger models. The GeNN simulator already supports efficient GPU-accelerated simulation of sparse SNNs for computational neuroscience and machine learning. Here, we present a new flexible framework for implementing GPU-accelerated structural plasticity rules and demonstrate this first using the e-prop supervised learning rule and DEEP R to train efficient, sparse SNN classifiers and then, in an unsupervised learning context, to learn topographic maps. Compared to baseline dense models, our sparse classifiers reduce training time by up to 10x while the DEEP R rewiring enables them to perform as well as the original models. We demonstrate topographic map formation in faster-than-realtime simulations, provide insights into the connectivity evolution, and measure simulation speed versus network size. The proposed framework will enable further research into achieving and maintaining sparsity in network structure and neural communication, as well as exploring the computational benefits of sparsity in a range of neuromorphic applications.",Neuroscience,http://arxiv.org/abs/2510.19764v1,arXiv,0
"Brain decoding is a key neuroscience field that reconstructs the visual stimuli from brain activity with fMRI, which helps illuminate how the brain represents the world. fMRI-to-image reconstruction has achieved impressive progress by leveraging diffusion models. However, brain signals infused with prior knowledge and associations exhibit a significant information asymmetry when compared to raw visual features, still posing challenges for decoding fMRI representations under the supervision of images. Consequently, the reconstructed images often lack fine-grained visual fidelity, such as missing attributes and distorted spatial relationships. To tackle this challenge, we propose BrainCognizer, a novel brain decoding model inspired by human visual cognition, which explores multi-level semantics and correlations without fine-tuning of generative models. Specifically, BrainCognizer introduces two modules: the Cognitive Integration Module which incorporates prior human knowledge to extract hierarchical region semantics; and the Cognitive Correlation Module which captures contextual semantic relationships across regions. Incorporating these two modules enhances intra-region semantic consistency and maintains inter-region contextual associations, thereby facilitating fine-grained brain decoding. Moreover, we quantitatively interpret our components from a neuroscience perspective and analyze the associations between different visual patterns and brain functions. Extensive quantitative and qualitative experiments demonstrate that BrainCognizer outperforms state-of-the-art approaches on multiple evaluation metrics.",Neuroscience,http://arxiv.org/abs/2510.20855v1,arXiv,0
"In humans and other animals, category learning enhances discrimination between stimuli close to the category boundary. This phenomenon, called categorical perception, was also empirically observed in artificial neural networks trained on classification tasks. In previous modeling works based on neuroscience data, we show that this expansion/compression is a necessary outcome of efficient learning. Here we extend our theoretical framework to artificial networks. We show that minimizing the Bayes cost (mean of the cross-entropy loss) implies maximizing the mutual information between the set of categories and the neural activities prior to the decision layer. Considering structured data with an underlying feature space of small dimension, we show that maximizing the mutual information implies (i) finding an appropriate projection space, and, (ii) building a neural representation with the appropriate metric. The latter is based on a Fisher information matrix measuring the sensitivity of the neural activity to changes in the projection space. Optimal learning makes this neural Fisher information follow a category-specific Fisher information, measuring the sensitivity of the category membership. Category learning thus induces an expansion of neural space near decision boundaries. We characterize the properties of the categorical Fisher information, showing that its eigenvectors give the most discriminant directions at each point of the projection space. We find that, unexpectedly, its maxima are in general not exactly at, but near, the class boundaries. Considering toy models and the MNIST dataset, we numerically illustrate how after learning the two Fisher information matrices match, and essentially align with the category boundaries. Finally, we relate our approach to the Information Bottleneck one, and we exhibit a bias-variance decomposition of the Bayes cost, of interest on its own.",Neuroscience,http://arxiv.org/abs/2510.19021v2,arXiv,0
"Biological learning unfolds continuously in time, yet most algorithmic models rely on discrete updates and separate inference and learning phases. We study a continuous-time neural model that unifies several biologically plausible learning algorithms and removes the need for phase separation. Rules including stochastic gradient descent (SGD), feedback alignment (FA), direct feedback alignment (DFA), and Kolen-Pollack (KP) emerge naturally as limiting cases of the dynamics. Simulations show that these continuous-time networks stably learn at biological timescales, even under temporal mismatches and integration noise. Through analysis and simulation, we show that learning depends on temporal overlap: a synapse updates correctly only when its input and the corresponding error signal coincide in time. When inputs are held constant, learning strength declines linearly as the delay between input and error approaches the stimulus duration, explaining observed robustness and failure across network depths. Critically, robust learning requires the synaptic plasticity timescale to exceed the stimulus duration by one to two orders of magnitude. For typical cortical stimuli (tens of milliseconds), this places the functional plasticity window in the few-second range, a testable prediction that identifies seconds-scale eligibility traces as necessary for error-driven learning in biological circuits.",Neuroscience,http://arxiv.org/abs/2510.18808v1,arXiv,0
"To study how the human brain works, we need to explore the organization of the cerebral cortex and its detailed cellular architecture. We introduce CytoNet, a foundation model that encodes high-resolution microscopic image patches of the cerebral cortex into highly expressive feature representations, enabling comprehensive brain analyses. CytoNet employs self-supervised learning using spatial proximity as a powerful training signal, without requiring manual labelling. The resulting features are anatomically sound and biologically relevant. They encode general aspects of cortical architecture and unique brain-specific traits. We demonstrate top-tier performance in tasks such as cortical area classification, cortical layer segmentation, cell morphology estimation, and unsupervised brain region mapping. As a foundation model, CytoNet offers a consistent framework for studying cortical microarchitecture, supporting analyses of its relationship with other structural and functional brain features, and paving the way for diverse neuroscientific investigations.",Neuroscience,http://arxiv.org/abs/2511.01870v1,arXiv,0
"Self-supervised learning (SSL) holds a great deal of promise for applications in neuroscience, due to the lack of large-scale, consistently labeled neural datasets. However, most neural datasets contain heterogeneous populations that mix stable, predictable cells with highly stochastic, stimulus-contingent ones, which has made it hard to identify consistent activity patterns during SSL. As a result, self-supervised pretraining has yet to show clear signs of benefits from scale on neural data. Here, we present a novel approach to self-supervised pretraining, POYO-SSL that exploits the heterogeneity of neural data to improve pre-training and achieve benefits of scale. Specifically, in POYO-SSL we pretrain only on predictable (statistically regular) neurons-identified on the pretraining split via simple higher-order statistics (skewness and kurtosis)-then we fine-tune on the unpredictable population for downstream tasks. On the Allen Brain Observatory dataset, this strategy yields approximately 12-13% relative gains over from-scratch training and exhibits smooth, monotonic scaling with model size. In contrast, existing state-of-the-art baselines plateau or destabilize as model size increases. By making predictability an explicit metric for crafting the data diet, POYO-SSL turns heterogeneity from a liability into an asset, providing a robust, biologically grounded recipe for scalable neural decoding and a path toward foundation models of neural dynamics.",Neuroscience,http://arxiv.org/abs/2510.18516v1,arXiv,0
"Clarifying the neural basis of speech intelligibility is critical for computational neuroscience and digital speech processing. Recent neuroimaging studies have shown that intelligibility modulates cortical activity beyond simple acoustics, primarily in the superior temporal and inferior frontal gyri. However, previous studies have been largely confined to clean speech, leaving it unclear whether the brain employs condition-invariant neural codes across diverse listening environments. To address this gap, we propose a novel architecture built upon a deep state space model for decoding intelligibility from fMRI signals, specifically tailored to their high-dimensional temporal structure. We present the first attempt to decode intelligibility across acoustically distinct conditions, showing our method significantly outperforms classical approaches. Furthermore, region-wise analysis highlights contributions from auditory, frontal, and parietal regions, and cross-condition transfer indicates the presence of condition-invariant neural codes, thereby advancing understanding of abstract linguistic representations in the brain.",Neuroscience,http://arxiv.org/abs/2511.01868v1,arXiv,0
"In the classic view of cortical rhythms, the interaction between excitatory pyramidal neurons (E) and inhibitory parvalbumin neurons (I) has been shown to be sufficient to generate gamma and beta band rhythms. However, it is now clear that there are multiple inhibitory interneuron subtypes and that they play important roles in the generation of these rhythms. In this paper we develop a spiking network that consists of populations of E, I and an additional interneuron type, the somatostatin (S) internerons that receive excitation from the E cells and inhibit both the E cells and the I cells. These S cells are modulated by a third inhibitory subtype, VIP neurons that receive inputs from other cortical areas. We reduce the spiking network to a system of nine differential equations that characterize the mean voltage, firing rate, and synaptic conductance for each population and using this we find many instances of multiple rhythms within the network. Using tools from nonlinear dynamics, we explore the roles of each of the two classes of inhibition as well as the role of the VIP modulation on the properties of these rhythms.",Neuroscience,http://arxiv.org/abs/2510.20848v1,arXiv,0
"The extent to which different neural or artificial neural networks (models) rely on equivalent representations to support similar tasks remains a central question in neuroscience and machine learning. Prior work has typically compared systems using a single representational similarity metric, yet each captures only one facet of representational structure. To address this, we leverage a suite of representational similarity metrics-each capturing a distinct facet of representational correspondence, such as geometry, unit-level tuning, or linear decodability-and assess brain region or model separability using multiple complementary measures. Metrics that preserve geometric or tuning structure (e.g., RSA, Soft Matching) yield stronger region-based discrimination, whereas more flexible mappings such as Linear Predictivity show weaker separation. These findings suggest that geometry and tuning encode brain-region- or model-family-specific signatures, while linearly decodable information tends to be more globally shared across regions or models. To integrate these complementary representational facets, we adapt Similarity Network Fusion (SNF), a framework originally developed for multi-omics data integration. SNF produces substantially sharper regional and model family-level separation than any single metric and yields robust composite similarity profiles. Moreover, clustering cortical regions using SNF-derived similarity scores reveals a clearer hierarchical organization that aligns closely with established anatomical and functional hierarchies of the visual cortex-surpassing the correspondence achieved by individual metrics.",Neuroscience,http://arxiv.org/abs/2510.20847v1,arXiv,0
"The presence of interictal epileptiform discharges (IEDs) in electroencephalogram (EEG) recordings is a critical biomarker of epilepsy. Even trained neurologists find detecting IEDs difficult, leading many practitioners to turn to machine learning for help. While existing machine learning algorithms can achieve strong accuracy on this task, most models are uninterpretable and cannot justify their conclusions. Absent the ability to understand model reasoning, doctors cannot leverage their expertise to identify incorrect model predictions and intervene accordingly. To improve the human-model interaction, we introduce ProtoEEG-kNN, an inherently interpretable model that follows a simple case-based reasoning process. ProtoEEG-kNN reasons by comparing an EEG to similar EEGs from the training set and visually demonstrates its reasoning both in terms of IED morphology (shape) and spatial distribution (location). We show that ProtoEEG-kNN can achieve state-of-the-art accuracy in IED detection while providing explanations that experts prefer over existing approaches.",Neuroscience,http://arxiv.org/abs/2510.20846v1,arXiv,0
"Neural activity forecasting is central to understanding neural systems and enabling closed-loop control. While deep learning has recently advanced the state-of-the-art in the time series forecasting literature, its application to neural activity forecasting remains limited. To bridge this gap, we systematically evaluated eight probabilistic deep learning models, including two foundation models, that have demonstrated strong performance on general forecasting benchmarks. We compared them against four classical statistical models and two baseline methods on spontaneous neural activity recorded from mouse cortex via widefield imaging. Across prediction horizons, several deep learning models consistently outperformed classical approaches, with the best model producing informative forecasts up to 1.5 seconds into the future. Our findings point toward future control applications and open new avenues for probing the intrinsic temporal structure of neural activity.",Neuroscience,http://arxiv.org/abs/2510.18037v2,arXiv,0
"In Hopfield-type associative memory models, memories are stored in the connectivity matrix and can be retrieved subsequently thanks to the collective dynamics of the network. In these models, the retrieval of a particular memory can be hampered by overlaps between the network state and other memories, termed spurious overlaps since these overlaps collectively introduce noise in the retrieval process. In classic models, spurious overlaps increase the variance of synaptic inputs but do not affect the mean. We show here that in models equipped with a learning rule inferred from neurobiological data, spurious overlaps collectively reduce the mean synaptic inputs to neurons, and that this mean reduction causes in turn an increase in storage capacity through a sparsening of network activity. Our paper demonstrates a link between a specific feature of experimentally inferred plasticity rules and network storage capacity.",Neuroscience,http://arxiv.org/abs/2510.17593v1,arXiv,0
"Recurrent neural networks with balanced excitation and inhibition exhibit irregular asynchronous dynamics, which is fundamental for cortical computations. Classical balance mechanisms require strong external inputs to sustain finite firing rates, raising concerns about their biological plausibility. Here, we investigate an alternative mechanism based on short-term synaptic depression (STD) acting on excitatory-excitatory synapses, which dynamically balances the network activity without the need of external inputs. By employing numerical simulations and theoretical investigations we characterize the dynamics of a massively coupled network made up of $N$ rate-neuron models. Depending on the synaptic strength $J_0$, the network exhibits two distinct regimes: at sufficiently small $J_0$, it converges to a homogeneous fixed point, while for sufficiently large $J_0$, it exhibits Rate Chaos. For finite networks, we observe several different routes to chaos depending on the network realization. The width of the transition region separating the homogeneous stable solution from Rate Chaos appears to shrink for increasing $N$ and eventually to vanish in the thermodynamic limit. The characterization of the Rate Chaos regime performed by employing Dynamical Mean Field approaches allow us on one side to confirm that this novel balancing mechanism is able to sustain finite irregular activity even in the thermodynamic limit, and on the other side to reveal that the balancing occurs via dynamic cancellation of the input correlations generated by the massive coupling. Our findings show that STD provides an intrinsic self-regulating mechanism for balanced networks, sustaining irregular yet stable activity without the need of biologically unrealistic inputs. This work extends the balanced network paradigm, offering insights into how cortical circuits could maintain robust dynamics via synaptic adaptation.",Neuroscience,http://arxiv.org/abs/2510.17492v1,arXiv,0
"This study introduces a novel method for predicting cognitive age using psychophysiological tests. To determine cognitive age, subjects were asked to complete a series of psychological tests measuring various cognitive functions, including reaction time and cognitive conflict, short-term memory, verbal functions, and color and spatial perception. Based on the tests completed, the average completion time, proportion of correct answers, average absolute delta of the color campimetry test, number of guessed words in the MÃ¼nsterberg matrix, and other parameters were calculated for each subject. The obtained characteristics of the subjects were preprocessed and used to train a machine learning algorithm implementing a regression task for predicting a person's cognitive age. These findings contribute to the field of remote screening using mobile devices for human health for diagnosing and monitoring cognitive aging.",Neuroscience,http://arxiv.org/abs/2511.00013v1,arXiv,0
"When conflicting images are presented to either eye, binocular fusion is disrupted. Rather than experiencing a blend of both percepts, often only one eye's image is experienced, whilst the other is suppressed from awareness. Importantly, suppression is transient - the two rival images compete for dominance, with stochastic switches between mutually exclusive percepts occurring every few seconds with law-like regularity. From the perspective of dynamical systems theory, visual rivalry offers an experimentally tractable window into the dynamical mechanisms governing perceptual awareness. In a recently developed visual rivalry paradigm - tracking continuous flash suppression (tCFS) - it was shown that the transition between awareness and suppression is hysteretic, with a higher contrast threshold required for a stimulus to breakthrough suppression into awareness than to be suppressed from awareness. Here, we present an analytically-tractable model of visual rivalry that quantitatively explains the hysteretic transition between periods of awareness and suppression in tCFS. Grounded in the theory of neural dynamics, we derive closed-form expressions for the duration of perceptual dominance and suppression, and for the degree of hysteresis (i.e. the depth of perceptual suppression), as a function of model parameters. Finally, our model yields a series of novel behavioural predictions, the first of which - distributions of dominance and suppression durations during tCFS should be approximately equal - we empirically validate in human psychophysical data.",Neuroscience,http://arxiv.org/abs/2510.17154v2,arXiv,0
"The Free Energy Principle (FEP) states that self-organizing systems must minimize variational free energy to persist, but the path from principle to implementable algorithm has remained unclear. We present a constructive proof that the FEP can be realized through exact local credit assignment. The system decomposes gradient computation hierarchically: spatial credit via feedback alignment, temporal credit via eligibility traces, and structural credit via a Trophic Field Map (TFM) that estimates expected gradient magnitude for each connection block. We prove these mechanisms are exact at their respective levels and validate the central claim empirically: the TFM achieves 0.9693 Pearson correlation with oracle gradients. This exactness produces emergent capabilities including 98.6% retention after task interference, autonomous recovery from 75% structural damage, self-organized criticality (spectral radius p ~= 1.0$), and sample-efficient reinforcement learning on continuous control tasks without replay buffers. The architecture unifies Prigogine's dissipative structures, Friston's free energy minimization, and Hopfield's attractor dynamics, demonstrating that exact hierarchical inference over network topology can be implemented with local, biologically plausible rules.",Neuroscience,http://arxiv.org/abs/2510.17916v1,arXiv,0
"In the human brain, the allowed patterns of activity are constrained by the correlations between brain regions. Yet it remains unclear which correlations -- and how many -- are needed to predict large-scale neural activity. Here, we present an information-theoretic framework to identify the most important correlations, which provide the most accurate predictions of neural states. Applying our framework to cortical activity in humans, we discover that the vast majority of variance in activity is explained by a small number of correlations. This means that the brain is highly compressible: only a sparse network of correlations is needed to predict large-scale activity. We find that this compressibility is strikingly consistent across different individuals and cognitive tasks, and that, counterintuitively, the most important correlations are not necessarily the strongest. Together, these results suggest that nearly all correlations are not needed to predict neural activity, and we provide the tools to uncover the key correlations that are.",Neuroscience,http://arxiv.org/abs/2510.16327v1,arXiv,0
"High-dimensional neural activity often reside in a low-dimensional subspace, referred to as neural manifolds. Grid cells in the medial entorhinal cortex provide a periodic spatial code that are organized near a toroidal manifold, independent of the spatial environment. Due to the periodic nature of its code, it is unclear how the brain utilizes the toroidal manifold to understand its state in a spatial environment. We introduce a novel framework that decodes spatial information from grid cell activity using topology. Our approach uses topological data analysis to extract toroidal coordinates from grid cell population activity and employs path-lifting to reconstruct trajectories in physical space. The reconstructed paths differ from the original by an affine transformation. We validated the method on both continuous attractor network simulations and experimental recordings of grid cells, demonstrating that local trajectories can be reliably reconstructed from a single grid cell module without external position information or training data. These results suggest that co-modular grid cells contain sufficient information for path integration and suggest a potential computational mechanism for spatial navigation.",Neuroscience,http://arxiv.org/abs/2510.16216v1,arXiv,0
"A central challenge in cognitive neuroscience is to explain how semantic and episodic memory, two major forms of declarative memory, typically associated with cortical and hippocampal processing, interact to support learning, recall, and imagination. Despite significant advances, we still lack a unified computational framework that jointly accounts for core empirical phenomena across both semantic and episodic processing domains. Here, we introduce the Generative Episodic-Semantic Integration System (GENESIS), a computational model that formalizes memory as the interaction between two limited-capacity generative systems: a Cortical-VAE, supporting semantic learning and generalization, and a Hippocampal-VAE, supporting episodic encoding and retrieval within a retrieval-augmented generation (RAG) architecture. GENESIS reproduces hallmark behavioral findings, including generalization in semantic memory, recognition, serial recall effects and gist-based distortions in episodic memory, and constructive episodic simulation, while capturing their dynamic interactions. The model elucidates how capacity constraints shape the fidelity and memorability of experiences, how semantic processing introduces systematic distortions in episodic recall, and how episodic replay can recombine previous experiences. Together, these results provide a principled account of memory as an active, constructive, and resource-bounded process. GENESIS thus advances a unified theoretical framework that bridges semantic and episodic memory, offering new insights into the generative foundations of human cognition.",Neuroscience,http://arxiv.org/abs/2510.15828v1,arXiv,0
"The State of Brain Emulation Report 2025 provides a comprehensive reassessment of the field's progress since Sandberg and Bostrom's 2008 Whole Brain Emulation roadmap. The report is organized around three core capabilities required for brain emulation: recording brain function (Neural Dynamics), mapping brain structure (Connectomics), and emulation and embodiment (Computational Neuroscience). It also identifies ongoing challenges and outlines strategic priorities to help the field move forward.",Neuroscience,http://arxiv.org/abs/2510.15745v3,arXiv,0
"In recent years, Electroencephalographic analysis has gained prominence in stress research when combined with AI and Machine Learning models for validation. In this study, a lightweight dynamic brain connectivity framework based on Time Varying Directed Transfer Function is proposed, where TV DTF features were validated through ML based stress classification. TV DTF estimates the directional information flow between brain regions across distinct EEG frequency bands, thereby capturing temporal and causal influences that are often overlooked by static functional connectivity measures. EEG recordings from the 32 channel SAM 40 dataset were employed, focusing on mental arithmetic task trials. The dynamic EEG-based TV-DTF features were validated through ML classifiers such as Support Vector Machine, Random Forest, Gradient Boosting, Adaptive Boosting, and Extreme Gradient Boosting. Experimental results show that alpha-TV-DTF provided the strongest discriminative power, with SVM achieving 89.73% accuracy in 3-class classification and with XGBoost achieving 93.69% accuracy in 2 class classification. Relative to absolute power and phase locking based functional connectivity features, alpha TV DTF and beta TV DTF achieved higher performance across the ML models, highlighting the advantages of dynamic over static measures. Feature importance analysis further highlighted dominant long-range frontal parietal and frontal occipital informational influences, emphasizing the regulatory role of frontal regions under stress. These findings validate the lightweight TV-DTF as a robust framework, revealing spatiotemporal brain dynamics and directional influences across different stress levels.",Neuroscience,http://arxiv.org/abs/2511.05505v1,arXiv,0
"This paper introduces SpikeFit, a novel training method for Spiking Neural Networks (SNNs) that enables efficient inference on neuromorphic hardware, considering all its stringent requirements: the number of neurons and synapses that can fit on a single device, and lower bit-width representations (e.g., 4-bit, 8-bit). Unlike conventional compressing approaches that address only a subset of these requirements (limited numerical precision and limited number of neurons in the network), SpikeFit treats the allowed weights' discrete values themselves as learnable parameters co-optimized with the model, allowing for optimal Clusterization-Aware Training (CAT) of the model's weights at low precision (2-, 4-, or 8-bit) which results in higher network compression efficiency, as well as limiting the number of unique synaptic connections to a value required by neuromorphic processor. This joint optimization allows SpikeFit to find a discrete weight set aligned with hardware constraints, enabling the most complete deployment across a broader range of neuromorphic processors than existing methods of SNN compression support. Moreover, SpikeFit introduces a new hardware-friendly Fisher Spike Contribution (FSC) pruning method showing the state-of-the-art performance. We demonstrate that for spiking neural networks constrained to only four unique synaptic weight values (M = 4), our SpikeFit method not only outperforms state-of-the-art SNNs compression methods and conventional baselines combining extreme quantization schemes and clustering algorithms, but also meets a wider range of neuromorphic hardware requirements and provides the lowest energy use in experiments.",Neuroscience,http://arxiv.org/abs/2510.15542v2,arXiv,0
"Precisely defining consciousness and identifying the mechanisms that effect it is a long-standing question, particularly relevant with advances in artificial intelligence. The scientific community is divided between physicalism and natural dualism. Physicalism posits consciousness is a physical process that can be modeled computationally; natural dualism rejects this hypothesis. Finding a computational model has proven elusive, particularly because of conflation of consciousness with other cognitive capabilities exhibited by humans, such as intelligence and physiological sensations. Here we show such a computational model that precisely models consciousness, natural or artificial, identifying the structural and functional mechanisms that effect it, confirming the physicalism hypothesis. We found such a model is obtainable when including the underlying (biological or digital) substrate and accounting for reactive behavior in substrate sub-systems (e.g., autonomous physiological responses). Results show that, unlike all other computational processes, consciousness is not independent of its substrate and possessing it is an evolutionary advantage for intelligent entities. Our result shows there is no impediment to the realization of fully artificial consciousness but, surprisingly, that it is also possible to realize artificial intelligence of arbitrary level without consciousness whatsoever, and that there is no advantage in imbuing artificial systems with consciousness.",Neuroscience,http://arxiv.org/abs/2510.20839v1,arXiv,0
"Background: Clinical trials are designed to prove the efficacy of an intervention by means of model-based approaches involving parametric hypothesis testing. Issues arise when no effect is observed in the study population. Indeed, an effect may be present in a subgroup and the statistical test cannot detect it. To investigate this possibility, we proposed to change the paradigm to a data-driven approach. We selected exploratory methods to provide another perspective on the data and to identify particular homogeneous subgroups of subjects within which an effect might be detected. In the setting of prevention trials, the endpoint is a trajectory of repeated measures. In the settings of prevention trials, the endpoint is a trajectory of repeated measures, which requires the use of methods that can take data autocorrelation into account. The primary aim of this work was to explore the applicability of different methods for clustering and classifying trajectories. Methods: The Multidomain Alzheimer Preventive Trial (MAPT) was a three-year randomized controlled trial with four parallel arms (NCT00672685). The primary outcome was a composite Z-score combining four cognitive tests. The data were analyzed by quadratic mixed effects model. This study was inconclusive. Exploratory analysis is therefore relevant to investigate the use of data-driven methods for trajectory classification. The methods used were unsupervised: k-means for longitudinal data, Hierarchical Cluster Analysis (HCA), graphic semiology, and supervised analysis with dichotomous classification according to responder status. Results: Using k-means for longitudinal data, three groups were obtained and one of these groups showed cognitive decline over the three years of follow-up. This method could be applied directly to the primary outcome, the composite Z-score with repeated observations over time. With the two others unsupervised methods, we were unable to process longitudinal data directly. It was therefore necessary to choose an indicator of change in trajectories and to consider the rate of change between two measurements. For the HCA method, Ward's aggregation was performed. The Euclidean distance and rates of change were applied for the graphic semiology method. Lastly, as there were no objective criteria to define responder status, we defined our responders based on clinical criteria. Discussion: In the princeps study, the prevention trial was found to be inconclusive, likely due to the heterogeneity of the population, which may have masked a treatment effect later identified in a refined subgroup of high Beta Amyloid subjects. So, we have adopted an alternative unsupervised approach to subject stratification based on their trajectories. We could then identify patterns of similar trajectories of cognitive decline and also highlight the potential problem of a large heterogeneity of the profiles, maybe due to the final endpoint considered.",Neuroscience,http://arxiv.org/abs/2510.24751v1,arXiv,0
"In the literature of cognitive neuroscience, researchers tend to assume a linear relationship between brain activation level and task performance; however, controversial findings have been reported in participants at different ages and different proficiency levels. Therefore, there may be a non-linear relationship between task performance and brain activation if a full range of task performance is considered. In the current study, using the Human Connectome Project (HCP) dataset we examined the relationship between brain activation and working memory performance in two conditions (i.e. faces and places). We found a gradual change from a U-shaped relationship to an inverted U-shaped relationship along the sensorimotor-association (S-A) axis in the face condition. In other words, in low-order sensorimotor areas, it is U-shaped and in the high-order prefrontal and association areas, it is inverted U-shaped, which suggests different properties in the encoding/representation region and in the cognitive calculation regions. However, in the place condition, such a shift is missing, presumably because most of the regions that are sensitive to task performance in the place condition are in the lower end of the S-A axis. Taken together, our study revealed a novel difference of functional property in response to task performance in the sensorimotor areas versus the association areas.",Neuroscience,http://arxiv.org/abs/2510.14601v1,arXiv,0
"Auditory sensory overload affects 50-70% of individuals with Autism Spectrum Disorder (ASD), yet existing approaches, such as mechanistic models (Hodgkin Huxley type, Wilson Cowan, excitation inhibition balance), clinical tools (EEG/MEG, Sensory Profile scales), and ML methods (Neural ODEs, predictive coding), either assume fixed parameters or lack interpretability, missing autism heterogeneity. We present a Scientific Machine Learning approach using Universal Differential Equations (UDEs) to model sensory adaptation dynamics in autism. Our framework combines ordinary differential equations grounded in biophysics with neural networks to capture both mechanistic understanding and individual variability. We demonstrate that UDEs achieve a 90.8% improvement over pure Neural ODEs while using 73.5% fewer parameters. The model successfully recovers physiological parameters within the 2% error and provides a quantitative risk assessment for sensory overload, predicting 17.2% risk for pulse stimuli with specific temporal patterns. This framework establishes foundations for personalized, evidence-based interventions in autism, with direct applications to wearable technology and clinical practice.",Neuroscience,http://arxiv.org/abs/2510.26804v1,arXiv,0
"Brains learn to represent information from a large set of stimuli, typically by weak supervision. Unsupervised learning is therefore a natural approach for exploring the design of biological neural networks and their computations. Accordingly, redundancy reduction has been suggested as a prominent design principle of neural encoding, but its ``mechanistic'' biological implementation is unclear. Analogously, unsupervised training of artificial neural networks yields internal representations that allow for accurate stimulus classification or decoding, but typically rely on biologically-implausible implementations. We suggest that interactions between parallel subnetworks in the brain may underlie such learning: we present a model of representation learning by ensembles of neural networks, where each network learns to encode stimuli into an abstract representation space by cross-supervising interactions with other networks, for inputs they receive simultaneously or in close temporal proximity. Aiming for biological plausibility, each network has a small ``receptive field'', thus receiving a fixed part of the external input, and the networks do not share weights. We find that for different types of network architectures, and for both visual or neuronal stimuli, these cross-supervising networks learn semantic representations that are easily decodable and that decoding accuracy is comparable to supervised networks -- both at the level of single networks and the ensemble. We further show that performance is optimal for small receptive fields, and that sparse connectivity between networks is nearly as accurate as all-to-all interactions, with far fewer computations. We thus suggest a sparsely interacting collective of cross-supervising networks as an algorithmic framework for representational learning and collective computation in the brain.",Neuroscience,http://arxiv.org/abs/2510.14486v1,arXiv,0
"The brain predicts the external world through an internal model refined by prediction errors. A complete prediction specifies what will happen, when it will happen, and with what probability, a construct we call the ""prediction object."" Existing models usually capture only what and when, omit probabilities, and rely on algorithms that are not biologically plausible. We show that a single population of spiking neurons can learn the full prediction object through a biologically grounded three factor Hebbian rule. In a heterogeneous Izhikevich reservoir, online timing learning and offline identity consolidation allow the network to fire at the correct times with amplitudes proportional to probability and to adapt instantly when the environment changes. Unlike global least squares methods such as FORCE, which require resets to relearn, our model recalibrates continuously through local error gated modulation. This single circuit provides a biologically grounded, flexible mechanism for predictive cognition.",Neuroscience,http://arxiv.org/abs/2510.14382v2,arXiv,0
"4E views of cognition seek to replace many of the long-held assumptions of tra- ditional cognitive science. One of the most radical shifts is the rejection of the sandwich model of cognition [8], which holds that mental processes are located be- tween action and perception. Subversion of such a long-held assumption requires an accessible theoretical alternative with firm experimental support. One unifying thread among the emerging 4E camps is their shared insistence that sensorimotor contingencies (SMCs) are such an alternative.",Neuroscience,http://arxiv.org/abs/2510.14227v1,arXiv,0
"In neuroscience, methods from information geometry (IG) have been successfully applied in the modelling of binary vectors from spike train data, using the orthogonal decomposition of the Kullback-Leibler divergence and mutual information to isolate different orders of interaction between neurons. While spike train data is well-approximated with a binary model, here we apply these IG methods to data from electroencephalography (EEG), a continuous signal requiring appropriate discretization strategies. We developed and compared three different binarization methods and used them to identify third-order interactions in an experiment involving imagined motor movements. The statistical significance of these interactions was assessed using phase-randomized surrogate data that eliminated higher-order dependencies while preserving the spectral characteristics of the original signals. We validated our approach by implementing known second- and third-order dependencies in a forward model and quantified information attenuation at different steps of the analysis. This revealed that the greatest loss in information occurred when going from the idealized binary case to enforcing these dependencies using oscillatory signals. When applied to the real EEG dataset, our analysis detected statistically significant third-order interactions during the task condition despite the relatively sparse data (45 trials per condition). This work demonstrates that IG methods can successfully extract genuine higher-order dependencies from continuous neural recordings when paired with appropriate binarization schemes.",Neuroscience,http://arxiv.org/abs/2510.14188v1,arXiv,0
"A key question for adapting modern deep learning architectures to functional MRI (fMRI) is how to represent the data for model input. To bridge the modality gap between fMRI and natural images, we transform the 4D volumetric fMRI data into videos of 2D fMRI activity flat maps. We train Vision Transformers on 2.3K hours of fMRI flat map videos from the Human Connectome Project using the spatiotemporal masked autoencoder (MAE) framework. We observe that masked fMRI modeling performance improves with dataset size according to a strict power scaling law. Downstream classification benchmarks show that our model learns rich representations supporting both fine-grained state decoding across subjects, as well as subject-specific trait decoding across changes in brain state. This work is part of an ongoing open science project to build foundation models for fMRI data. Our code and datasets are available at https://github.com/MedARC-AI/fmri-fm.",Neuroscience,http://arxiv.org/abs/2510.13768v1,arXiv,0
"Deep reinforcement learning (DRL) algorithms have the potential to provide new insights into psychiatric disorders. Here we create a DRL model of schizophrenia: a complex psychotic disorder characterized by anhedonia, avoidance, temporal discounting, catatonia, and hallucinations. Schizophrenia's causes are not well understood: dopaminergic theories emphasize dopamine system dysfunction, while neurodevelopmental theories emphasize abnormal connectivity, including excitation/inhibition (E/I) imbalance in the brain. In this study, we suppressed positive (excitatory) connections within an artificial neural network to simulate E/I imbalance. Interestingly, this is insufficient to create behavioral changes; the network simply compensates for the imbalance. But in doing so it becomes more sensitive to noise. Injecting noise into the network then creates a range of schizophrenic-like behaviours. These findings point to an interesting potential pathology of schizophrenia: E/I imbalance leads to a compensatory response by the network to increase the excitability of neurons, which increases susceptibility to noise. This suggests that the combination of E/I imbalance and neural noise may be key in the emergence of schizophrenic symptoms. We further notice altered response to reward prediction error in our model, and thus propose that E/I imbalance plus noise can account for both schizophrenia symptoms and dopamine system dysfunction: potentially unifying dopaminergic and neurodevelopmental theories of schizophrenia pathology.",Neuroscience,http://arxiv.org/abs/2510.24741v1,arXiv,0
"In recent years, the alignment between artificial neural network (ANN) embeddings and blood oxygenation level dependent (BOLD) responses in functional magnetic resonance imaging (fMRI) via neural encoding models has significantly advanced research on neural representation mechanisms and interpretability in the brain. However, these approaches remain limited in characterizing the brain's inherently nonlinear response properties. To address this, we propose the Jacobian-based Nonlinearity Evaluation (JNE), an interpretability metric for nonlinear neural encoding models. JNE quantifies nonlinearity by statistically measuring the dispersion of local linear mappings (Jacobians) from model representations to predicted BOLD responses, thereby approximating the nonlinearity of BOLD signals. Centered on proposing JNE as a novel interpretability metric, we validated its effectiveness through controlled simulation experiments on various activation functions and network architectures, and further verified it on real fMRI data, demonstrating a hierarchical progression of nonlinear characteristics from primary to higher-order visual cortices, consistent with established cortical organization. We further extended JNE with Sample-Specificity (JNE-SS), revealing stimulus-selective nonlinear response patterns in functionally specialized brain regions. As the first interpretability metric for quantifying nonlinear responses, JNE provides new insights into brain information processing. Code available at https://github.com/Gaitxh/JNE.",Neuroscience,http://arxiv.org/abs/2510.13688v1,arXiv,0
"Artificial Recurrent Neural Networks (RNNs) are widely used in neuroscience to model the collective activity of neurons during behavioral tasks. The high dimensionality of their parameter and activity spaces, however, often make it challenging to infer and interpret the fundamental features of their dynamics.   In this study, we employ recent nonlinear dynamical system techniques to uncover the core dynamics of several RNNs used in contemporary neuroscience. Specifically, using a data-driven approach, we identify Spectral Submanifolds (SSMs), i.e., low-dimensional attracting invariant manifolds tangent to the eigenspaces of fixed points. The internal dynamics of SSMs serve as nonlinear models that reduce the dimensionality of the full RNNs by orders of magnitude.   Through low-dimensional, SSM-reduced models, we give mathematically precise definitions of line and ring attractors, which are intuitive concepts commonly used to explain decision-making and working memory. The new level of understanding of RNNs obtained from SSM reduction enables the interpretation of mathematically well-defined and robust structures in neuronal dynamics, leading to novel predictions about the neural computations underlying behavior.",Neuroscience,http://arxiv.org/abs/2510.13519v1,arXiv,0
"The theta rhythm is important for many cognitive functions including spatial processing, memory encoding, and memory recall. The information processing underlying these functions is thought to rely on consistent, phase-specific spiking throughout a theta oscillation that may fluctuate significantly in baseline (center of oscillations), frequency, or amplitude. Experimental evidence shows that spikes can occur at specific phases even when the baseline membrane potential varies significantly, such that the integrity of phase-locking persists across a large variability in spike threshold. The mechanism of this precise spike timing during the theta rhythm is not yet known and previous mathematical models have not reflected the large variability in threshold potential seen experimentally. Here we introduce a straightforward mathematical neural model capable of demonstrating a phase-locked spiking in the face of significant baseline membrane potential fluctuation during theta rhythm. This novel approach incorporates a degenerate grazing bifurcation of an asymptotically stable oscillation. This model suggests a potential mechanism for how biological neurons can consistently produce spikes near the peak of a variable membrane potential oscillation.",Neuroscience,http://arxiv.org/abs/2510.13156v1,arXiv,0
"Spatial world models, representations that support flexible reasoning about spatial relations, are central to developing computational models that could operate in the physical world, but their precise mechanistic underpinnings are nuanced by the borrowing of underspecified or misguided accounts of human cognition. This paper revisits the simulation versus rendering dichotomy and draws on evidence from aphantasia to argue that fine-grained perceptual content is critical for model-based spatial reasoning. Drawing on recent research into the neural basis of visual awareness, we propose that spatial simulation and perceptual experience depend on shared representational geometries captured by higher-order indices of perceptual relations. We argue that recent developments in embodied AI support this claim, where rich perceptual details improve performance on physics-based world engagements. To this end, we call for the development of architectures capable of maintaining structured perceptual representations as a step toward spatial world modelling in AI.",Neuroscience,http://arxiv.org/abs/2510.20835v1,arXiv,0
"Background: Non-linear alterations in brain network connectivity may represent early neural signatures of Alzheimer's disease (AD) pathology in cognitively normal older adults. Understanding these changes and their cognitive relevance could provide sensitive biomarkers for early detection. Most prior studies recruited participants from memory clinics, often with subjective memory concerns, limiting generalizability.   Methods: We examined 14 large-scale functional brain networks in 968 cognitively normal older adults recruited from the community using resting-state functional MRI, cerebrospinal fluid (CSF) biomarkers (amyloid-$Î²$ 1-42 [A$Î²$], total tau, phosphorylated tau 181), and neuropsychological assessments. Functional networks were identified using group independent component analysis.   Results: Inverted U-shaped associations between CSF A$Î²$ and functional connectivity were observed in the precuneus network and ventral default mode network (DMN), but not in the dorsal DMN, indicating network-specific vulnerability to early amyloid pathology. Higher connectivity in A$Î²$-related networks, including dorsal and ventral DMN, precuneus, and posterior salience networks, was associated with better visual memory, visuospatial, and executive performance. No significant relationships were observed between CSF tau and functional connectivity.   Conclusions: Using a large, community-based cohort, we demonstrate that non-linear alterations in functional connectivity occur in specific networks even during the asymptomatic phase of AD. Moreover, A$Î²$-related network connectivity is cognitively relevant, highlighting functional brain networks as promising imaging markers for early detection and prognosis of AD.",Neuroscience,http://arxiv.org/abs/2510.12751v1,arXiv,0
"Although quantum systems are generally described by quantum state vectors, we show that in certain cases their measurement processes can be reformulated as probabilistic equations expressed in terms of probabilistic state vectors. These probabilistic representations can, in turn, be approximated by the neural network dynamics of the Tensor Brain (TB) model.   The Tensor Brain is a recently proposed framework for modeling perception and memory in the brain, providing a biologically inspired mechanism for efficiently integrating generated symbolic representations into reasoning processes.",Neuroscience,http://arxiv.org/abs/2510.13894v2,arXiv,0
"Sensory representation is typically understood through a hierarchical-causal framework where progressively abstract features are extracted sequentially. However, this causal view fails to explain misrepresentation, a phenomenon better handled by an informational view based on decodable content. This creates a tension: how does a system that abstracts away details still preserve the fine-grained information needed for downstream functions? We propose readout representation to resolve this, defining representation by the information recoverable from features rather than their causal origin. Empirically, we show that inputs can be accurately reconstructed even from heavily perturbed mid-level features, demonstrating that a single input corresponds to a broad, redundant region of feature space, challenging the causal mapping perspective. To quantify this property, we introduce representation size, a metric linked to model robustness and representational redundancy. Our framework offers a new lens for analyzing how both biological and artificial neural systems learn complex features while maintaining robust, information-rich representations of the world.",Neuroscience,http://arxiv.org/abs/2510.12228v1,arXiv,0
"Human core object recognition depends on the selective use of visual information, but the strategies guiding these choices are difficult to measure directly. We present MAPS (Masked Attribution-based Probing of Strategies), a behaviorally validated computational tool that tests whether explanations derived from artificial neural networks (ANNs) can also explain human vision. MAPS converts attribution maps into explanation-masked images (EMIs) and compares image-by-image human accuracies on these minimal images with limited pixel budgets with accuracies on the full stimuli. MAPS provides a principled way to evaluate and choose among competing ANN interpretability methods. In silico, EMI-based behavioral similarity between models reliably recovers the ground-truth similarity computed from their attribution maps, establishing which explanation methods best capture the model's strategy. When applied to humans and macaques, MAPS identifies ANN-explanation combinations whose explanations align most closely with biological vision, achieving the behavioral validity of Bubble masks while requiring far fewer behavioral trials. Because it needs only access to model attributions and a modest set of behavioral data on the original images, MAPS avoids exhaustive psychophysics while offering a scalable tool for adjudicating explanations and linking human behavior, neural activity, and model decisions under a common standard.",Neuroscience,http://arxiv.org/abs/2510.12141v2,arXiv,0
"Microwell microfluidics has been utilized for single-cell analysis to reveal heterogeneity in gene expression, signaling pathways, and phenotypic responses for identifying rare cell types, understanding disease progression, and developing more precise therapeutic strategies. However, designing microwell microfluidics is a considerably complex task, requiring knowledge, experience, and CAD software, as well as manual intervention, which often fails initial designs, demanding multiple costly and time-consuming iterations. In this study, we establish an autonomous large language model (LLM)-driven microwell design framework to generate code-based computer-aided design (CAD) scripts, that enables the rapid and reproducible creation of microwells with diverse geometries and imaging-based analysis. We propose a multimodal large language model (MLLM)-logistic regression framework based on integrating high-level semantic descriptions generated by MLLMs with image embeddings for image classification tasks, aiming to identify microwell occupancy and microwell shape. The fused multimodal representation is input to a logistic regression model, which is both interpretable and computationally efficient. We achieved significant improvements, exceeding 0.92 for occupancy classification and 0.99 for shape classification, across all evaluated MLLMs, compared with 0.50 and 0.55, respectively, when relying solely on direct classification. The MLLM-logistic regression framework is a scalable, efficient solution for high-throughput microwell image analysis. Our study demonstrates an autonomous design microwell platform by translating natural language prompts into optimized device geometries, CAD scripts and image analysis, facilitating the development of next-generation digital discovery by integration of literature mining, autonomous design and experimental data analysis.",Neuroscience,http://arxiv.org/abs/2510.13883v1,arXiv,0
"This thesis delves into the world of non-invasive electrophysiological brain signals like electroencephalography (EEG) and magnetoencephalography (MEG), focusing on modelling and decoding such data. The research aims to investigate what happens in the brain when we perceive visual stimuli or engage in covert speech (inner speech) and enhance the decoding performance of such stimuli. The thesis is divided into two main sections, methodological and experimental work. A central concern in both sections is the large variability present in electrophysiological recordings, whether it be within-subject or between-subject variability, and to a certain extent between-dataset variability. In the methodological sections, we explore the potential of deep learning for brain decoding. We present advancements in decoding visual stimuli using linear models at the individual subject level. We then explore how deep learning techniques can be employed for group decoding, introducing new methods to deal with between-subject variability. Finally, we also explores novel forecasting models of MEG data based on convolutional and Transformer-based architectures. In particular, Transformer-based models demonstrate superior capabilities in generating signals that closely match real brain data, thereby enhancing the accuracy and reliability of modelling the brain's electrophysiology. In the experimental section, we present a unique dataset containing high-trial inner speech EEG, MEG, and preliminary optically pumped magnetometer (OPM) data. Our aim is to investigate different types of inner speech and push decoding performance by collecting a high number of trials and sessions from a few participants. However, the decoding results are found to be mostly negative, underscoring the difficulty of decoding inner speech.",Neuroscience,http://arxiv.org/abs/2510.24733v1,arXiv,0
"Characterizing interactions between brain areas is a fundamental goal of systems neuroscience. While such analyses are possible when areas are recorded simultaneously, it is rare to observe all combinations of areas of interest within a single animal or recording session. How can we leverage multi-animal datasets to better understand multi-area interactions? Building on recent progress in large-scale, multi-animal models, we introduce NeuroPaint, a masked autoencoding approach for inferring the dynamics of unrecorded brain areas. By training across animals with overlapping subsets of recorded areas, NeuroPaint learns to reconstruct activity in missing areas based on shared structure across individuals. We train and evaluate our approach on synthetic data and two multi-animal, multi-area Neuropixels datasets. Our results demonstrate that models trained across animals with partial observations can successfully in-paint the dynamics of unrecorded areas, enabling multi-area analyses that transcend the limitations of any single experiment.",Neuroscience,http://arxiv.org/abs/2510.11924v1,arXiv,0
"The accuracy with which the human proprioceptive system estimates hand speed is not well understood. To investigate this, we designed an experiment using hobby-grade mechatronics parts and integrated it as a laboratory exercise in a large remote laboratory course. In a simple joint position reproduction task, participants (N = 191) grasped a servomotor-driven shaft with one hand as it followed a randomized trajectory composed of sinusoidal submovements. They simultaneously attempted to reproduce the movement by turning the shaft of a potentiometer with the other hand. Focusing on the first movement of the trajectory, we found that participants consistently overestimated the speed of the slowest rotations by ~45% and underestimated the speed of the fastest rotations also by ~30%. Speed estimation errors were near zero for trajectories with peak velocities ~63 deg/s. Participants' movements also overshot slow trajectories and undershot fast trajectories. We show that these trajectory errors can be explained by a model in which the proprioceptive system integrates velocity misestimates to infer position.",Neuroscience,http://arxiv.org/abs/2510.11664v1,arXiv,0
"Games have long been a microcosm for studying planning and reasoning in both natural and artificial intelligence, especially with a focus on expert-level or even super-human play. But real life also pushes human intelligence along a different frontier, requiring people to flexibly navigate decision-making problems that they have never thought about before. Here, we use novice gameplay to study how people make decisions and form judgments in new problem settings. We show that people are systematic and adaptively rational in how they play a game for the first time, or evaluate a game (e.g., how fair or how fun it is likely to be) before they have played it even once. We explain these capacities via a computational cognitive model that we call the ""Intuitive Gamer"". The model is based on mechanisms of fast and flat (depth-limited) goal-directed probabilistic simulation--analogous to those used in Monte Carlo tree-search models of expert game-play, but scaled down to use very few stochastic samples, simple goal heuristics for evaluating actions, and no deep search. In a series of large-scale behavioral studies with over 1000 participants and 121 two-player strategic board games (almost all novel to our participants), our model quantitatively captures human judgments and decisions varying the amount and kind of experience people have with a game--from no experience at all (""just thinking""), to a single round of play, to indirect experience watching another person and predicting how they should play--and does so significantly better than much more compute-intensive expert-level models. More broadly, our work offers new insights into how people rapidly evaluate, act, and make suggestions when encountering novel problems, and could inform the design of more flexible and human-like AI systems that can determine not just how to solve new tasks, but whether a task is worth thinking about at all.",Neuroscience,http://arxiv.org/abs/2510.11503v1,arXiv,0
"Understanding how learning algorithms shape the computational strategies that emerge in neural networks remains a fundamental challenge in machine intelligence. While network architectures receive extensive attention, the role of the learning paradigm itself in determining emergent dynamics remains largely unexplored. Here we demonstrate that reinforcement learning (RL) and supervised learning (SL) drive recurrent neural networks (RNNs) toward fundamentally different computational solutions when trained on identical decision-making tasks. Through systematic dynamical systems analysis, we reveal that RL spontaneously discovers hybrid attractor architectures, combining stable fixed-point attractors for decision maintenance with quasi-periodic attractors for flexible evidence integration. This contrasts sharply with SL, which converges almost exclusively to simpler fixed-point-only solutions. We further show that RL sculpts functionally balanced neural populations through a powerful form of implicit regularization -- a structural signature that enhances robustness and is conspicuously absent in the more heterogeneous solutions found by SL-trained networks. The prevalence of these complex dynamics in RL is controllably modulated by weight initialization and correlates strongly with performance gains, particularly as task complexity increases. Our results establish the learning algorithm as a primary determinant of emergent computation, revealing how reward-based optimization autonomously discovers sophisticated dynamical mechanisms that are less accessible to direct gradient-based optimization. These findings provide both mechanistic insights into neural computation and actionable principles for designing adaptive AI systems.",Neuroscience,http://arxiv.org/abs/2510.11162v1,arXiv,0
"Chromatin is repeatedly deformed in vivo during transcription, nuclear remodeling, and confined migration - yet how mechanical response varies from locus to locus, and how it relates to epigenetic state, remains unclear. We develop a theory to infer locus-specific viscoelasticity from three-dimensional genome organization. Using chromatin structures derived from contact maps, we calculate frequency-dependent storage and loss moduli for individual loci and establish that the mechanical properties are determined both by chromatin epigenetic marks and organization. On large length scales, chromatin exhibits Rouse-like viscoelastic scaling, but this coarse behavior masks extensive heterogeneity at the single-locus level. Loci segregate into two mechanical subpopulations with distinct longest relaxation times: one characterized by single-timescale and another by multi-timescale relaxation. The multi-timescale loci are strongly enriched in active marks, and the longest relaxation time for individual loci correlates inversely with effective local stiffness. Pull-release simulations further predict a time-dependent susceptibility: H3K27ac-rich loci deform more under sustained forcing yet can resist brief, large impulses. At finer genomic scales, promoters, enhancers, and gene bodies emerge as ""viscoelastic islands"" aligned with their focal interactions. Together, these results suggest that chromatin viscoelasticity is an organized, epigenetically coupled property of the 3D genome, providing a mechanistic layer that may influence enhancer-promoter communication, condensate-mediated organization, and response to cellular mechanical stress. The prediction that locus-specific mechanics in chromatin are controlled by 3D structures as well as the epigenetic states is amenable to experimental test.",Bioinformatics,http://arxiv.org/abs/2512.22820v1,arXiv,1
"Population-scale pangenome analysis increasingly requires representations that unify single-nucleotide and structural variation while remaining scalable across large cohorts. Existing formats are typically sequence-centric, path-centric, or sample-centric, and often obscure population structure or fail to exploit carrier sparsity. We introduce the H1 pan-graph-matrix, an allele-centric representation that encodes exact haplotype membership using adaptive per-allele compression. By treating alleles as first-class objects and selecting optimal encodings based on carrier distribution, H1 achieves near-optimal storage across both common and rare variants. We further introduce H2, a path-centric dual representation derived from the same underlying allele-haplotype incidence information that restores explicit haplotype ordering while remaining exactly equivalent in information content. Using real human genome data, we show that this representation yields substantial compression gains, particularly for structural variants, while remaining equivalent in information content to pangenome graphs. H1 provides a unified, population-aware foundation for scalable pangenome analysis and downstream applications such as rare-variant interpretation and drug discovery.",Bioinformatics,http://arxiv.org/abs/2512.21320v1,arXiv,1
"Chromatin conformation capture technologies such as Hi-C have revealed that the genome is organized in a hierarchy of structures spanning multiple scales observed at different resolutions. Current algorithms often focus on specific interaction patterns found at a specific Hi-C resolution. We present BHi-Cect 2.0, a method that leverages Hi-C data at multiple resolutions to describe chromosome architecture as nested preferentially self-interacting clusters using spectral clustering. This new version describes the hierarchical configuration of chromosomes by now integrating multiple Hi-C data resolutions. Our new implementation offers a more comprehensive description of the multi-scale architecture of the chromosomes. We further provide these functionalities as an R package to assist their integration with other computational pipelines. The BHiCect 2.0 R packages is available on github at https://github.com/princeps091-binf/BHiCect2with the version used for this manuscript on Zenodo at https://doi.org/10.5281/zenodo.17985844.",Bioinformatics,http://arxiv.org/abs/2512.17512v1,arXiv,1
"DNA language models have advanced genomics, but their downstream performance varies widely due to differences in tokenization, pretraining data, and architecture. We argue that a major bottleneck lies in tokenizing sparse and unevenly distributed DNA sequence motifs, which are critical for accurate and interpretable models. To investigate, we systematically benchmark k-mer and Byte-Pair Encoding (BPE) tokenizers under controlled pretraining budget, evaluating across multiple downstream tasks from five datasets. We find that tokenizer choice induces task-specific trade-offs, and that vocabulary size and tokenizer training data strongly influence the biological knowledge captured. Notably, BPE tokenizers achieve strong performance when trained on smaller but biologically significant data. Building on these insights, we introduce DNAMotifTokenizer, which directly incorporates domain knowledge of DNA sequence motifs into the tokenization process. DNAMotifTokenizer consistently outperforms BPE across diverse benchmarks, demonstrating that knowledge-infused tokenization is crucial for learning powerful, interpretable, and generalizable genomic representations.",Bioinformatics,http://arxiv.org/abs/2512.17126v2,arXiv,1
"Predicting the sensitivity of cancer cell lines to PLX-4720, a preclinical BRAF inhibitor, requires models capable of capturing the multilayered regulation of oncogenic signaling. Single-omics predictors are often insufficient because drug response is shaped by interactions among genomic alterations, epigenetic regulation, transcriptional activity, protein signaling, metabolic state, and network-level context. In this study we develop an attention-based multi-omics integration framework using genomic, epigenomic, transcriptomic, proteomic, metabolomic, and protein interaction data from the GDSC1 panel. Each modality is encoded into a latent representation using feed-forward neural networks or graph convolutional networks, and fused through an attention mechanism that assigns modality-specific importance weights. A regression model is then used to predict PLX-4720 response. Across single- and multi-omics configurations, the best performance is achieved by integrating genomics and transcriptomics, which yields validation R2 values above 0.92. This reflects the complementary roles of mutational status and downstream transcriptional activation in shaping sensitivity to BRAF inhibition. Epigenomics is the strongest single-omics predictor, while metabolomics and PPI data contribute additional context when combined with other modalities. Integration of three to five omics layers improves stability but does not surpass the accuracy of the best two-modality combinations, likely due to information redundancy and sample-size imbalance. These findings highlight the importance of modality selection rather than maximal data depth. The proposed framework provides an efficient and biologically grounded strategy for drug response prediction and supports the development of precision pharmacogenomics.",Bioinformatics,http://arxiv.org/abs/2512.12113v1,arXiv,1
"Affordable, high-quality whole-genome assemblies have made it possible to construct rich pangenomes that capture haplotype diversity across many species. As these datasets grow, they motivate the development of specialized techniques capable of handling the dense sequence variation found in large groups of related genomes. A common strategy is to encode pangenomic information in graph form, which provides a flexible substrate for improving algorithms in areas such as alignment, visualization, and functional analysis. Methods built on these graph models have already shown clear advantages in core bioinformatics workflows, including read mapping, variant discovery, and genotyping. By integrating multiple sequence and coordinate representations into a single structure, pangenome graphs offer a unified and expressive framework for comparative genomics. Although it remains unclear whether graph-based references will ultimately supplant traditional linear genomes, their versatility ensures that they will play a central role in emerging pangenomic approaches. This paper introduces an algorithm to mine a chain of sequences in pangenome graphs that might be useful in the functional analysis of pangenome graphs. Specifically, the algorithm calculates all maximal paths in a pangenome graph aligning with a given chain of sequences in the segments of the path vertices, possibly with some maximal gap as specified by the user.",Bioinformatics,http://arxiv.org/abs/2512.12052v2,arXiv,1
"Early detection and characterization of coronavirus disease (COVID-19), caused by SARS-CoV-2, remain critical for effective clinical response and public-health planning. The global availability of large-scale viral sequence data presents significant opportunities for computational analysis; however, existing approaches face notable limitations. Phylogenetic tree-based methods are computationally intensive and do not scale efficiently to today's multi-million-sequence datasets. Similarly, current embedding-based techniques often rely on aligned sequences or exhibit suboptimal predictive performance and high runtime costs, creating barriers to practical large-scale analysis. In this study, we focus on the most prevalent SARS-CoV-2 lineages associated with the spike protein region and introduce a scalable embedding method that leverages hashing to generate compact, low-dimensional representations of spike sequences. These embeddings are subsequently used to train a variety of machine learning models for supervised lineage classification. We conduct an extensive evaluation comparing our approach with multiple baseline and state-of-the-art biological sequence embedding methods across diverse metrics. Our results demonstrate that the proposed embeddings offer substantial improvements in efficiency, achieving up to 86.4\% classification accuracy while reducing embedding generation time by as much as 99.81\%. This highlights the method's potential as a fast, effective, and scalable solution for large-scale viral sequence analysis.",Bioinformatics,http://arxiv.org/abs/2512.10147v1,arXiv,1
"Next-Generation Sequencing (NGS) has become a cornerstone of genomic research, yet the complexity of downstream analysis-ranging from differential expression gene (DEG) identification to biological interpretations-remains a significant barrier for researchers lacking specialized computational and biological expertise. While recent studies have introduced AI agents for RNA-seq analysis, most focus on general workflows without offering tailored interpretations or guidance for novices. To address this gap, we developed an Agentic AI model designed to automate NGS downstream analysis, provide literature-backed interpretations, and autonomously recommend advanced analytical methods. Built on the Llama 3 70B Large Language Model (LLM) and a Retrieval-Augmented Generation (RAG) framework, the model is deployed as an interactive Streamlit web application. The system integrates standard bioinformatics tools (Biopython, GSEApy, gProfiler) to execute core analyses, including DEG identification, clustering, and pathway enrichment. Uniquely, the agent utilizes RAG to query PubMed via Entrez, synthesizing biological insights and validating hypotheses with current literature. In a case study using cancer-related dataset, the model successfully identified significant DEGs, visualized clinical correlations, and derived evidence-based insights (e.g., linking BRAF mutations to prognosis), subsequently executing advanced survival modeling upon user selection. This framework democratizes bioinformatics by enabling researchers with limited backgrounds to seamlessly transition from basic data processing to advanced hypothesis testing and validation.",Bioinformatics,http://arxiv.org/abs/2512.09964v1,arXiv,1
"Batch effects pose a significant challenge in the analysis of single-cell omics data, introducing technical artifacts that confound biological signals. While various computational methods have achieved empirical success in correcting these effects, they lack the formal theoretical guarantees required to assess their reliability and generalization. To bridge this gap, we introduce Mixture-Model-based Data Harmonization (MoDaH), a principled batch correction algorithm grounded in a rigorous statistical framework.   Under a new Gaussian-mixture-model with explicit parametrization of batch effects, we establish the minimax optimal error rates for batch correction and prove that MoDaH achieves this rate by leveraging the recent theoretical advances in clustering data from anisotropic Gaussian mixtures. This constitutes, to the best of our knowledge, the first theoretical guarantee for batch correction. Extensive experiments on diverse single-cell RNA-seq and spatial proteomics datasets demonstrate that MoDaH not only attains theoretical optimality but also achieves empirical performance comparable to or even surpassing those of state-of-the-art heuristics (e.g., Harmony, Seurat-V5, and LIGER), effectively balancing the removal of technical noise with the conservation of biological signal.",Bioinformatics,http://arxiv.org/abs/2512.09259v1,arXiv,1
"Personalized neoantigen vaccines represent a promising immunotherapy approach that harnesses tumor-specific antigens to stimulate anti-tumor immune responses. However, the design of these vaccines requires sophisticated computational workflows to predict and prioritize neoantigen candidates from patient sequencing data, coupled with rigorous review to ensure candidate quality. While numerous computational tools exist for neoantigen prediction, to our knowledge, there are no established protocols detailing the complete process from raw sequencing data through systematic candidate selection. Here, we present ImmunoNX (Immunogenomics Neoantigen eXplorer), an end-to-end protocol for neoantigen prediction and vaccine design that has supported over 185 patients across 11 clinical trials. The workflow integrates tumor DNA/RNA and matched normal DNA sequencing data through a computational pipeline built with Workflow Definition Language (WDL) and executed via Cromwell on Google Cloud Platform. ImmunoNX employs consensus-based variant calling, in-silico HLA typing, and pVACtools for neoantigen prediction. Additionally, we describe a two-stage immunogenomics review process with prioritization of neoantigen candidates, enabled by pVACview, followed by manual assessment of variants using the Integrative Genomics Viewer (IGV). This workflow enables vaccine design in under three months. We demonstrate the protocol using the HCC1395 breast cancer cell line dataset, identifying 78 high-confidence neoantigen candidates from 322 initial predictions. Although demonstrated here for vaccine development, this workflow can be adapted for diverse neoantigen therapies and experiments. Therefore, this protocol provides the research community with a reproducible, version-controlled framework for designing personalized neoantigen vaccines, supported by detailed documentation, example datasets, and open-source code.",Bioinformatics,http://arxiv.org/abs/2512.08226v1,arXiv,1
"Summary: We present needLR, a structural variant (SV) annotation tool that can be used for filtering and prioritization of candidate pathogenic SVs from long-read sequencing data using population allele frequencies, annotations for genomic context, and gene-phenotype associations. When using population data from 500 presumably healthy individuals to evaluate nine test cases with known pathogenic SVs, needLR assigned allele frequencies to over 97.5% of all detected SVs and reduced the average number of novel genic SVs to 121 per case while retaining all known pathogenic variants. Availability and Implementation: needLR is implemented in bash with dependencies including Truvari v4.2.2, BEDTools v2.31.1, and BCFtools v1.19. Source code, documentation, and pre-computed population allele frequency data are freely available at https://github.com/jgust1/needLR under an MIT license.",Bioinformatics,http://arxiv.org/abs/2512.08175v1,arXiv,1
"Understanding the underlying linguistic rules of plant genomes remains a fundamental challenge in computational biology. Recent advances including AgroNT and PDLLMs have made notable progress although, they suffer from excessive parameter size and limited ability to model the bidirectional nature of DNA strands respectively. To address these limitations, we propose PlantBiMoE, a lightweight and expressive plant genome language model that integrates bidirectional Mamba and a Sparse Mixture-of-Experts (SparseMoE) framework. The bidirectional Mamba enables the model to effectively capture structural dependencies across both the forward and reverse DNA strands, while SparseMoE significantly reduces the number of active parameters, improving computational efficiency without sacrificing modeling capacity. We evaluated and tested our model on the Modified Plants Genome Benchmark (MPGB), an enhanced genomic benchmark, which consolidates 31 datasets across 11 representative tasks, with input sequence lengths ranging from 50 to 6,000 bp. Experimental results demonstrate that PlantBiMoE achieves the best performance on 20 out of 31 datasets and the average best when comparing with existing models. In summary, all above results demonstrate that our model can effectively represent plant genomic sequences, serving as a robust computational tool for diverse genomic tasks, while making substantive contributions to plant genomics, gene editing, and synthetic biology. The code is available at: https://github.com/HUST-Keep-Lin/PlantBiMoE",Bioinformatics,http://arxiv.org/abs/2512.07113v1,arXiv,1
"Summary: Modern omics experiments now involve multiple conditions and complex designs, producing an increasingly large set of differential expression and functional enrichment analysis results. However, no standardized data structure exists to store and contextualize these results together with their metadata, leaving researchers with an unmanageable and potentially non-reproducible collection of results that are difficult to navigate and/or share. Here we introduce DeeDeeExperiment, a new S4 class for managing and storing omics data analysis results, implemented within the Bioconductor ecosystem, which promotes interoperability, reproducibility and good documentation. This class extends the widely used SingleCellExperiment object by introducing dedicated slots for Differential Expression (DEA) and Functional Enrichment Analysis (FEA) results, allowing users to organize, store, and retrieve information on multiple contrasts and associated metadata within a single data object, ultimately streamlining the management and interpretation of many omics datasets. Availability and implementation: DeeDeeExperiment is available on Bioconductor under the MIT license (https://bioconductor.org/packages/DeeDeeExperiment), with its development version also available on Github (https://github.com/imbeimainz/DeeDeeExperiment).",Bioinformatics,http://arxiv.org/abs/2512.05731v1,arXiv,1
"Numerous diseases, particularly autoimmune disorders, are associated with the human leukocyte antigen (HLA), a small genomic region located on human chromosome 6. Adequate characterization of linkage disequilibrium (LD) in the HLA across populations is crucial for identifying genetic markers associated with specific traits and phenotypes. However, current LD measures often fail to capture HLA's structural complexity due to methodological limitations and sensitivity to low-frequency variants, marginal allele frequencies, and haplotype composition. To address these challenges, we introduced the Conditional Informatics Correlation Coefficient (CICC), which integrates conditional probability, information content, and haplotype-aware XOR logic to quantify LD robustly. When applied to high-resolution haploid genomes from the Human Pangenome Reference Consortium (HPRC), CICC revealed 10 novel high-LD regions in HLA. Further analyses using the 1000 Genomes Project and Genome Asia datasets identified nine strongly linked regions shared across five global populations-five in Class I and four in Class II. These results demonstrate CICC's ability to capture complex HLA LD structures across populations, highlighting its broad potential for disease gene mapping, population genomics, and guiding precision medicine.",Bioinformatics,http://arxiv.org/abs/2512.05573v1,arXiv,1
"Computing haplotypes from sequencing data, i.e. haplotype assembly, is an important component of foundational molecular and population genetics problems, including interpreting the effects of genetic variation on complex traits and reconstructing genealogical relationships. Assembling the haplotypes of polyploid genomes remains a significant challenge due to the exponential search space of haplotype phasings and read assignment ambiguity; the latter challenge is particularly difficult for polyploid haplotype assemblers since the information contained within the observed sequence reads is often insufficient for unambiguous haplotype assignment in polyploid genomes. We present pHapCompass, probabilistic haplotype assembly algorithms for diploid and polyploid genomes that explicitly model and propagate read assignment ambiguity to compute a distribution over polyploid haplotype phasings. We develop graph theoretic algorithms to enable statistical inference and uncertainty quantification despite an exponential space of possible phasings. Since prior work evaluates polyploid haplotype assembly on synthetic genomes that do not reflect the realistic genomic complexity of polyploidy organisms, we develop a computational workflow for simulating genomes and DNA-seq for auto- and allopolyploids. Additionally, we generalize the vector error rate and minimum error correction evaluation criteria for partially phased haplotypes. Benchmarking of pHapCompass and several existing polyploid haplotype assemblers shows that pHapCompass yields competitive performance across varying genomic complexities and polyploid structures while retaining an accurate quantification of phase uncertainty. The source code for pHapCompass, simulation scripts, and datasets are freely available at https://github.com/bayesomicslab/pHapCompass.",Bioinformatics,http://arxiv.org/abs/2512.04393v1,arXiv,1
"Accurate and scalable cell type annotation remains a challenge in single-cell transcriptomics, especially when datasets exhibit strong batch effects or contain previously unseen cell populations. Here we introduce SpikGPT, a hybrid deep learning framework that integrates scGPT-derived cell embeddings with a spiking Transformer architecture to achieve efficient and robust annotation. scGPT provides biologically informed dense representations of each cell, which are further processed by a multi-head Spiking Self-Attention mechanism for energy-efficient feature extraction. Across multiple benchmark datasets, SpikGPT consistently matches or exceeds the performance of leading annotation tools. Notably, SpikGPT uniquely identifies unseen cell types by assigning low-confidence predictions to an ""Unknown"" category, allowing accurate rejection of cell states absent from the training reference. Together, these results demonstrate that SpikGPT is a versatile and reliable annotation tool capable of generalizing across datasets, resolving complex cellular heterogeneity, and facilitating discovery of novel or disease-associated cell populations.",Bioinformatics,http://arxiv.org/abs/2512.03286v1,arXiv,1
"Wastewater-based genomic surveillance has emerged as a powerful tool for population-level viral monitoring, offering comprehensive insights into circulating viral variants across entire communities. However, this approach faces significant computational challenges stemming from high sequencing noise, low viral coverage, fragmented reads, and the complete absence of labeled variant annotations. Traditional reference-based variant calling pipelines struggle with novel mutations and require extensive computational resources. We present a comprehensive framework for unsupervised viral variant detection using Vector-Quantized Variational Autoencoders (VQ-VAE) that learns discrete codebooks of genomic patterns from k-mer tokenized sequences without requiring reference genomes or variant labels. Our approach extends the base VQ-VAE architecture with masked reconstruction pretraining for robustness to missing data and contrastive learning for highly discriminative embeddings. Evaluated on SARS-CoV-2 wastewater sequencing data comprising approximately 100,000 reads, our VQ-VAE achieves 99.52% mean token-level accuracy and 56.33% exact sequence match rate while maintaining 19.73% codebook utilization (101 of 512 codes active), demonstrating efficient discrete representation learning. Contrastive fine-tuning with different projection dimensions yields substantial clustering improvements: 64-dimensional embeddings achieve +35% Silhouette score improvement (0.31 to 0.42), while 128-dimensional embeddings achieve +42% improvement (0.31 to 0.44), clearly demonstrating the impact of embedding dimensionality on variant discrimination capability. Our reference-free framework provides a scalable, interpretable approach to genomic surveillance with direct applications to public health monitoring.",Bioinformatics,http://arxiv.org/abs/2512.03158v1,arXiv,1
"Single-cell RNA sequencing (scRNA-seq) is essential for decoding tumor heterogeneity. However, pan-cancer research still faces two key challenges: learning discriminative and efficient single-cell representations, and establishing a comprehensive evaluation benchmark. In this paper, we introduce PanFoMa, a lightweight hybrid neural network that combines the strengths of Transformers and state-space models to achieve a balance between performance and efficiency. PanFoMa consists of a front-end local-context encoder with shared self-attention layers to capture complex, order-independent gene interactions; and a back-end global sequential feature decoder that efficiently integrates global context using a linear-time state-space model. This modular design preserves the expressive power of Transformers while leveraging the scalability of Mamba to enable transcriptome modeling, effectively capturing both local and global regulatory signals. To enable robust evaluation, we also construct a large-scale pan-cancer single-cell benchmark, PanFoMaBench, containing over 3.5 million high-quality cells across 33 cancer subtypes, curated through a rigorous preprocessing pipeline. Experimental results show that PanFoMa outperforms state-of-the-art models on our pan-cancer benchmark (+4.0\%) and across multiple public tasks, including cell type annotation (+7.4\%), batch integration (+4.0\%) and multi-omics integration (+3.1\%). The code is available at https://github.com/Xiaoshui-Huang/PanFoMa.",Bioinformatics,http://arxiv.org/abs/2512.03111v1,arXiv,1
"Cell clustering is crucial for uncovering cellular heterogeneity in single-cell RNA sequencing (scRNA-seq) data by identifying cell types and marker genes. Despite its importance, benchmarks for scRNA-seq clustering methods remain fragmented, often lacking standardized protocols and failing to incorporate recent advances in artificial intelligence. To fill these gaps, we present scCluBench, a comprehensive benchmark of clustering algorithms for scRNA-seq data. First, scCluBench provides 36 scRNA-seq datasets collected from diverse public sources, covering multiple tissues, which are uniformly processed and standardized to ensure consistency for systematic evaluation and downstream analyses. To evaluate performance, we collect and reproduce a range of scRNA-seq clustering methods, including traditional, deep learning-based, graph-based, and biological foundation models. We comprehensively evaluate each method both quantitatively and qualitatively, using core performance metrics as well as visualization analyses. Furthermore, we construct representative downstream biological tasks, such as marker gene identification and cell type annotation, to further assess the practical utility. scCluBench then investigates the performance differences and applicability boundaries of various clustering models across diverse analytical tasks, systematically assessing their robustness and scalability in real-world scenarios. Overall, scCluBench offers a standardized and user-friendly benchmark for scRNA-seq clustering, with curated datasets, unified evaluation protocols, and transparent analyses, facilitating informed method selection and providing valuable insights into model generalizability and application scope.",Bioinformatics,http://arxiv.org/abs/2512.02471v1,arXiv,1
"Machine- and deep-learning approaches for biological sequences depend critically on transforming raw DNA, RNA, and protein FASTA files into informative numerical representations. However, this process is often fragmented across multiple libraries and preprocessing steps, which creates a barrier for researchers without extensive computational expertise. To address this gap, we developed deepFEPS, an open-source toolkit that unifies state-of-the-art feature extraction methods for sequence data within a single, reproducible workflow. deepFEPS integrates five families of modern feature extractors - k-mer embeddings (Word2Vec, FastText), document-level embeddings (Doc2Vec), transformer-based encoders (DNABERT, ProtBERT, and ESM2), autoencoder-derived latent features, and graph-based embeddings - into one consistent platform. The system accepts FASTA input via a web interface or command-line tool, exposes key model parameters, and outputs analysis-ready feature matrices (CSV). Each run is accompanied by an automatic quality-control report including sequence counts, dimensionality, sparsity, variance distributions, class balance, and diagnostic visualizations. By consolidating advanced sequence embeddings into one environment, deepFEPS reduces preprocessing overhead, improves reproducibility, and shortens the path from raw sequences to downstream machine- and deep-learning applications. deepFEPS lowers the practical barrier to modern representation learning for bioinformatics, enabling both novice and expert users to generate advanced embeddings for classification, clustering, and predictive modeling. Its unified framework supports exploratory analyses, high-throughput studies, and integration into institutional workflows, while remaining extensible to emerging models and methods. The webserver is accessible at https://hdismail.com/deepfeps2/.",Bioinformatics,http://arxiv.org/abs/2511.22821v1,arXiv,1
"The ancestral recombination graph (ARG) is the model of choice in statistical genetics to model population ancestries. Software capable of simulating ARGs on a genome scale within a reasonable amount of time are now widely available for most practical use cases. While the inverse problem of inferring ancestries from a sample of haplotypes has seen major progress in the last decade, it does not enjoy the same level of advancement as its counterpart. Up until recently, even moderately sized samples could only be handled using heuristics. In recent years, the possibility of model-based inference for datasets closer to ""real world"" scenarios has become a reality, largely due to the development of threading-based samplers. This article introduces Moonshine.jl, a Julia package that has the ability, among other things, to infer ARGs for samples of thousands of human haplotypes of sizes on the order of hundreds of megabases within a reasonable amount of time. On recent hardware, our package is able to infer an ARG for samples of densely haplotyped (over one marker/kilobase) human chromosomes of sizes up to 10000 in well under a day on data simulated by msprime. Scaling up simulation on a compute cluster is straightforward thanks to a strictly single-threaded implementation. While model-based, it does not resort to threading but rather places restrictions on probability distributions typically used in simulation software in order to enforce sample consistency. In addition to being efficient, a strong emphasis is placed on ease of use and integration into the biostatistical software ecosystem.",Bioinformatics,http://arxiv.org/abs/2511.21124v1,arXiv,1
"Representation learning on multi-omics data is challenging due to extreme dimensionality, modality heterogeneity, and cohort-specific batch effects. While pre-trained transformer backbones have shown broad generalization capabilities in biological sequence modeling, their application to multi-omics integration remains underexplored. We present MoRE (Multi-Omics Representation Embedding), a framework that repurposes frozen pre-trained transformers to align heterogeneous assays into a shared latent space. Unlike purely generative approaches, MoRE employs a parameter-efficient fine-tuning (PEFT) strategy, prioritizing cross-sample and cross-modality alignment over simple sequence reconstruction. Specifically, MoRE attaches lightweight, modality-specific adapters and a task-adaptive fusion layer to the frozen backbone. It optimizes a masked modeling objective jointly with supervised contrastive and batch-invariant alignment losses, yielding structure-preserving embeddings that generalize across unseen cell types and platforms. We benchmark MoRE against established baselines, including scGPT, scVI, and Harmony with Scrublet, evaluating integration fidelity, rare population detection, and modality transfer. Our results demonstrate that MoRE achieves competitive batch robustness and biological conservation while significantly reducing trainable parameters compared to fully fine-tuned models. This work positions MoRE as a practical step toward general-purpose omics foundation models.",Bioinformatics,http://arxiv.org/abs/2511.20382v2,arXiv,1
"Motivation: Modern genomics laboratories generate massive volumes of sequencing data, often resulting in significant storage costs. Genomics storage consists of duplicate files, temporary processing files, and redundant intermediate data. Results: We developed SeqManager, a web-based application that provides automated identification, classification, and management of sequencing data files with intelligent duplicate detection. It also detects intermediate sequencing files that can safely be removed. Evaluation across four genomics laboratory settings demonstrate that our tool is fast and has a very low memory footprint.",Bioinformatics,http://arxiv.org/abs/2511.20727v1,arXiv,1
"Multi-assembly methods rely at their core on a flow decomposition problem, namely, decomposing a weighted graph into weighted paths or walks. However, most results over the past decade have focused on decompositions over directed acyclic graphs (DAGs). This limitation has lead to either purely heuristic methods, or in applications transforming a graph with cycles into a DAG via preprocessing heuristics. In this paper we show that flow decomposition problems can be solved in practice also on general graphs with cycles, via a framework that yields fast and flexible Mixed Integer Linear Programming (MILP) formulations.   Our key technique relies on the graph-theoretic notion of dominator tree, which we use to find all safe sequences of edges, that are guaranteed to appear in some walk of any flow decomposition solution. We generalize previous results from DAGs to cyclic graphs, by showing that maximal safe sequences correspond to extensions of common leaves of two dominator trees, and that we can find all of them in time linear in their size. Using these, we can accelerate MILPs for any flow decomposition into walks in general graphs, by setting to (at least) 1 suitable variables encoding solution walks, and by setting to 0 other walks variables non-reachable to and from safe sequences. This reduces model size and eliminates costly linearizations of MILP variable products.   We experiment with three decomposition models (Minimum Flow Decomposition, Least Absolute Errors and Minimum Path Error), on four bacterial datasets. Our pre-processing enables up to thousand-fold speedups and solves even under 30 seconds many instances otherwise timing out. We thus hope that our dominator-based MILP simplification framework, and the accompanying software library can become building blocks in multi-assembly applications.",Bioinformatics,http://arxiv.org/abs/2511.19153v1,arXiv,1
"Modern genomic analyses increasingly rely on pangenomes, that is, representations of the genome of entire populations. The simplest representation of a pangenome is a set of individual genome sequences. Compared to e.g. sequence graphs, this has the advantage that efficient exact search via indexes based on the Burrows-Wheeler Transform (BWT) is possible, that no chimeric sequences are created, and that the results are not influenced by heuristics. However, such an index may report a match in thousands of positions even if these all correspond to the same locus, making downstream analysis unnecessarily expensive. For sufficiently similar sequences (e.g. human chromosomes), a multiple sequence alignment (MSA) can be computed. Since an MSA tends to group similar strings in the same columns, it is likely that a string occurring thousands of times in the pangenome can be described by very few columns in the MSA. We describe a method to tag entries in the BWT with the corresponding column in the MSA and develop an index that can map matches in the BWT to columns in the MSA in time proportional to the output. As a by-product, we can efficiently project a match to a designated reference genome, a capability that current pangenome aligners based on the BWT lack.",Bioinformatics,http://arxiv.org/abs/2511.19068v1,arXiv,1
"Spatial transcriptomics (ST) is a novel technology that enables the observation of gene expression at the resolution of individual spots within pathological tissues. ST quantifies the expression of tens of thousands of genes in a tissue section; however, heavy observational noise is often introduced during measurement. In prior studies, to ensure meaningful assessment, both training and evaluation have been restricted to only a small subset of highly variable genes, and genes outside this subset have also been excluded from the training process. However, since there are likely co-expression relationships between genes, low-expression genes may still contribute to the estimation of the evaluation target. In this paper, we propose $Auxiliary \ Gene \ Learning$ (AGL) that utilizes the benefit of the ignored genes by reformulating their expression estimation as auxiliary tasks and training them jointly with the primary tasks. To effectively leverage auxiliary genes, we must select a subset of auxiliary genes that positively influence the prediction of the target genes. However, this is a challenging optimization problem due to the vast number of possible combinations. To overcome this challenge, we propose Prior-Knowledge-Based Differentiable Top-$k$ Gene Selection via Bi-level Optimization (DkGSB), a method that ranks genes by leveraging prior knowledge and relaxes the combinatorial selection problem into a differentiable top-$k$ selection problem. The experiments confirm the effectiveness of incorporating auxiliary genes and show that the proposed method outperforms conventional auxiliary task learning approaches.",Bioinformatics,http://arxiv.org/abs/2511.18336v1,arXiv,1
"Single-cell RNA sequencing (scRNA-seq) enables transcriptomic profiling at cellular resolution but suffers from pervasive dropout events that obscure biological signals. We present SCR-MF, a modular two-stage workflow that combines principled dropout detection using scRecover with robust non-parametric imputation via missForest. Across public and simulated datasets, SCR-MF achieves robust and interpretable performance comparable to or exceeding existing imputation methods in most cases, while preserving biological fidelity and transparency. Runtime analysis demonstrates that SCR-MF provides a competitive balance between accuracy and computational efficiency, making it suitable for mid-scale single-cell datasets.",Bioinformatics,http://arxiv.org/abs/2511.16923v1,arXiv,1
"Large-scale genomic workflows used in precision medicine can process datasets spanning tens to hundreds of gigabytes per sample, leading to high memory spikes, intensive disk I/O, and task failures due to out-of-memory errors. Simple static resource allocation methods struggle to handle the variability in per-chromosome RAM demands, resulting in poor resource utilization and long runtimes. In this work, we propose multiple mechanisms for adaptive, RAM-efficient parallelization of chromosome-level bioinformatics workflows. First, we develop a symbolic regression model that estimates per-chromosome memory consumption for a given task and introduces an interpolating bias to conservatively minimize over-allocation. Second, we present a dynamic scheduler that adaptively predicts RAM usage with a polynomial regression model, treating task packing as a Knapsack problem to optimally batch jobs based on predicted memory requirements. Additionally, we present a static scheduler that optimizes chromosome processing order to minimize peak memory while preserving throughput. Our proposed methods, evaluated on simulations and real-world genomic pipelines, provide new mechanisms to reduce memory overruns and balance load across threads. We thereby achieve faster end-to-end execution, showcasing the potential to optimize large-scale genomic workflows.",Bioinformatics,http://arxiv.org/abs/2511.15977v1,arXiv,1
"Biological data sets are often high-dimensional, noisy, and governed by complex interactions among sparse signals. This poses major challenges for interpretability and reliable feature selection. Tasks such as identifying motif interactions in genomics exemplify these difficulties, as only a small subset of biologically relevant features (e.g., motifs) are typically active, and their effects are often non-linear and context-dependent. While statistical approaches often result in more interpretable models, deep learning models have proven effective in modeling complex interactions and prediction accuracy, yet their black-box nature limits interpretability. We introduce BaGGLS, a flexible and interpretable probabilistic binary regression model designed for high-dimensional biological inference involving feature interactions. BaGGLS incorporates a Bayesian group global-local shrinkage prior, aligned with the group structure introduced by interaction terms. This prior encourages sparsity while retaining interpretability, helping to isolate meaningful signals and suppress noise. To enable scalable inference, we employ a partially factorized variational approximation that captures posterior skewness and supports efficient learning even in large feature spaces. In extensive simulations, we can show that BaGGLS outperforms the other methods with regard to interaction detection and is many times faster than MCMC sampling under the horseshoe prior. We also demonstrate the usefulness of BaGGLS in the context of interaction discovery from motif scanner outputs and noisy attribution scores from deep learning models. This shows that BaGGLS is a promising approach for uncovering biologically relevant interaction patterns, with potential applicability across a range of high-dimensional tasks in computational biology.",Bioinformatics,http://arxiv.org/abs/2511.15330v1,arXiv,1
"Spatial Transcriptomics enables mapping of gene expression within its native tissue context, but current platforms measure only a limited set of genes due to experimental constraints and excessive costs. To overcome this, computational models integrate Single-Cell RNA Sequencing data with Spatial Transcriptomics to predict unmeasured genes. We propose CASPER, a cross-attention based framework that predicts unmeasured gene expression in Spatial Transcriptomics by leveraging centroid-level representations from Single-Cell RNA Sequencing. We performed rigorous testing over four state-of-the-art Spatial Transcriptomics/Single-Cell RNA Sequencing dataset pairs across four existing baseline models. CASPER shows significant improvement in nine out of the twelve metrics for our experiments. This work paves the way for further work in Spatial Transcriptomics to Single-Cell RNA Sequencing modality translation. The code for CASPER is available at https://github.com/AI4Med-Lab/CASPER.",Bioinformatics,http://arxiv.org/abs/2511.15139v1,arXiv,1
"Precise prognostic stratification of colorectal cancer (CRC) remains a major clinical challenge due to its high heterogeneity. The conventional TNM staging system is inadequate for personalized medicine. We aimed to develop and validate a novel multiple instance learning model TDAM-CRC using histopathological whole-slide images for accurate prognostic prediction and to uncover its underlying molecular mechanisms. We trained the model on the TCGA discovery cohort (n=581), validated it in an independent external cohort (n=1031), and further we integrated multi-omics data to improve model interpretability and identify novel prognostic biomarkers. The results demonstrated that the TDAM-CRC achieved robust risk stratification in both cohorts. Its predictive performance significantly outperformed the conventional clinical staging system and multiple state-of-the-art models. The TDAM-CRC risk score was confirmed as an independent prognostic factor in multivariable analysis. Multi-omics analysis revealed that the high-risk subtype is closely associated with metabolic reprogramming and an immunosuppressive tumor microenvironment. Through interaction network analysis, we identified and validated Mitochondrial Ribosomal Protein L37 (MRPL37) as a key hub gene linking deep pathomic features to clinical prognosis. We found that high expression of MRPL37, driven by promoter hypomethylation, serves as an independent biomarker of favorable prognosis. Finally, we constructed a nomogram incorporating the TDAM-CRC risk score and clinical factors to provide a precise and interpretable clinical decision-making tool for CRC patients. Our AI-driven pathological model TDAM-CRC provides a robust tool for improved CRC risk stratification, reveals new molecular targets, and facilitates personalized clinical decision-making.",Bioinformatics,http://arxiv.org/abs/2511.15067v1,arXiv,1
"Trained on massive cross-species DNA corpora, DNA large language models (LLMs) learn the fundamental ""grammar"" and evolutionary patterns of genomic sequences. This makes them powerful priors for DNA sequence modeling, particularly over long ranges. However, two major constraints hinder their use in practice: the quadratic computational cost of self-attention and the growing memory required for key-value (KV) caches during autoregressive decoding. These constraints force the use of heuristics such as fixed-window truncation or sliding windows, which compromise fidelity on ultra-long sequences by discarding distant information. We introduce FOCUS (Feature-Oriented Compression for Ultra-long Self-attention), a progressive context-compression module that can be plugged into pretrained DNA LLMs. FOCUS combines the established k-mer representation in genomics with learnable hierarchical compression: it inserts summary tokens at k-mer granularity and progressively compresses attention key and value activations across multiple Transformer layers, retaining only the summary KV states across windows while discarding ordinary-token KV. A shared-boundary windowing scheme yields a stationary cross-window interface that propagates long-range information with minimal loss. We validate FOCUS on an Evo-2-based DNA LLM fine-tuned on GRCh38 chromosome 1 with self-supervised training and randomized compression schedules to promote robustness across compression ratios. On held-out human chromosomes, FOCUS achieves near-lossless fidelity: compressing a 1 kb context into only 10 summary tokens (about 100x) shifts the average per-nucleotide probability by only about 0.0004. Compared to a baseline without compression, FOCUS reduces KV-cache memory and converts effective inference scaling from O(N^2) to near-linear O(N), enabling about 100x longer inference windows on commodity GPUs with near-lossless fidelity.",Bioinformatics,http://arxiv.org/abs/2511.14694v1,arXiv,1
"Modeling genomic sequences faces two unsolved challenges: the information density varies widely across different regions, while there is no clearly defined minimum vocabulary unit. Relying on either four primitive bases or independently designed DNA tokenizers, existing approaches with naive masked language modeling pre-training often fail to adapt to the varying complexities of genomic sequences. Leveraging Token Merging techniques, this paper introduces a hierarchical architecture that jointly optimizes a dynamic genomic tokenizer and latent Transformers with context-aware pre-training tasks. As for network structures, the tokenization module automatically chunks adjacent bases into words by stacking multiple layers of the differentiable token merging blocks with local-window constraints, then a Latent Encoder captures the global context of these merged words by full-attention blocks. Symmetrically employing a Latent Decoder and a Local Decoder, MergeDNA learns with two pre-training tasks: Merged Token Reconstruction simultaneously trains the dynamic tokenization module and adaptively filters important tokens, while Adaptive Masked Token Modeling learns to predict these filtered tokens to capture informative contents. Extensive experiments show that MergeDNA achieves superior performance on three popular DNA benchmarks and several multi-omics tasks with fine-tuning or zero-shot evaluation, outperforming typical tokenization methods and large-scale DNA foundation models.",Bioinformatics,http://arxiv.org/abs/2511.14806v1,arXiv,1
"Unsupervised learning on high-dimensional RNA-seq data can reveal molecular subtypes beyond standard labels. We combine an autoencoder-based representation with clustering and stability analysis to search for rare but reproducible genomic subtypes. On the UCI ""Gene Expression Cancer RNA-Seq"" dataset (801 samples, 20,531 genes; BRCA, COAD, KIRC, LUAD, PRAD), a pan-cancer analysis shows clusters aligning almost perfectly with tissue of origin (Cramer's V = 0.887), serving as a negative control. We therefore reframe the problem within KIRC (n = 146): we select the top 2,000 highly variable genes, standardize them, train a feed-forward autoencoder (128-dimensional latent space), and run k-means for k = 2-10. While global indices favor small k, scanning k with a pre-specified discovery rule (rare < 10 percent and stable with Jaccard >= 0.60 across 20 seeds after Hungarian alignment) yields a simple solution at k = 5 (silhouette = 0.129, DBI = 2.045) with a rare cluster C0 (6.85 percent of patients) that is highly stable (Jaccard = 0.787). Cluster-vs-rest differential expression (Welch's t-test, Benjamini-Hochberg FDR) identifies coherent markers. Overall, pan-cancer clustering is dominated by tissue of origin, whereas a stability-aware within-cancer approach reveals a rare, reproducible KIRC subtype.",Bioinformatics,http://arxiv.org/abs/2511.13705v1,arXiv,1
"In-context learning (ICL) -- the capacity of a model to infer and apply abstract patterns from examples provided within its input -- has been extensively studied in large language models trained for next-token prediction on human text. In fact, prior work often attributes this emergent behavior to distinctive statistical properties in human language. This raises a fundamental question: can ICL arise organically in other sequence domains purely through large-scale predictive training?   To explore this, we turn to genomic sequences, an alternative symbolic domain rich in statistical structure. Specifically, we study the Evo2 genomic model, trained predominantly on next-nucleotide (A/T/C/G) prediction, at a scale comparable to mid-sized LLMs. We develop a controlled experimental framework comprising symbolic reasoning tasks instantiated in both linguistic and genomic forms, enabling direct comparison of ICL across genomic and linguistic models. Our results show that genomic models, like their linguistic counterparts, exhibit log-linear gains in pattern induction as the number of in-context demonstrations increases. To the best of our knowledge, this is the first evidence of organically emergent ICL in genomic sequences, supporting the hypothesis that ICL arises as a consequence of large-scale predictive modeling over rich data. These findings extend emergent meta-learning beyond language, pointing toward a unified, modality-agnostic view of in-context learning.",Bioinformatics,http://arxiv.org/abs/2511.12797v2,arXiv,1
"Single-cell RNA sequencing (scRNA-seq), especially temporally resolved datasets, enables genome-wide profiling of gene expression dynamics at single-cell resolution across discrete time points. However, current technologies provide only sparse, static snapshots of cell states and are inherently influenced by technical noise, complicating the inference and representation of continuous transcriptional dynamics. Although embedding methods can reduce dimensionality and mitigate technical noise, the majority of existing approaches typically treat trajectory inference separately from embedding construction, often neglecting temporal structure. To address this challenge, here we introduce CellStream, a novel deep learning framework that jointly learns embedding and cellular dynamics from single-cell snapshot data by integrating an autoencoder with unbalanced dynamical optimal transport. Compared to existing methods, CellStream generates dynamics-informed embeddings that robustly capture temporal developmental processes while maintaining high consistency with the underlying data manifold. We demonstrate CellStream's effectiveness on both simulated datasets and real scRNA-seq data, including spatial transcriptomics. Our experiments indicate significant quantitative improvements over state-of-the-art methods in representing cellular trajectories with enhanced temporal coherence and reduced noise sensitivity. Overall, CellStream provides a new tool for learning and representing continuous streams from the noisy, static snapshots of single-cell gene expression.",Bioinformatics,http://arxiv.org/abs/2511.13786v1,arXiv,1
"Efficient and consistent string processing is critical in the exponentially growing genomic data era. Locally Consistent Parsing (LCP) addresses this need by partitioning an input genome string into short, exactly matching substrings (e.g., ""cores""), ensuring consistency across partitions. Labeling the cores of an input string consistently not only provides a compact representation of the input but also enables the reapplication of LCP to refine the cores over multiple iterations, providing a progressively longer and more informative set of substrings for downstream analyses.   We present the first iterative implementation of LCP with Lcptools and demonstrate its effectiveness in identifying cores with minimal collisions. Experimental results show that the number of cores at the i^th iteration is O(n/c^i) for c ~ 2.34, while the average length and the average distance between consecutive cores are O(c^i). Compared to the popular sketching techniques, LCP produces significantly fewer cores, enabling a more compact representation and faster analyses. To demonstrate the advantages of LCP in genomic string processing in terms of computation and memory efficiency, we also introduce LCPan, an efficient variation graph constructor. We show that LCPan generates variation graphs >10x faster than vg, while using >13x less memory.",Bioinformatics,http://arxiv.org/abs/2511.12205v2,arXiv,1
"Classes, as fundamental elements of Computer Vision, have been extensively studied within incremental learning frameworks. In contrast, tokens, which play essential roles in many research fields, exhibit similar characteristics of growth, yet investigations into their incremental learning remain significantly scarce. This research gap primarily stems from the holistic nature of tokens in language, which imposes significant challenges on the design of incremental learning frameworks for them. To overcome this obstacle, in this work, we turn to a type of token, gene, for a large-scale biological dataset--single-cell transcriptomics--to formulate a pipeline for gene incremental learning and establish corresponding evaluations. We found that the forgetting problem also exists in gene incremental learning, thus we adapted existing class incremental learning methods to mitigate the forgetting of genes. Through extensive experiments, we demonstrated the soundness of our framework design and evaluations, as well as the effectiveness of our method adaptations. Finally, we provide a complete benchmark for gene incremental learning in single-cell transcriptomics.",Bioinformatics,http://arxiv.org/abs/2511.13762v1,arXiv,1
"Single-cell data analysis seeks to characterize cellular heterogeneity based on high-dimensional gene expression profiles. Conventional approaches represent each cell as a vector in Euclidean space, which limits their ability to capture intrinsic correlations and multiscale geometric structures. We propose a multiscale framework based on Grassmann manifolds that integrates machine learning with subspace geometry for single-cell data analysis. By generating embeddings under multiple representation scales, the framework combines their features from different geometric views into a unified Grassmann manifold. A power-based scale sampling function is introduced to control the selection of scales and balance in- formation across resolutions. Experiments on nine benchmark single-cell RNA-seq datasets demonstrate that the proposed approach effectively preserves meaningful structures and provides stable clustering performance, particularly for small to medium-sized datasets. These results suggest that Grassmann manifolds offer a coherent and informative foundation for analyzing single cell data.",Bioinformatics,http://arxiv.org/abs/2511.11717v1,arXiv,1
"The identification of disease-gene associations is instrumental in understanding the mechanisms of diseases and developing novel treatments. Besides identifying genes from RNA-Seq datasets, it is often necessary to identify gene clusters that have relationships with a disease. In this work, we propose a graph-based method for using an RNA-Seq dataset with known genes related to a disease and perform a robust clustering analysis to identify clusters of genes. Our method involves the construction of a gene co-expression network, followed by the computation of gene embeddings leveraging Node2Vec+, an algorithm applying weighted biased random walks and skipgram with negative sampling to compute node embeddings from undirected graphs with weighted edges. Finally, we perform spectral clustering to identify clusters of genes. All processes in our entire method are jointly optimized for stability, robustness, and optimality by applying Tree-structured Parzen Estimator. Our method was applied to an RNA-Seq dataset of known genes that have associations with Age-related Macular Degeneration (AMD). We also performed tests to validate and verify the robustness and statistical significance of our methods due to the stochastic nature of the involved processes. Our results show that our method is capable of generating consistent and robust clustering results. Our method can be seamlessly applied to other RNA-Seq datasets due to our process of joint optimization, ensuring the stability and optimality of the several steps in our method, including the construction of a gene co-expression network, computation of gene embeddings, and clustering of genes. Our work will aid in the discovery of natural structures in the RNA-Seq data, and understanding gene regulation and gene functions not just for AMD but for any disease in general.",Bioinformatics,http://arxiv.org/abs/2511.09590v1,arXiv,1
"Whole-genome sequencing (WGS) has revealed numerous non-coding short variants whose functional impacts remain poorly understood. Despite recent advances in deep-learning genomic approaches, accurately predicting and prioritizing clinically relevant mutations in gene regulatory regions remains a major challenge. Here we introduce Deep VRegulome, a deep-learning method for prediction and interpretation of functionally disruptive variants in the human regulome, which combines 700 DNABERT fine-tuned models, trained on vast amounts of ENCODE gene regulatory regions, with variant scoring, motif analysis, attention-based visualization, and survival analysis. We showcase its application on TCGA glioblastoma WGS dataset in prioritizing survival-associated mutations and regulatory regions. The analysis identified 572 splice-disrupting and 9,837 transcription-factor binding site altering mutations occurring in greater than 10% of glioblastoma samples. Survival analysis linked 1352 mutations and 563 disrupted regulatory regions to patient outcomes, enabling stratification via non-coding mutation signatures. All the code, fine-tuned models, and an interactive data portal are publicly available.",Bioinformatics,http://arxiv.org/abs/2511.09026v1,arXiv,1
"Accurate cell type annotation across datasets is a key challenge in single-cell analysis. snRNA-seq enables profiling of frozen or difficult-to-dissociate tissues, complementing scRNA-seq by capturing fragile or rare cell types. However, cross-annotation between these two datasets remains largely unexplored, as existing methods treat them independently. We introduce ScNucAdapt, the first method designed for cross-annotation between scRNA-seq and snRNA-seq datasets. To address distributional and cell composition differences, ScNucAdapt employs partial domain adaptation. Experiments across diverse samples show that ScNucAdapt achieves robust and accurate cell type annotation, outperforming existing approaches. Therefore, ScNucAdapt provides a practical framework for the cross-domain cell type annotation between scRNA-seq and snRNA-seq data.",Bioinformatics,http://arxiv.org/abs/2511.08996v2,arXiv,1
"Detecting chemical modifications on RNA molecules remains a key challenge in epitranscriptomics. Traditional reverse transcription-based sequencing methods introduce enzyme- and sequence-dependent biases and fragment RNA molecules, confounding the accurate mapping of modifications across the transcriptome. Nanopore direct RNA sequencing offers a powerful alternative by preserving native RNA molecules, enabling the detection of modifications at single-molecule resolution. However, current computational tools can identify only a limited subset of modification types within well-characterized sequence contexts for which ample training data exists. Here, we introduce a model-free computational method that reframes modification detection as an anomaly detection problem, requiring only canonical (unmodified) RNA reads without any other annotated data. For each nanopore read, our approach extracts robust, modification-sensitive features from the raw ionic current signal at a site using the signature transform, then computes an anomaly score by comparing the resulting feature vector to its nearest neighbors in an unmodified reference dataset. We convert anomaly scores into statistical p-values to enable anomaly detection at both individual read and site levels. Validation on densely-modified \textit{E. coli} rRNA demonstrates that our approach detects known sites harboring diverse modification types, without prior training on these modifications. We further applyied this framework to dengue virus (DENV) transcripts and mammalian mRNAs. For DENV sfRNA, it led to revealing a novel 2'-O-methylated site, which we validate orthogonally by qRT-PCR assays. These results demonstrate that our model-free approach operates robustly across different types of RNAs and datasets generated with different nanopore sequencing chemistries.",Bioinformatics,http://arxiv.org/abs/2511.08855v1,arXiv,1
"Biological age, which may be older or younger than chronological age due to factors such as genetic predisposition, environmental exposures, serves as a meaningful biomarker of aging processes and can inform risk stratification, treatment planning, and survivorship care in cancer patients. We propose EpiCAge, a multimodal framework that integrates epigenetic and phenotypic data to improve biological age prediction. Evaluated on eight internal and four external cancer cohorts, EpiCAge consistently outperforms existing epigenetic and phenotypic age clocks. Our analyses show that EpiCAge identifies biologically relevant markers, and its derived age acceleration is significantly associated with mortality risk. These results highlight EpiCAge as a promising multimodal machine learning tool for biological age assessment in oncology.",Bioinformatics,http://arxiv.org/abs/2511.07219v1,arXiv,1
"Alternative splicing creates complex bubbles in splicing graphs where more than two transcript paths compete, challenging methods designed for simple binary events. We present a unified framework that compares paths using distinct exonic parts observed directly from reads. We build a GrASE splicing graph (DAG) per gene, enumerate bubbles, and quantify shared and distinct exonic parts across three comparison structures. (i) all-pairwise contrasts (ii) a multinomial n-way comparison and (iii) valid bipartitions of paths. For (iii) we introduce lower-set bipartitioning, which respects subset relations among paths by enumerating downward-closed sets in a containment graph, yielding valid two-group splits with nonempty distinguishing parts. Our test statistic is the fraction of reads mapped to distinct parts relative to distinct + shared parts, enabling differential usage across samples. Applied to genome annotations, the approach examines more bubbles than prior tools while remaining tractable and interpretable.",Bioinformatics,http://arxiv.org/abs/2511.05992v1,arXiv,1
"The discovery of genetic risk factors has transformed human genetics, yet the pace of new gene identification has slowed despite the exponential expansion of sequencing and biobank resources. Current approaches are optimized for the extremes of the allele frequency spectrum: rare, high-penetrance variants identified through burden testing, and common, low-effect variants mapped by genome-wide association studies. Between these extremes lies variants of intermediate frequency and effect size where statistical power is limited, pathogenicity is often misclassified, and gene discovery lags behind empirical evidence of heritable contribution. This 'missing middle' represents a critical blind spot across disease areas, from neurodevelopmental and psychiatric disorders to cancer and aging. In this review, we organize strategies for risk gene identification by variant frequency class, highlighting methodological strengths and constraints at each scale. We draw on lessons across fields to illustrate how innovations in variant annotation, joint modeling, phenotype refinement, and network-based inference can extend discovery into the intermediate range. By framing the frequency spectrum as a unifying axis, we provide a conceptual map of current capabilities, their limitations, and emerging directions toward more comprehensive risk gene discovery.",Bioinformatics,http://arxiv.org/abs/2511.04637v2,arXiv,1
"Since its emergence, SARS-CoV-2 has demonstrated a rapid and unpredictable evolutionary trajectory, characterized by the continual emergence of immune-evasive variants. This poses persistent challenges to public health and vaccine development.   While large-scale generative pre-trained transformers (GPTs) have revolutionized the modeling of sequential data, their direct applications to noisy viral genomic sequences are limited. In this paper, we introduce PETRA(Pretrained Evolutionary TRAnsformer), a novel transformer approach based on evolutionary trajectories derived from phylogenetic trees rather than raw RNA sequences. This method effectively mitigates sequencing noise and captures the hierarchical structure of viral evolution.   With a weighted training framework to address substantial geographical and temporal imbalances in global sequence data, PETRA excels in predicting future SARS-CoV-2 mutations, achieving a weighted recall@1 of 9.45% for nucleotide mutations and 17.10\% for spike amino-acid mutations, compared to 0.49% and 6.64% respectively for the best baseline. PETRA also demonstrates its ability to aid in the real-time mutation prediction of major clades like 24F(XEC) and 25A(LP.8.1). The code is open sourced on https://github.com/xz-keg/PETra",Bioinformatics,http://arxiv.org/abs/2511.03976v1,arXiv,1
"Computational modeling of single-cell gene expression is crucial for understanding cellular processes, but generating realistic expression profiles remains a major challenge. This difficulty arises from the count nature of gene expression data and complex latent dependencies among genes. Existing generative models often impose artificial gene orderings or rely on shallow neural network architectures. We introduce a scalable latent diffusion model for single-cell gene expression data, which we refer to as scLDM, that respects the fundamental exchangeability property of the data. Our VAE uses fixed-size latent variables leveraging a unified Multi-head Cross-Attention Block (MCAB) architecture, which serves dual roles: permutation-invariant pooling in the encoder and permutation-equivariant unpooling in the decoder. We enhance this framework by replacing the Gaussian prior with a latent diffusion model using Diffusion Transformers and linear interpolants, enabling high-quality generation with multi-conditional classifier-free guidance. We show its superior performance in a variety of experiments for both observational and perturbational single-cell data, as well as downstream tasks like cell-level classification.",Bioinformatics,http://arxiv.org/abs/2511.02986v1,arXiv,1
"In clinical proteomics, available input is often limited. In addition, phospho-proteomics is of particular interest since the dysregulation of these post-translational modifications (PTMs) has been implicated in various diseases such as cancer. We therefore assessed the feasibility of low input phospho-proteomics via phospho-bulk titration and low-input starting material. We found that there was identification of more phospho-peptides through phospho-bulk titration because of sample loss during preparation of low input starting material. Additionally, we explored various lysis buffers and boiling times for efficiency of decrosslinking formalin-fixed cells since cells and tissues are often fixed for preservation and sorting via FACS. We found that boiling in 0.05M Tris pH 7.6 with 5% SDS for 60 min yielded the highest number of phospho-peptides. Lastly, we applied Evotips Pure and phospho-bulk titration to treated Jurkat cells and identified 7 phospho-sites involved in T-cell stimulation.",Bioinformatics,http://arxiv.org/abs/2511.02932v2,arXiv,1
"Nucleotide sequence variation can induce significant shifts in functional fitness. Recent nucleotide foundation models promise to predict such fitness effects directly from sequence, yet heterogeneous datasets and inconsistent preprocessing make it difficult to compare methods fairly across DNA and RNA families. Here we introduce NABench, a large-scale, systematic benchmark for nucleic acid fitness prediction. NABench aggregates 162 high-throughput assays and curates 2.6 million mutated sequences spanning diverse DNA and RNA families, with standardized splits and rich metadata. We show that NABench surpasses prior nucleotide fitness benchmarks in scale, diversity, and data quality. Under a unified evaluation suite, we rigorously assess 29 representative foundation models across zero-shot, few-shot prediction, transfer learning, and supervised settings. The results quantify performance heterogeneity across tasks and nucleic-acid types, demonstrating clear strengths and failure modes for different modeling choices and establishing strong, reproducible baselines. We release NABench to advance nucleic acid modeling, supporting downstream applications in RNA/DNA design, synthetic biology, and biochemistry. Our code is available at https://github.com/mrzzmrzz/NABench.",Bioinformatics,http://arxiv.org/abs/2511.02888v1,arXiv,1
"Diagnosing rare diseases requires linking gene findings with often unstructured reference text. Current pipelines collect many candidate genes, but clinicians still spend a lot of time filtering false positives and combining evidence from papers and databases. A key challenge is language: phenotype descriptions and inheritance patterns are written in prose, not fully captured by tables. Large language models (LLMs) can read such text, but clinical use needs grounding in citable knowledge and stable, repeatable behavior. We explore a knowledge-grounded and language-aware reranking layer on top of a high-recall first-stage pipeline. The goal is to improve precision and explainability, not to replace standard bioinformatics steps. We use expert-built context and a consensus method to reduce LLM variability, producing shorter, better-justified gene lists for expert review. LA-MARRVEL achieves the highest accuracy, outperforming other methods -- including traditional bioinformatics diagnostic tools (AI-MARRVEL, Exomiser, LIRICAL) and naive large language models (e.g., Anthropic Claude) -- with an average Recall@5 of 94.10%, a +3.65 percentage-point improvement over AI-MARRVEL. The LLM-generated reasoning provides clear prose on phenotype matching and inheritance patterns, making clinical review faster and easier. LA-MARRVEL has three parts: expert-engineered context that enriches phenotype and disease information; a ranked voting algorithm that combines multiple LLM runs to choose a consensus ranked gene list; and the AI-MARRVEL pipeline that provides first-stage ranks and gene annotations, already known as a state-of-the-art method in Rare Disease Diagnosis on BG, DDD, and UDN cohorts. The online AI-MARRVEL includes LA-MARRVEL as an LLM feature at https://ai.marrvel.org . We evaluate LA-MARRVEL on three datasets from independent cohorts of real-world diagnosed patients.",Bioinformatics,http://arxiv.org/abs/2511.02263v3,arXiv,1
"DNA methylation can be associated with phenotypic plasticity, yet how temperature shapes DNA methylation diversity in natural populations is unclear. Analyzing whole-genome bisulfite sequencing from 1075 Arabidopsis thaliana accessions grown at 10Â°C, 16Â°C, and 22Â°C, we quantified single-cytosine diversity using Jensen-Shannon Divergence (JSD). Diversity consistently peaked at intermediate methylation levels across the CpG, CHG, and CHH sequence contexts. Temperature modulated this diversity, primarily impacting intermediately methylated sites, with non-CG contexts (CHG and CHH) exhibiting increased diversity at warmer temperatures. Notably, at 22Â°C, CHH diversity patterns indicated altered balance between the RdDM and CMT2 pathways that regulate specific transposable element (TE) superfamilies. Furthermore, accessions from Southern Europe displayed higher non-CG diversity at 22Â°C compared to Northern European accessions. Our findings reveal that temperature influences the epigenomic diversity landscape, highlighting context-dependent plasticity, a dynamic interplay between silencing pathways, and potential geographic adaptation in response to environmental cues.",Bioinformatics,http://arxiv.org/abs/2511.01834v1,arXiv,1
"Introduction: Epigenomic datasets from high-throughput sequencing experiments are commonly summarized as genomic intervals. As the volume of this data grows, so does interest in analyzing it through deep learning. However, the heterogeneity of genomic interval data, where each dataset defines its own regions, creates barriers for machine learning methods that require consistent, discrete vocabularies. Methods: We introduce gtars-tokenizers, a high-performance library that maps genomic intervals to a predefined universe or vocabulary of regions, analogous to text tokenization in natural language processing. Built in Rust with bindings for Python, R, CLI, and WebAssembly, gtars-tokenizers implements two overlap methods (BITS and AIList) and integrates seamlessly with modern ML frameworks through Hugging Face-compatible APIs. Results: The gtars-tokenizers package achieves top efficiency for large-scale datasets, while enabling genomic intervals to be processed using standard ML workflows in PyTorch and TensorFlow without ad hoc preprocessing. This token-based approach bridges genomics and machine learning, supporting scalable and standardized analysis of interval data across diverse computational environments. Availability: PyPI and GitHub: https://github.com/databio/gtars.",Bioinformatics,http://arxiv.org/abs/2511.01555v1,arXiv,1
"Motivation: Advances in high-throughput chromatin conformation capture have provided insight into the three-dimensional structure and organization of chromatin. While bulk Hi-C experiments capture spatio-temporally averaged chromatin interactions across millions of cells, single-cell Hi-C experiments report on the chromatin interactions of individual cells. Supervised and unsupervised algorithms have been developed to embed single-cell Hi-C maps and identify different cell types. However, single-cell Hi-C maps are often difficult to cluster due to their high sparsity, with state-of-the-art algorithms achieving a maximum Adjusted Rand Index (ARI) of only < 0.4 on several datasets while requiring labels for training.   Results: We introduce a novel unsupervised algorithm, Single-cell Clustering Using Diagonal Diffusion Operators (SCUDDO), to embed and cluster single-cell Hi-C maps. We evaluate SCUDDO on three previously difficult-to-cluster single-cell Hi-C datasets, and show that it can outperform other current algorithms in ARI by > 0.2. Further, SCUDDO outperforms all other tested algorithms even when we restrict the number of intrachromosomal maps for each cell type and when we use only a small fraction of contacts in each Hi-C map. Thus, SCUDDO can capture the underlying latent features of single-cell Hi-C maps and provide accurate labeling of cell types even when cell types are not known a priori.   Availability: SCUDDO is freely available at www.github.com/lmaisuradze/scuddo. The tested datasets are publicly available and can be downloaded from the Gene Expression Omnibus.",Bioinformatics,http://arxiv.org/abs/2511.00278v1,arXiv,1
"Summary: We present AmpliconHunter2 (AHv2), a highly scalable in silico PCR engine written in C that can handle degenerate primers and uses a highly accurate melting temperature model. AHv2 implements a bit-mask IUPAC matcher with AVX2 SIMD acceleration, supports user-specified mismatches and 3' clamp constraints, calls amplicons in all four primer pair orientations (FR/RF/FF/RR), and optionally trims primers and extracts fixed-length flanking barcodes into FASTA headers. The pipeline packs FASTA into 2-bit batches, streams them in 16 MB chunks, writes amplicons to per-thread temp files and concatenates outputs, minimizing peak RSS during amplicon finding. We also summarize updates to the Python reference (AHv1.1).   Availability and Implementation: AmpliconHunter2 is available as a freely available webserver at: https://ah2.uconn.engr.edu and source code is available at: https://github.com/rhowardstone/AmpliconHunter2 under an MIT license. AHv2 was implemented in C; AHv1.1 using Python 3 with Hyperscan.   Contact: rye.howard-stone@uconn.edu",Bioinformatics,http://arxiv.org/abs/2511.00170v1,arXiv,1
"Bulk tissue RNA sequencing of heterogeneous samples provides averaged gene expression profiles, obscuring cell type-specific dynamics. To address this, we present a probabilistic hierarchical Bayesian model that deconvolves bulk RNA-seq data into constituent cell-type expression profiles and proportions, leveraging a high-resolution single-cell reference. We apply our model to human endometrial tissue across the menstrual cycle, a context characterized by dramatic hormone-driven cellular composition changes. Our extended framework provides a principled inference of cell type proportions and cell-specific gene expression changes across cycle phases. We demonstrate the model's structure, priors, and inference strategy in detail, and we validate its performance with simulations and comparisons to existing methods. The results reveal dynamic shifts in epithelial, stromal, and immune cell fractions between menstrual phases, and identify cell-type-specific differential gene expression associated with endometrial function (e.g., decidualization markers in stromal cells during the secretory phase). We further conduct robustness tests and show that our Bayesian approach is resilient to reference mismatches and noise. Finally, we discuss the biological significance of our findings, potential clinical implications for fertility and endometrial disorders, and future directions, including integration of spatial transcriptomics.",Bioinformatics,http://arxiv.org/abs/2510.27097v2,arXiv,1
"Single-cell RNA-seq foundation models achieve strong performance on downstream tasks but remain black boxes, limiting their utility for biological discovery. Recent work has shown that sparse dictionary learning can extract concepts from deep learning models, with promising applications in biomedical imaging and protein models. However, interpreting biological concepts remains challenging, as biological sequences are not inherently human-interpretable. We introduce a novel concept-based interpretability framework for single-cell RNA-seq models with a focus on concept interpretation and evaluation. We propose an attribution method with counterfactual perturbations that identifies genes that influence concept activation, moving beyond correlational approaches like differential expression analysis. We then provide two complementary interpretation approaches: an expert-driven analysis facilitated by an interactive interface and an ontology-driven method with attribution-based biological pathway enrichment. Applying our framework to two well-known single-cell RNA-seq models from the literature, we interpret concepts extracted by Top-K Sparse Auto-Encoders trained on two immune cell datasets. With a domain expert in immunology, we show that concepts improve interpretability compared to individual neurons while preserving the richness and informativeness of the latent representations. This work provides a principled framework for interpreting what biological knowledge foundation models have encoded, paving the way for their use for hypothesis generation and discovery.",Bioinformatics,http://arxiv.org/abs/2510.25807v1,arXiv,1
"Advances in single-cell sequencing have enabled high-resolution profiling of diverse molecular modalities, while integrating unpaired multi-omics single-cell data remains challenging. Existing approaches either rely on pair information or prior correspondences, or require computing a global pairwise coupling matrix, limiting their scalability and flexibility. In this paper, we introduce a scalable and flexible generative framework called single-cell Multi-omics Regularized Disentangled Representations (scMRDR) for unpaired multi-omics integration. Specifically, we disentangle each cell's latent representations into modality-shared and modality-specific components using a well-designed $Î²$-VAE architecture, which are augmented with isometric regularization to preserve intra-omics biological heterogeneity, adversarial objective to encourage cross-modal alignment, and masked reconstruction loss strategy to address the issue of missing features across modalities. Our method achieves excellent performance on benchmark datasets in terms of batch correction, modality alignment, and biological signal preservation. Crucially, it scales effectively to large-level datasets and supports integration of more than two omics, offering a powerful and flexible solution for large-scale multi-omics data integration and downstream biological discovery.",Bioinformatics,http://arxiv.org/abs/2510.24987v1,arXiv,1
"Analysis of genomics data is central to nearly all areas of modern biology. Despite significant progress in artificial intelligence (AI) and computational methods, these technologies require significant human oversight to generate novel and reliable biological insights. Consequently, the genomics community has developed a substantial number of diverse visualization approaches and a proliferation of tools that biologists rely on in their data analysis workflows. While there are a few commonly used visualization tools for genomics data, many tools target specific use cases for genomics data interpretation and offer only a limited, predefined set of visualization types. Moreover, static visualizations often fail to support exploratory analysis. Developing interactive visualizations and tools typically requires significant time and technical expertise, even when supported by modern LLM-powered coding assistants, and the resulting visualizations can be difficult to share among collaborators. We developed Gosling Designer, an all-in-one platform for editing, exploring, and sharing visualizations of genomics data. Gosling Designer addresses four key challenges observed in existing genomics visualization tools: (1) limited versatility, (2) difficulty of visualization authoring, (3) complexity of data management, and (4) barriers to sharing and collaboration.",Bioinformatics,http://arxiv.org/abs/2510.24888v2,arXiv,1
"Background: Patients carrying MEF2C haploinsufficiency develop a recognizable neurodevelopmental syndrome featuring intellectual disability, treatment-resistant seizures, and autism spectrum behaviors. While MEF2C's critical roles in cardiac development and neuronal function are well-established, its specific transcriptional operations within microglia (the brain's resident immune cells) have remained surprisingly undefined. This knowledge gap is particularly notable given that MEF2C syndrome patients consistently present with neurological symptoms while cardiac abnormalities are rarely observed.   Results: We used human iPSC-derived microglia with MEF2C knockout to perform integrated ChIP-seq and RNA-seq analyses. Our data demonstrate that MEF2C directly binds 1,258 genomic loci and regulates 755 differentially expressed genes (FDR < 0.05). Integration identified 69 high-confidence direct targets with statistically significant overlap (p = 8.87 x 10^-5). The most dramatic changes included ADAMDEC1, a microglia-enriched metalloprotease for extracellular matrix remodeling (log2FC = -4.76, adj. p = 3.30 x 10^-19), and CARD11, an NF-kappaB signaling component (log2FC = -5.16, adj. p = 5.95 x 10^-5). Pathway analysis revealed profound disruption of Fc-gamma receptor signaling (p = 3.11 x 10^-7), alongside widespread changes in immune response and synaptic organization pathways.   Conclusion: These findings establish MEF2C as a master transcriptional regulator coordinating both immune effector functions and synaptic interaction programs in microglia. The observed changes, particularly in Fc receptor signaling critical for synaptic pruning, likely underlie the neurological manifestations of MEF2C syndrome.   Keywords: MEF2C, microglia, ChIP-seq, RNA-seq, neurodevelopmental disorders",Bioinformatics,http://arxiv.org/abs/2510.25780v1,arXiv,1
"The identification of homologous gene families across multiple genomes is a central task in bacterial pangenomics traditionally requiring computationally demanding all-against-all comparisons. PanDelos addresses this challenge with an alignment-free and parameter-free approach based on k-mer profiles, combining high speed, ease of use, and competitive accuracy with state-of-the-art methods. However, the increasing availability of genomic data requires tools that can scale efficiently to larger datasets. To address this need, we present PanDelos-plus, a fully parallel, gene-centric redesign of PanDelos. The algorithm parallelizes the most computationally intensive phases (Best Hit detection and Bidirectional Best Hit extraction) through data decomposition and a thread pool strategy, while employing lightweight data structures to reduce memory usage. Benchmarks on synthetic datasets show that PanDelos-plus achieves up to 14x faster execution and reduces memory usage by up to 96%, while maintaining accuracy. These improvements enable population-scale comparative genomics to be performed on standard multicore workstations, making large-scale bacterial pangenome analysis accessible for routine use in everyday research.",Bioinformatics,http://arxiv.org/abs/2510.23679v1,arXiv,1
"Arsenic (As), a widespread environmental toxin, poses major health risks due to its inorganic forms (iAs), which are linked to cancer, cardiovascular disease, and endocrine disruption. Although its toxic effects have been extensively studied, the molecular mechanisms underlying arsenic-induced perturbations remain incompletely understood. This complexity arises from its ability to reprogram epigenetic landscapes, alter gene expression, and disrupt metabolic balance through interconnected regulatory networks. Existing studies often analyze epigenomic, transcriptomic, and metabolomic datasets independently, overlooking their interdependence. Here, we present a coupled matrix factorization (CMF) framework based on the PARAFAC2-AOADMM model for joint integration of DNA methylation (RRBS), RNA-seq, and metabolomics data from mouse embryonic stem cells (ESCs) and epiblast-like cells (EpiLCs) exposed to arsenic. By jointly decomposing multi-omics matrices, our approach identifies shared and dataset-specific components that capture coordinated molecular responses to arsenic exposure. This integrative methodology demonstrates the potential of CMF-based models in computational toxicology and offers a generalizable framework for dissecting complex multi-layered biological perturbations.",Bioinformatics,http://arxiv.org/abs/2510.19294v1,arXiv,1
"Motivation: Standard genome-wide association studies in cancer genomics rely on statistical significance with multiple testing correction, but systematically fail in underpowered cohorts. In TCGA breast cancer (n=967, 133 deaths), low event rates (13.8%) create severe power limitations, producing false negatives for known drivers and false positives for large passenger genes. Results: We developed a five-criteria computational framework integrating causal inference (inverse probability weighting, doubly robust estimation) with orthogonal biological validation (expression, mutation patterns, literature evidence). Applied to TCGA-BRCA mortality analysis, standard Cox+FDR detected zero genes at FDR<0.05, confirming complete failure in underpowered settings. Our framework correctly identified RYR2 -- a cardiac gene with no cancer function -- as a false positive despite nominal significance (p=0.024), while identifying KMT2C as a complex candidate requiring validation despite marginal significance (p=0.047, q=0.954). Power analysis revealed median power of 15.1% across genes, with KMT2C achieving only 29.8% power (HR=1.55), explaining borderline statistical significance despite strong biological evidence. The framework distinguished true signals from artifacts through mutation pattern analysis: RYR2 showed 29.8% silent mutations (passenger signature) with no hotspots, while KMT2C showed 6.7% silent mutations with 31.4% truncating variants (driver signature). This multi-evidence approach provides a template for analyzing underpowered cohorts, prioritizing biological interpretability over purely statistical significance.   Availability: All code and analysis pipelines available at github.com/akarlaraytu/causal-inference-for-cancer-genomics",Bioinformatics,http://arxiv.org/abs/2510.18571v1,arXiv,1
"Cancer exhibits diverse and complex phenotypes driven by multifaceted molecular interactions. Recent biomedical research has emphasized the comprehensive study of such diseases by integrating multi-omics datasets (genome, proteome, transcriptome, epigenome). This approach provides an efficient method for identifying genetic variants associated with cancer and offers a deeper understanding of how the disease develops and spreads. However, it is challenging to comprehend complex interactions among the features of multi-omics datasets compared to single omics. In this paper, we analyze lung cancer multi-omics datasets from The Cancer Genome Atlas (TCGA). Using four statistical methods, LIMMA, the T test, Canonical Correlation Analysis (CCA), and the Wilcoxon test, we identified differentially expressed genes across gene expression, DNA methylation, and miRNA expression data. We then integrated these multi-omics data using the Kernel Machine Regression (KMR) approach. Our findings reveal significant interactions among the three omics: gene expression, miRNA expression, and DNA methylation in lung cancer. From our data analysis, we identified 38 genes significantly associated with lung cancer. From our data analysis, we identified 38 genes significantly associated with lung cancer. Among these, eight genes of highest ranking (PDGFRB, PDGFRA, SNAI1, ID1, FGF11, TNXB, ITGB1, ZIC1) were highlighted by rigorous statistical analysis. Furthermore, in silico studies identified three top-ranked potential candidate drugs (Selinexor, Orapred, and Capmatinib) that could play a crucial role in the treatment of lung cancer. These proposed drugs are also supported by the findings of other independent studies, which underscore their potential efficacy in the fight against lung cancer.",Bioinformatics,http://arxiv.org/abs/2510.16093v1,arXiv,1
"Nanopore sequencing enables real-time long-read DNA sequencing with reads exceeding 10 kilobases, but inherent error rates of 12-15 percent present significant computational challenges for read alignment. The critical seed chaining step must connect exact k-mer matches between reads and reference genomes while filtering spurious matches, yet state-of-the-art methods rely on fixed gap penalty functions unable to adapt to varying genomic contexts including tandem repeats and structural variants. This paper presents RawHash3, a hybrid framework combining graph neural networks with classical dynamic programming for adaptive seed chaining that maintains real-time performance while providing statistical guarantees. We formalize seed chaining as graph learning where seeds constitute nodes with 12-dimensional feature vectors and edges encode 8-dimensional spatial relationships including gap consistency. Our architecture employs three-layer EdgeConv GNN with confidence-based method selection that dynamically switches between learned guidance and algorithmic fallback. Comprehensive evaluation on 1,000 synthetic nanopore reads with 5,200 test seeds demonstrates RawHash3 achieves 99.94 percent precision and 40.07 percent recall, representing statistically significant 25.0 percent relative improvement over baseline with p less than 0.001. The system maintains median inference latency of 1.59ms meeting real-time constraints, while demonstrating superior robustness with 100 percent success rate under 20 percent label corruption versus baseline degradation to 30.3 percent. Cross-validation confirms stability establishing graph neural networks as viable approach for production genomics pipelines.",Bioinformatics,http://arxiv.org/abs/2510.16013v3,arXiv,1
"Single-cell RNA sequencing (scRNA-seq) data simulation is limited by classical methods that rely on linear correlations, failing to capture the intrinsic, nonlinear dependencies. No existing simulator jointly models gene-gene and cell-cell interactions. We introduce qSimCells, a novel quantum computing-based simulator that employs entanglement to model intra- and inter-cellular interactions, generating realistic single-cell transcriptomes with cellular heterogeneity. The core innovation is a quantum kernel that uses a parameterized quantum circuit with CNOT gates to encode complex, nonlinear gene regulatory network (GRN) as well as cell-cell communication topologies with explicit causal directionality. The resulting synthetic data exhibits non-classical dependencies: standard correlation-based analyses (Pearson and Spearman) fail to recover the programmed causal pathways and instead report spurious associations driven by high baseline gene-expression probabilities. Furthermore, applying cell-cell communication detection to the simulated data validates the true mechanistic links, revealing a robust, up to 75-fold relative increase in inferred communication probability only when quantum entanglement is active. These results demonstrate that the quantum kernel is essential for producing high-fidelity ground-truth datasets and highlight the need for advanced inference techniques to capture the complex, non-classical dependencies inherent in gene regulation.",Bioinformatics,http://arxiv.org/abs/2510.12776v3,arXiv,1
"Large Language Models are increasingly popular in genomics due to their potential to decode complex biological sequences. Hence, researchers require a standardized benchmark to evaluate DNA Language Models (DNA LMs) capabilities. However, evaluating DNA LMs is a complex task that intersects genomic's domain-specific challenges and machine learning methodologies, where seemingly minor implementation details can significantly compromise benchmark validity. We demonstrate this through BEND (Benchmarking DNA Language Models), where hardware-dependent hyperparameters -- number of data loading workers and buffer sizes -- create spurious performance variations of up to 4% for identical models. The problem stems from inadequate data shuffling interacting with domain specific data characteristics. Experiments with three DNA language models (HyenaDNA, DNABERT-2, ResNet-LM) show these artifacts affect both absolute performance and relative model rankings. We propose a simple solution: pre-shuffling data before storage eliminates hardware dependencies while maintaining efficiency. This work highlights how standard ML practices can interact unexpectedly with domain-specific data characteristics, with broader implications for benchmark design in specialized domains.",Bioinformatics,http://arxiv.org/abs/2510.12617v2,arXiv,1
"Aging is a highly complex and heterogeneous process that progresses at different rates across individuals, making biological age (BA) a more accurate indicator of physiological decline than chronological age. While previous studies have built aging clocks using single-omics data, they often fail to capture the full molecular complexity of human aging. In this work, we leveraged the Human Phenotype Project, a large-scale cohort of 10,000 adults aged 40-70 years, with extensive longitudinal profiling that includes clinical, behavioral, environmental, and multi-omics datasets spanning transcriptomics, lipidomics, metabolomics, and the microbiome. By employing advanced machine learning frameworks capable of modeling nonlinear biological dynamics, we developed and rigorously validated a multi-omics aging clock that robustly predicts diverse health outcomes and future disease risk. Unsupervised clustering of the integrated molecular profiles from multi-omics uncovered distinct biological subtypes of aging, revealing striking heterogeneity in aging trajectories and pinpointing pathway-specific alterations associated with different aging patterns. These findings demonstrate the power of multi-omics integration to decode the molecular landscape of aging and lay the groundwork for personalized healthspan monitoring and precision strategies to prevent age-related diseases.",Bioinformatics,http://arxiv.org/abs/2510.12384v3,arXiv,1
"Genome-resolved metagenomics has contributed largely to discovering prokaryotic genomes. When applied to microscopic eukaryotes, challenges such as the high number of introns and repeat regions found in nuclear genomes have hampered the mining and discovery of novel protistan lineages. Organellar genomes are simpler, smaller, have higher abundance than their nuclear counterparts and contain valuable phylogenetic information, but are yet to be widely used to identify new protist lineages from metagenomes. Here we present ""ChloroScan"", a new bioinformatics pipeline to extract eukaryotic plastid genomes from metagenomes. It incorporates a deep learning contig classifier to identify putative plastid contigs and an automated binning module to recover bins with guidance from a curated marker gene database. Additionally, ChloroScan summarizes the results in different user-friendly formats, including annotated coding sequences and proteins for each bin. We show that ChloroScan recovers more high-quality plastid bins than MetaBAT2 for simulated metagenomes. The practical utility of ChloroScan is illustrated by recovering 16 medium to high-quality metagenome assembled genomes from four protist-size fractioned metagenomes, with several bins showing high taxonomic novelty.",Bioinformatics,http://arxiv.org/abs/2510.10950v1,arXiv,1
"Rare genetic disease diagnosis faces critical challenges: insufficient patient data, inaccessible full genome sequencing, and the immense number of possible causative genes. These limitations cause prolonged diagnostic journeys, inappropriate treatments, and critical delays, disproportionately affecting patients in resource-limited settings where diagnostic tools are scarce. We propose RareNet, a subgraph-based Graph Neural Network that requires only patient phenotypes to identify the most likely causal gene and retrieve focused patient subgraphs for targeted clinical investigation. RareNet can function as a standalone method or serve as a pre-processing or post-processing filter for other candidate gene prioritization methods, consistently enhancing their performance while potentially enabling explainable insights. Through comprehensive evaluation on two biomedical datasets, we demonstrate competitive and robust causal gene prediction and significant performance gains when integrated with other frameworks. By requiring only phenotypic data, which is readily available in any clinical setting, RareNet democratizes access to sophisticated genetic analysis, offering particular value for underserved populations lacking advanced genomic infrastructure.",Bioinformatics,http://arxiv.org/abs/2510.08655v1,arXiv,1
"Spatial variable genes (SVGs) reveal critical information about tissue architecture, cellular interactions, and disease microenvironments. As spatial transcriptomics (ST) technologies proliferate, accurately identifying SVGs across diverse platforms, tissue types, and disease contexts has become both a major opportunity and a significant computational challenge. Here, we present a comprehensive benchmarking study of 20 state-of-the-art SVG detection methods using human slides from STimage-1K4M, a large-scale resource of ST data comprising 662 slides from more than 18 tissue types. We evaluate each method across a range of biologically and technically meaningful criteria, including recovery of pathologist-annotated domain-specific markers, cross-slide reproducibility, scalability to high-resolution data, and robustness to technical variation. Our results reveal marked differences in performance depending on tissue type, spatial resolution, and study design. Beyond benchmarking, we construct the first cross-tissue atlas of SVGs, enabling comparative analysis of spatial gene programs across cancer and normal tissues. We observe similarities between pairs of tissues that reflect developmental and functional relationships, such as high overlap between thymus and lymph node, and uncover spatial gene programs associated with metastasis, immune infiltration, and tissue-of-origin identity in cancer. Together, our work defines a framework for evaluating and interpreting spatial gene expression and establishes a reference resource for the ST community.",Bioinformatics,http://arxiv.org/abs/2510.07653v2,arXiv,1
"Single nucleotide polymorphism (SNP) datasets are fundamental to genetic studies but pose significant privacy risks when shared. The correlation of SNPs with each other makes strong adversarial attacks such as masked-value reconstruction, kin, and membership inference attacks possible. Existing privacy-preserving approaches either apply differential privacy to statistical summaries of these datasets or offer complex methods that require post-processing and the usage of a publicly available dataset to suppress or selectively share SNPs.   In this study, we introduce an innovative framework for generating synthetic SNP sequence datasets using samples derived from time-inhomogeneous hidden Markov models (TIHMMs). To preserve the privacy of the training data, we ensure that each SNP sequence contributes only a bounded influence during training, enabling strong differential privacy guarantees. Crucially, by operating on full SNP sequences and bounding their gradient contributions, our method directly addresses the privacy risks introduced by their inherent correlations.   Through experiments conducted on the real-world 1000 Genomes dataset, we demonstrate the efficacy of our method using privacy budgets of $\varepsilon \in [1, 10]$ at $Î´=10^{-4}$. Notably, by allowing the transition models of the HMM to be dependent on the location in the sequence, we significantly enhance performance, enabling the synthetic datasets to closely replicate the statistical properties of non-private datasets. This framework facilitates the private sharing of genomic data while offering researchers exceptional flexibility and utility.",Bioinformatics,http://arxiv.org/abs/2510.05777v1,arXiv,1
"Identifying cancer driver genes (CDGs) is essential for understanding cancer mechanisms and developing targeted therapies. Graph neural networks (GNNs) have recently been employed to identify CDGs by capturing patterns in biological interaction networks. However, most GNN-based approaches rely on a single protein-protein interaction (PPI) network, ignoring complementary information from other biological networks. Some studies integrate multiple networks by aligning features with consistency constraints to learn unified gene representations for CDG identification. However, such representation-level fusion often assumes congruent gene relationships across networks, which may overlook network heterogeneity and introduce conflicting information. To address this, we propose Soft-Evidence Fusion Graph Neural Network (SEFGNN), a novel framework for CDG identification across multiple networks at the decision level. Instead of enforcing feature-level consistency, SEFGNN treats each biological network as an independent evidence source and performs uncertainty-aware fusion at the decision level using Dempster-Shafer Theory (DST). To alleviate the risk of overconfidence from DST, we further introduce a Soft Evidence Smoothing (SES) module that improves ranking stability while preserving discriminative performance. Experiments on three cancer datasets show that SEFGNN consistently outperforms state-of-the-art baselines and exhibits strong potential in discovering novel CDGs.",Bioinformatics,http://arxiv.org/abs/2510.06290v1,arXiv,1
"Nanopore sequencing technologies continue to advance rapidly, offering critical benefits such as real-time analysis, the ability to sequence extremely long DNA fragments (up to millions of bases in a single read), and the option to selectively stop sequencing a molecule before completion. Traditionally, the raw electrical signals generated during sequencing are converted into DNA sequences through a process called basecalling, which typically relies on large neural network models. Raw signal analysis has emerged as a promising alternative to these resource-intensive approaches. While attempts have been made to benchmark conventional basecalling methods, existing evaluation frameworks 1) overlook raw signal analysis techniques, 2) lack the flexibility to accommodate new raw signal analysis tools easily, and 3) fail to include the latest improvements in nanopore datasets. Our goal is to provide an extensible benchmarking framework that enables designing and comparing new methods for raw signal analysis. To this end, we introduce RawBench, the first flexible framework for evaluating raw nanopore signal analysis techniques. RawBench provides modular evaluation of three core pipeline components: 1) reference genome encoding (using different pore models), 2) signal encoding (through various segmentation methods), and 3) representation matching (via different data structures). We extensively evaluate raw signal analysis techniques in terms of 1) quality and performance for read mapping, 2) quality and performance for read classification, and 3) quality of raw signal analysis-assisted basecalling. Our evaluations show that raw signal analysis can achieve competitive quality while significantly reducing resource requirements, particularly in settings where real-time processing or edge deployment is necessary.",Bioinformatics,http://arxiv.org/abs/2510.03629v1,arXiv,1
"Deep learning, particularly with the advancement of Large Language Models, has transformed biomolecular modeling, with protein advances (e.g., ESM) inspiring emerging RNA language models such as RiNALMo. Yet how and what these RNA Language Models internally encode about messenger RNA (mRNA) or non-coding RNA (ncRNA) families remains unclear. We present SAE- RNA, interpretability model that analyzes RiNALMo representations and maps them to known human-level biological features. Our work frames RNA interpretability as concept discovery in pretrained embeddings, without end-to-end retraining, and provides practical tools to probe what RNA LMs may encode about ncRNA families. The model can be extended to close comparisons between RNA groups, and supporting hypothesis generation about previously unrecognized relationships.",Bioinformatics,http://arxiv.org/abs/2510.02734v1,arXiv,1
"Gene expression data is essential for understanding how genes are regulated and interact within biological systems, providing insights into disease pathways and potential therapeutic targets. Gene knockout has proven to be a fundamental technique in molecular biology, allowing the investigation of the function of specific genes in an organism, as well as in specific cell types. However, gene expression patterns are quite heterogeneous in single-cell transcriptional data from a uniform environment, representing different cell states, which produce cell-type and cell-specific gene knockout impacts. A computational method that can predict the single-cell resolution knockout impact is still lacking. Here, we present a data-driven framework for learning the mapping between gene expression profiles derived from gene assemblages, enabling the accurate prediction of perturbed expression profiles following knockout (KO) for any cell, without relying on prior perturbed data. We systematically validated our framework using synthetic data generated from gene regulatory dynamics models, two mouse knockout single-cell datasets, and high-throughput in vitro CRISPRi Perturb-seq data. Our results demonstrate that the framework can accurately predict both expression profiles and KO effects at the single-cell level. Our approach provides a generalizable tool for inferring gene function at single-cell resolution, offering new opportunities to study genetic perturbations in contexts where large-scale experimental screens are infeasible.",Bioinformatics,http://arxiv.org/abs/2510.03359v1,arXiv,1
"Medulloblastoma is a malignant pediatric brain cancer, and the discovery of molecular subgroups is enabling personalized treatment strategies. In 2019, a consensus identified eight novel subtypes within Groups 3 and 4, each displaying heterogeneous characteristics. Classifiers are essential for translating these findings into clinical practice by supporting clinical trials, personalized therapy development and application, and patient monitoring. This study presents a DNA methylation-based, cross-platform machine learning classifier capable of distinguishing these subtypes on both HM450 and EPIC methylation array samples. Across two independent test sets, the model achieved weighted F1 = 0.95 and balanced accuracy = 0.957, consistent across platforms. As the first cross-platform solution, it provides backward compatibility while extending applicability to a newer platform, also enhancing accessibility. It also has the potential to become the first publicly available classifier for these subtypes once deployed through a web application, as planned in the future. This work overall takes steps in the direction of advancing precision medicine and improving clinical outcomes for patients within the majority prevalence medulloblastoma subgroups, groups 3 and 4.",Bioinformatics,http://arxiv.org/abs/2510.02416v1,arXiv,1
"Background. Preeclampsia (PE) complicates 2-8% of pregnancies and is marked by placental hypoxia and HIF-pathway activation, especially in early-onset PE (eoPE). Integrating patient tissue analyses with experimental models may reveal common molecular markers of trophoblast hypoxia.   Methods. We analyzed scRNA-seq data from 10 eoPE, 7 late-onset PE (loPE), and corresponding control placentas, identifying villous cytotrophoblast (VCT), syncytiotrophoblast (SCT), and extravillous trophoblast (EVT) subpopulations. BeWo b30 cells were treated for 24 h with CoCl2 (300 $Î¼$M) or an oxyquinoline derivative (OD, 5 $Î¼$M) to induce hypoxia. RNA and small RNA sequencing quantified mRNA and microRNA changes. PROGENy inferred pathway activities.   Results. ScRNA-seq revealed highest hypoxia pathway activation in eoPE, with EVT showing maximum activity among trophoblast populations. Nine genes were upregulated across all trophoblast types in eoPE: EBI3, CST6, FN1, RFK, COL17A1, LDHA, PKP2, RPS4Y1, and RPS26. In vitro, OD induced more specific hypoxia responses than CoCl2, with 1,284 versus 3,032 differentially expressed genes respectively. Critically, EBI3, FN1, and COL17A1 showed concordant upregulation in both placental tissue and OD-treated cells, while CoCl2 treatment produced opposite expression patterns. MicroRNA analysis identified hsa-miR-27a-5p and hsa-miR-193b-5p as consistently elevated in both experimental conditions and previously reported in PE placental vesicles. We also identified isoforms of hsa-miR-9-5p and hsa-miR-92b-3p as hypoxia-associated in trophoblast.   Conclusions. EBI3, COL17A1, hsa-miR-27a-5p, and hsa-miR-193b-5p emerge as trophoblast hypoxia markers in PE. Oxyquinoline derivatives offer a more physiologically relevant in vitro hypoxia model than CoCl2. This integrated approach advances understanding of PE pathophysiology and suggests candidate therapeutic targets.",Bioinformatics,http://arxiv.org/abs/2510.01935v1,arXiv,1
"Epilepsy is a chronic neurological condition characterized by recurrent seizures, with global prevalence estimated at 50 million people worldwide. While progress in high-throughput sequencing has allowed for broad-based transcriptomic profiling of brain tissues, the deciphering of these highly complex datasets remains one of the challenges. To address this issue, in this paper we propose a new analysis pipeline that integrates the power of deep learning strategies with GPU-acceleration computation for investigating Gene expression patterns in epilepsy. Specifically, our proposed approach employs GPT-2 XL, a transformer-based Large Language Model (LLM) with 1.5 billion parameters for genomic sequence analysis over the latest NVIDIA H100 Tensor Core GPUs based on Hopper architecture. Our proposed method enables efficient preprocessing of RNA sequence data, gene sequence encoding, and subsequent pattern identification. We conducted experiments on two epilepsy datasets including GEO accession GSE264537 and GSE275235. The obtained results reveal several significant transcriptomic modifications, including reduced hippocampal astrogliosis after ketogenic diet treatment as well as restored excitatory-inhibitory signaling equilibrium in zebrafish epilepsy model. Moreover, our results highlight the effectiveness of leveraging LLMs in combination with advanced hardware acceleration for transcriptomic characterization in neurological diseases.",Bioinformatics,http://arxiv.org/abs/2510.00392v1,arXiv,1
"Genetic nurture effects and assortative mating (AM) occur across many human behaviors and can bias estimates from traditional genetic models. These influences are typically studied univariately, within the same trait. However, estimation of cross-trait genetic nurture effects and cross-trait AM remains underexplored due to the absence of suitable approaches. To address this, we developed a multivariate extension of the SEM-PGS model for datasets with genotyped and phenotyped parents and offspring, enabling joint estimation of within-trait and cross-trait genetic and environmental influences. By integrating haplotypic polygenic scores (PGS) into a structural equation modeling framework, the model simultaneously estimates same-trait and cross-trait direct effects, genetic nurture, vertical transmission, and assortative mating. We also provide the first formal description of how copaths can be used to model multivariate assortative mating and derive the corresponding parameter expectations in matrix form. Forward-time Monte Carlo simulations under varying conditions of r^2_PGS and N_trio demonstrate that the model yields unbiased estimates of both within-trait and cross-trait effects when assumptions are met. The precision of estimates was adequate with large sample sizes (N_trio > 16k) and improved as PGS predictive power increased. In addition, our simulation results show that failing to model cross-trait effects biases within-trait estimates, underscoring the importance of incorporating cross-trait effects. The multivariate SEM-PGS model offers a powerful and flexible tool for disentangling gene-environment interplay and advancing the understanding of familial influences on human traits.",Bioinformatics,http://arxiv.org/abs/2510.00353v2,arXiv,1
"Background: Large foundation models have revolutionized single-cell analysis, yet no kidney-specific model currently exists, and it remains unclear whether organ-focused models can outperform generalized models. The kidney's complex cellular architecture further complicate integration of large-scale omics data, where current frameworks trained on limited datasets struggle to correct batch effects, capture cross-modality variation, and generalize across species. Methods: We developed Nephrobase Cell+, the first kidney-focused large foundation model, pretrained on ~100 billion tokens from ~39.5 million single-cell and single-nucleus profiles across 4,319 samples. Nephrobase Cell+ uses a transformer-based encoder-decoder architecture with gene-token cross-attention and a mixture-of-experts module for scalable representation learning. Results: Nephrobase Cell+ sets a new benchmark for kidney single-cell analysis. It produces tightly clustered, biologically coherent embeddings in human and mouse kidneys, far surpassing previous foundation models such as Geneformer, scGPT, and UCE, as well as traditional methods such as PCA and autoencoders. It achieves the highest cluster concordance and batch-mixing scores, effectively removing donor/assay batch effects while preserving cell-type structure. Cross-species evaluation shows superior alignment of homologous cell types and >90% zero-shot annotation accuracy for major kidney lineages in both human and mouse. Even its 1B-parameter and 500M variants consistently outperform all existing models. Conclusions: Nephrobase Cell+ delivers a unified, high-fidelity representation of kidney biology that is robust, cross-species transferable, and unmatched by current single-cell foundation models, offering a powerful resource for kidney genomics and disease research.",Bioinformatics,http://arxiv.org/abs/2509.26223v1,arXiv,1
"Single-cell RNA sequencing (scRNA-seq) technology enables systematic delineation of cellular states and interactions, providing crucial insights into cellular heterogeneity. Building on this potential, numerous computational methods have been developed for tasks such as cell clustering, cell type annotation, and marker gene identification. To fully assess and compare these methods, standardized, analysis-ready datasets are essential. However, such datasets remain scarce, and variations in data formats, preprocessing workflows, and annotation strategies hinder reproducibility and complicate systematic evaluation of existing methods. To address these challenges, we present scUnified, an AI-ready standardized resource for single-cell RNA sequencing data that consolidates 13 high-quality datasets spanning two species (human and mouse) and nine tissue types. All datasets undergo standardized quality control and preprocessing and are stored in a uniform format to enable direct application in diverse computational analyses without additional data cleaning. We further demonstrate the utility of scUnified through experimental analyses of representative biological tasks, providing a reproducible foundation for the standardized evaluation of computational methods on a unified dataset.",Bioinformatics,http://arxiv.org/abs/2509.25884v2,arXiv,1
"Distinguishing the rare ""driver"" mutations that fuel cancer progression from the vast background of ""passenger"" mutations in the non-coding genome is a fundamental challenge in cancer biology. A primary mechanism that non-coding driver mutations contribute to cancer is by affecting gene expression, potentially from millions of nucleotides away. However, existing predictors of gene expression from mutations are unable to simultaneously handle interactions spanning millions of base pairs, the extreme sparsity of somatic mutations, and generalize to unseen genes. To overcome these limitations, we introduce GenVarFormer (GVF), a novel transformer-based architecture designed to learn mutation representations and their impact on gene expression. GVF efficiently predicts the effect of mutations up to 8 million base pairs away from a gene by only considering mutations and their local DNA context, while omitting the vast intermediate sequence. Using data from 864 breast cancer samples from The Cancer Genome Atlas, we demonstrate that GVF predicts gene expression with 26-fold higher correlation across samples than current models. In addition, GVF is the first model of its kind to generalize to unseen genes and samples simultaneously. Finally, we find that GVF patient embeddings are more informative than ground-truth gene expression for predicting overall patient survival in the most prevalent breast cancer subtype, luminal A. GVF embeddings and gene expression yielded concordance indices of $0.706^{\pm0.136}$ and $0.573^{\pm0.234}$, respectively. Our work establishes a new state-of-the-art for modeling the functional impact of non-coding mutations in cancer and provides a powerful new tool for identifying potential driver events and prognostic biomarkers.",Bioinformatics,http://arxiv.org/abs/2509.25573v1,arXiv,1
"Predicting cellular responses to genetic perturbations represents a fundamental challenge in systems biology, critical for advancing therapeutic discovery and virtual cell modeling. While large language models (LLMs) show promise for biological reasoning, their application to perturbation prediction remains underexplored due to challenges in adapting them to structured experimental data. We present SynthPert, a novel method that enhances LLM performance through supervised fine-tuning on synthetic reasoning traces generated by frontier models. Using the PerturbQA benchmark, we demonstrate that our approach not only achieves state-of-the-art performance but surpasses the capabilities of the frontier model that generated the training data. Our results reveal three key insights: (1) Synthetic reasoning traces effectively distill biological knowledge even when partially inaccurate, (2) This approach enables cross-cell-type generalization with 87% accuracy on unseen RPE1 cells, and (3) Performance gains persist despite using only 2% of quality-filtered training data. This work shows the effectiveness of synthetic reasoning distillation for enhancing domain-specific reasoning in LLMs.",Bioinformatics,http://arxiv.org/abs/2509.25346v1,arXiv,1
"Language models are increasingly applied to biological sequences like proteins and mRNA, yet their default Euclidean geometry may mismatch the hierarchical structures inherent to biological data. While hyperbolic geometry provides a better alternative for accommodating hierarchical data, it has yet to find a way into language modeling for mRNA sequences. In this work, we introduce HyperHELM, a framework that implements masked language model pre-training in hyperbolic space for mRNA sequences. Using a hybrid design with hyperbolic layers atop Euclidean backbone, HyperHELM aligns learned representations with the biological hierarchy defined by the relationship between mRNA and amino acids. Across multiple multi-species datasets, it outperforms Euclidean baselines on 9 out of 10 tasks involving property prediction, with 10% improvement on average, and excels in out-of-distribution generalization to long and low-GC content sequences; for antibody region annotation, it surpasses hierarchy-aware Euclidean models by 3% in annotation accuracy. Our results highlight hyperbolic geometry as an effective inductive bias for hierarchical language modeling of mRNA sequences.",Bioinformatics,http://arxiv.org/abs/2509.24655v2,arXiv,1
"Gene enhancers control when and where genes switch on, yet their sequence diversity and tissue specificity make them hard to pinpoint in colorectal cancer. We take a sequence-only route and fine-tune DNABERT-2, a transformer genomic language model that uses byte-pair encoding to learn variable-length tokens from DNA. Using assays curated via the Johnston Cancer Research Centre at Queen's University Belfast, we assembled a balanced corpus of 2.34 million 1 kb enhancer sequences, applied summit-centered extraction and rigorous de-duplication including reverse-complement collapse, and split the data stratified by class. With a 4096-term vocabulary and a 232-token context chosen empirically, the DNABERT-2-117M classifier was trained with Optuna-tuned hyperparameters and evaluated on 350742 held-out sequences. The model reached PR-AUC 0.759, ROC-AUC 0.743, and best F1 0.704 at an optimized threshold (0.359), with recall 0.835 and precision 0.609. Against a CNN-based EnhancerNet trained on the same data, DNABERT-2 delivered stronger threshold-independent ranking and higher recall, although point accuracy was lower. To our knowledge, this is the first study to apply a second-generation genomic language model with BPE tokenization to enhancer classification in colorectal cancer, demonstrating the feasibility of capturing tumor-associated regulatory signals directly from DNA sequence alone. Overall, our results show that transformer-based genomic models can move beyond motif-level encodings toward holistic classification of regulatory elements, offering a novel path for cancer genomics. Next steps will focus on improving precision, exploring hybrid CNN-transformer designs, and validating across independent datasets to strengthen real-world utility.",Bioinformatics,http://arxiv.org/abs/2509.25274v1,arXiv,1
"Antimicrobial Resistance (AMR) is a rapidly escalating global health crisis. While genomic sequencing enables rapid prediction of resistance phenotypes, current computational methods have limitations. Standard machine learning models treat the genome as an unordered collection of features, ignoring the sequential context of Single Nucleotide Polymorphisms (SNPs). State-of-the-art sequence models like Transformers are often too data-hungry and computationally expensive for the moderately-sized datasets that are typical in this domain. To address these challenges, we propose AMR-EnsembleNet, an ensemble framework that synergistically combines sequence-based and feature-based learning. We developed a lightweight, custom 1D Convolutional Neural Network (CNN) to efficiently learn predictive sequence motifs from high-dimensional SNP data. This sequence-aware model was ensembled with an XGBoost model, a powerful gradient boosting system adept at capturing complex, non-local feature interactions. We trained and evaluated our framework on a benchmark dataset of 809 E. coli strains, predicting resistance across four antibiotics with varying class imbalance. Our 1D CNN-XGBoost ensemble consistently achieved top-tier performance across all the antibiotics, reaching a Matthews Correlation Coefficient (MCC) of 0.926 for Ciprofloxacin (CIP) and the highest Macro F1-score of 0.691 for the challenging Gentamicin (GEN) AMR prediction. We also show that our model consistently focuses on SNPs within well-known AMR genes like fusA and parC, confirming it learns the correct genetic signals for resistance. Our work demonstrates that fusing a sequence-aware 1D CNN with a feature-based XGBoost model creates a powerful ensemble, overcoming the limitations of using either an order-agnostic or a standalone sequence model.",Bioinformatics,http://arxiv.org/abs/2509.23552v1,arXiv,1
"Large language models (LLMs) have shown strong ability in generating rich representations across domains such as natural language processing and generation, computer vision, and multimodal learning. However, their application in biomedical data analysis remains nascent. Single-cell transcriptomic profiling is essential for dissecting cell subtype diversity in development and disease, but rare subtypes pose challenges for scaling laws. We present a computational framework that integrates single-cell RNA sequencing (scRNA-seq) with LLMs to derive knowledge-informed gene embeddings. Highly expressed genes for each cell are mapped to NCBI Gene descriptions and embedded using models such as text-embedding-ada-002, BioBERT, and SciBERT. Applied to retinal ganglion cells (RGCs), which differ in vulnerability to glaucoma-related neurodegeneration, this strategy improves subtype classification, highlights biologically significant features, and reveals pathways underlying selective neuronal vulnerability. More broadly, it illustrates how LLM-derived embeddings can augment biological analysis under data-limited conditions and lay the groundwork for future foundation models in single-cell biology.",Bioinformatics,http://arxiv.org/abs/2509.23543v1,arXiv,1
"Background: Structural variants (SVs) are genomic differences $\ge$50 bp in length. They remain challenging to detect even with long sequence reads, and the sources of these difficulties are not well quantified.   Results: We identified 35.4 Mb of low-complexity regions (LCRs) in GRCh38. Although these regions cover only 1.2% of the genome, they contain 69.1% of confident SVs in sample HG002. Across long-read SV callers, 77.3-91.3% of erroneous SV calls occur within LCRs, with error rates increasing with LCR length.   Conclusion: SVs are enriched and difficult to call in LCRs. Special care need to be taken for calling and analyzing these variants.",Bioinformatics,http://arxiv.org/abs/2509.23057v1,arXiv,1
"Polygenic risk scores can be used to model the individual genetic liability for human traits. Current methods primarily focus on modeling the mean of a phenotype neglecting the variance. However, genetic variants associated with phenotypic variance can provide important insights to gene-environment interaction studies. To overcome this, we propose snpboostlss, a cyclical gradient boosting algorithm for a Gaussian location-scale model to jointly derive sparse polygenic models for both the mean and the variance of a quantitative phenotype. To improve computational efficiency on high-dimensional and large-scale genotype data (large n and large p), we only consider a batch of most relevant variants in each boosting step. We investigate the effect of statins therapy (the environmental factor) on low-density lipoprotein in the UK Biobank cohort using the new snpboostlss algorithm. We are able to verify the interaction between statins usage and the polygenic risk scores for phenotypic variance in both cross sectional and longitudinal analyses. Particularly, following the spirit of target trial emulation, we observe that the treatment effect of statins is more substantial in people with higher polygenic risk scores for phenotypic variance, indicating gene-environment interaction. When applying to body mass index, the newly constructed polygenic risk scores for variance show significant interaction with physical activity and sedentary behavior. Therefore, the polygenic risk scores for phenotypic variance derived by snpboostlss have potential to identify individuals that could benefit more from environmental changes (e.g. medical intervention and lifestyle changes).",Bioinformatics,http://arxiv.org/abs/2509.20850v1,arXiv,1
"Recent advances in large language model (LLM) embeddings have enabled powerful representations for biological data, but most applications to date focus only on gene-level information. We present one of the first systematic frameworks to generate variant-level embeddings across the entire human genome. Using curated annotations from FAVOR, ClinVar, and the GWAS Catalog, we constructed semantic text descriptions for 8.9 billion possible variants and generated embeddings at three scales: 1.5 million HapMap3+MEGA variants, ~90 million imputed UK Biobank variants, and ~9 billion all possible variants. Embeddings were produced with both OpenAI's text-embedding-3-large and the open-source Qwen3-Embedding-0.6B models. Baseline experiments demonstrate high predictive accuracy for variant properties, validating the embeddings as structured representations of genomic variation. We outline two downstream applications: embedding-informed hypothesis testing by extending the Frequentist And Bayesian framework to genome-wide association studies, and embedding-augmented genetic risk prediction that enhances standard polygenic risk scores. These resources, publicly available on Hugging Face, provide a foundation for advancing large-scale genomic discovery and precision medicine.",Bioinformatics,http://arxiv.org/abs/2509.20702v1,arXiv,1
"We investigate the application of Small Language Models (<10 billion parameters) for genomics question answering via agentic framework to address hallucination issues and computational cost challenges. The Nano Bio-Agent (NBA) framework we implemented incorporates task decomposition, tool orchestration, and API access into well-established systems such as NCBI and AlphaGenome. Results show that SLMs combined with such agentic framework can achieve comparable and in many cases superior performance versus existing approaches utilising larger models, with our best model-agent combination achieving 98% accuracy on the GeneTuring benchmark. Notably, small 3-10B parameter models consistently achieve 85-97% accuracy while requiring much lower computational resources than conventional approaches. This demonstrates promising potential for efficiency gains, cost savings, and democratization of ML-powered genomics tools while retaining highly robust and accurate performance.",Bioinformatics,http://arxiv.org/abs/2509.19566v1,arXiv,1
"A fundamental property of DNA is that the reverse complement (RC) of a sequence often carries identical biological meaning. However, state-of-the-art DNA language models frequently fail to capture this symmetry, producing inconsistent predictions for a sequence and its RC counterpart, which undermines their reliability. In this work, we introduce Reverse-Complement Consistency Regularization (RCCR), a simple and model-agnostic fine-tuning objective that directly penalizes the divergence between a model's prediction on a sequence and the aligned prediction on its reverse complement. We evaluate RCCR across three diverse backbones (Nucleotide Transformer, HyenaDNA, DNABERT-2) on a wide range of genomic tasks, including sequence classification, scalar regression, and profile prediction. Our experiments show that RCCR substantially improves RC robustness by dramatically reducing prediction flips and errors, all while maintaining or improving task accuracy compared to baselines such as RC data augmentation and test-time averaging. By integrating a key biological prior directly into the learning process, RCCR produces a single, intrinsically robust, and computationally efficient model fine-tuning recipe for diverse biology tasks.",Bioinformatics,http://arxiv.org/abs/2509.18529v1,arXiv,1
"DNA language models have revolutionized our ability to understand and design DNA sequences--the fundamental language of life--with unprecedented precision, enabling transformative applications in therapeutics, synthetic biology, and gene editing. However, this capability also poses substantial dual-use risks, including the potential for creating pathogens, viruses, and even bioweapons. To address these biosecurity challenges, we introduce two innovative watermarking techniques to reliably track the designed DNA: DNAMark and CentralMark. DNAMark employs synonymous codon substitutions to embed watermarks in DNA sequences while preserving the original function. CentralMark further advances this by creating inheritable watermarks that transfer from DNA to translated proteins, leveraging protein embeddings to ensure detection across the central dogma. Both methods utilize semantic embeddings to generate watermark logits, enhancing robustness against natural mutations, synthesis errors, and adversarial attacks. Evaluated on our therapeutic DNA benchmark, DNAMark and CentralMark achieve F1 detection scores above 0.85 under various conditions, while maintaining over 60% sequence similarity to ground truth and degeneracy scores below 15%. A case study on the CRISPR-Cas9 system underscores CentralMark's utility in real-world settings. This work establishes a vital framework for securing DNA language models, balancing innovation with accountability to mitigate biosecurity risks.",Bioinformatics,http://arxiv.org/abs/2509.18207v1,arXiv,1
"Data visualization is a fundamental tool in genomics research, enabling the exploration, interpretation, and communication of complex genomic features. While machine learning models show promise for transforming data into insightful visualizations, current models lack the training foundation for domain-specific tasks. In an effort to provide a foundational resource for genomics-focused model training, we present a framework for generating a dataset that pairs abstract, low-level questions about genomics data with corresponding visualizations. Building on prior work with statistical plots, our approach adapts to the complexity of genomics data and the specialized representations used to depict them. We further incorporate multiple linked queries and visualizations, along with justifications for design choices, figure captions, and image alt-texts for each item in the dataset. We use genomics data retrieved from three distinct genomics data repositories (4DN, ENCODE, Chromoscope) to produce GQVis: a dataset consisting of 1.14 million single-query data points, 628k query pairs, and 589k query chains. The GQVis dataset and generation code are available at https://huggingface.co/datasets/HIDIVE/GQVis and https://github.com/hms-dbmi/GQVis-Generation.",Bioinformatics,http://arxiv.org/abs/2510.13816v1,arXiv,1
"Small interfering RNA (siRNA) is a short double-stranded RNA molecule (about 21-23 nucleotides) with the potential to cure diseases by silencing the function of target genes. Due to its well-understood mechanism, many siRNA-based drugs have been evaluated in clinical trials. However, selecting effective binding regions and designing siRNA sequences requires extensive experimentation, making the process costly. As genomic resources and publicly available siRNA datasets continue to grow, data-driven models can be leveraged to better understand siRNA-mRNA interactions. To fully exploit such data, curating high-quality siRNA datasets is essential to minimize experimental errors and noise. We propose siDPT: siRNA efficacy Prediction via Debiased Preference-Pair Transformer, a framework that constructs a preference-pair dataset and designs an siRNA-mRNA interactive transformer with debiased ranking objectives to improve siRNA inhibition prediction and generalization. We evaluate our approach using two public datasets and one newly collected patent dataset. Our model demonstrates substantial improvement in Pearson correlation and strong performance across other metrics.",Bioinformatics,http://arxiv.org/abs/2509.15664v1,arXiv,1
"Understanding disease similarity is critical for advancing diagnostics, drug discovery, and personalized treatment strategies. We present PhenoGnet, a novel graph-based contrastive learning framework designed to predict disease similarity by integrating gene functional interaction networks with the Human Phenotype Ontology (HPO). PhenoGnet comprises two key components: an intra-view model that separately encodes gene and phenotype graphs using Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs), and a cross view model implemented as a shared weight multilayer perceptron (MLP) that aligns gene and phenotype embeddings through contrastive learning. The model is trained using known gene phenotype associations as positive pairs and randomly sampled unrelated pairs as negatives. Diseases are represented by the mean embeddings of their associated genes and/or phenotypes, and pairwise similarity is computed via cosine similarity. Evaluation on a curated benchmark of 1,100 similar and 866 dissimilar disease pairs demonstrates strong performance, with gene based embeddings achieving an AUCPR of 0.9012 and AUROC of 0.8764, outperforming existing state of the art methods. Notably, PhenoGnet captures latent biological relationships beyond direct overlap, offering a scalable and interpretable solution for disease similarity prediction. These results underscore its potential for enabling downstream applications in rare disease research and precision medicine.",Bioinformatics,http://arxiv.org/abs/2509.14037v1,arXiv,1
"Sequencing of PCR amplicons generated using degenerate primers (typically targeting a region of the 16S ribosomal gene) is widely used in metagenomics to profile the taxonomic composition of complex microbial samples. To reduce taxonomic biases in primer selection it is important to conduct in silico PCR analyses of the primers against large collections of up to millions of bacterial genomes. However, existing in silico PCR tools have impractical running time for analyses of this scale. In this paper we introduce AmpliconHunter, a highly scalable in silico PCR package distributed as open-source command-line tool and publicly available through a user-friendly web interface at https://ah1.engr.uconn.edu/. AmpliconHunter implements an accurate nearest-neighbor model for melting temperature calculations, allowing for primer-template hybridization with mismatches, along with three complementary methods for estimating off-target amplification. By taking advantage of multi-core parallelism and SIMD operations available on modern CPUs, the AmpliconHunter web server can complete in silico PCR analyses of commonly used degenerate primer pairs against the 2.4M genomes in the latest AllTheBacteria collection in as few as 6-7 hours.",Bioinformatics,http://arxiv.org/abs/2509.13300v1,arXiv,1
"Summary: Uchimata is a toolkit for visualization of 3D structures of genomes. It consists of two packages: a Javascript library facilitating the rendering of 3D models of genomes, and a Python widget for visualization in Jupyter Notebooks. Main features include an expressive way to specify visual encodings, and filtering of 3D genome structures based on genomic semantics and spatial aspects. Uchimata is designed to be highly integratable with biological tooling available in Python. Availability and Implementation: Uchimata is released under the MIT License. The Javascript library is available on NPM, while the widget is available as a Python package hosted on PyPI. The source code for both is available publicly on Github (https://github.com/hms-dbmi/uchimata and https://github.com/hms-dbmi/uchimata-py) and Zenodo: (https://doi.org/10.5281/zenodo.17831959 and https://doi.org/10.5281/zenodo.17832045). The documentation with examples is hosted at https://hms-dbmi.github.io/uchimata/ Contact: david_kouril@hms.harvard.edu or nils@hms.harvard.edu.",Bioinformatics,http://arxiv.org/abs/2509.13290v2,arXiv,1
"Summary: Microbiome HiFi Amplicon Sequence Simulator (MHASS) creates realistic synthetic PacBio HiFi amplicon sequencing datasets for microbiome studies, by integrating genome-aware abundance modeling, realistic dual-barcoding strategies, and empirically derived pass-number distributions from actual sequencing runs. MHASS generates datasets tailored for rigorous benchmarking and validation of long-read microbiome analysis workflows, including ASV clustering and taxonomic assignment.   Availability and Implementation: Implemented in Python with automated dependency management, the source code for MHASS is freely available at https://github.com/rhowardstone/MHASS along with installation instructions.   Contact: rye.howard-stone@uconn.edu or ion.mandoiu@uconn.edu   Supplementary information: Supplementary data are available online at https://github.com/rhowardstone/MHASS_evaluation.",Bioinformatics,http://arxiv.org/abs/2509.12428v1,arXiv,1
"TRPM4 is overexpressed in prostate cancer (PCa) associated with metastasis or recurrence. There is paucity of information pertaining to TRPM4 characterization and functions at single-cell level in PCa. This study introduces the CEP-IP framework, a novel explainable machine learning approach for the identification of biologically relevant cell subpopulations in single-cell transcriptomics of pairwise genes. The framework combines cell explanatory power (CEP) classification with inflection point (IP) analysis to assign model performance into individual cells, while stratifying transcriptional space into distinct regions. To demonstrate the CEP-IP framework, generalized additive model (GAM) was applied to model relationships between TRPM4 and co-expressed genes in aggressive PCa and benign prostate cells from scRNA-seq data. Genes were identified using Spearman-Kendall dual-filter. Seven ribosomal genes (RPL10, RPL27, RPL28, RPS2, RPS8, RPS12, RPS26; averaged as Ribo gene set) passed dual-filtering specifically in PCa cells. TRPM4-Ribo modeling outperformed alternative cancer gene sets (deviance explained FDR<0.05). The CEP-IP framework identified top-ranked explanatory power (TREP) cells and stratified them using IP analysis into pre-IP and post-IP regions, creating four distinct cell subpopulations per patient. Gene Ontology analysis revealed pre-IP TREP cells enriched for immune-related processes, while post-IP TREP cells enriched for ribosomal, translation, and cell adhesion pathways. In conclusion, the explainable CEP-IP framework allows for the identification of cancer cell subpopulations with distinctive biology and therapeutic implications, applicable to any pairwise gene analysis.",Bioinformatics,http://arxiv.org/abs/2509.12073v2,arXiv,1
"We introduce Genome-Factory, an integrated Python library for tuning, deploying, and interpreting genomic models. Our core contribution is to simplify and unify the workflow for genomic model development: data collection, model tuning, inference, benchmarking, and interpretability. For data collection, Genome-Factory offers an automated pipeline to download genomic sequences and preprocess them. It also includes quality control, such as GC content normalization. For model tuning, Genome-Factory supports three approaches: full-parameter, low-rank adaptation, and adapter-based fine-tuning. It is compatible with a wide range of genomic models. For inference, Genome-Factory enables both embedding extraction and DNA sequence generation. For benchmarking, we include two existing benchmarks and provide a flexible interface for users to incorporate additional benchmarks. For interpretability, Genome-Factory introduces the first open-source biological interpreter based on a sparse auto-encoder. This module disentangles embeddings into sparse, near-monosemantic latent units and links them to interpretable genomic features by regressing on external readouts. To improve accessibility, Genome-Factory features both a zero-code command-line interface and a user-friendly web interface. We validate the utility of Genome-Factory across three dimensions: (i) Compatibility with diverse models and fine-tuning methods; (ii) Benchmarking downstream performance using two open-source benchmarks; (iii) Biological interpretation of learned representations with DNABERT-2. These results highlight its end-to-end usability and practical value for real-world genomic analysis.",Bioinformatics,http://arxiv.org/abs/2509.12266v1,arXiv,1
"We introduce a unified framework for evaluating dimensionality reduction techniques in spatial transcriptomics beyond standard PCA approaches. We benchmark six methods PCA, NMF, autoencoder, VAE, and two hybrid embeddings on a cholangiocarcinoma Xenium dataset, systematically varying latent dimensions ($k$=5-40) and clustering resolutions ($Ï$=0.1-1.2). Each configuration is evaluated using complementary metrics including reconstruction error, explained variance, cluster cohesion, and two novel biologically-motivated measures: Cluster Marker Coherence (CMC) and Marker Exclusion Rate (MER). Our results demonstrate distinct performance profiles: PCA provides a fast baseline, NMF maximizes marker enrichment, VAE balances reconstruction and interpretability, while autoencoders occupy a middle ground. We provide systematic hyperparameter selection using Pareto optimal analysis and demonstrate how MER-guided reassignment improves biological fidelity across all methods, with CMC scores improving by up to 12\% on average. This framework enables principled selection of dimensionality reduction methods tailored to specific spatial transcriptomics analyses.",Bioinformatics,http://arxiv.org/abs/2509.13344v1,arXiv,1
"Differentiating between the two main subtypes of Inflammatory Bowel Disease (IBD): Crohns disease (CD) and ulcerative colitis (UC) is a persistent clinical challenge due to overlapping presentations. This study introduces a novel computational framework that employs spatial transcriptomics (ST) to create an explainable machine learning model for IBD classification. We analyzed ST data from the colonic mucosa of healthy controls (HC), UC, and CD patients. Using Non-negative Matrix Factorization (NMF), we first identified four recurring cellular niches, representing distinct functional microenvironments within the tissue. From these niches, we systematically engineered 44 features capturing three key aspects of tissue pathology: niche composition, neighborhood enrichment, and niche-gene signals. A multilayer perceptron (MLP) classifier trained on these features achieved an accuracy of 0.774 +/- 0.161 for the more challenging three-class problem (HC, UC, and CD) and 0.916 +/- 0.118 in the two-class problem of distinguishing IBD from healthy tissue. Crucially, model explainability analysis revealed that disruptions in the spatial organization of niches were the strongest predictors of general inflammation, while the classification between UC and CD relied on specific niche-gene expression signatures. This work provides a robust, proof-of-concept pipeline that transforms descriptive spatial data into an accurate and explainable predictive tool, offering not only a potential new diagnostic paradigm but also deeper insights into the distinct biological mechanisms that drive IBD subtypes.",Bioinformatics,http://arxiv.org/abs/2509.09923v1,arXiv,1
"The gene set analysis (GSA) is a foundational approach for uncovering the molecular functions associated with a group of genes. Recently, LLM-powered methods have emerged to annotate gene sets with biological functions together with coherent explanatory insights. However, existing studies primarily focus on proprietary models, which have been shown to outperform their open-source counterparts despite concerns over cost and data privacy. Furthermore, no research has investigated the application of advanced reasoning strategies to the GSA task. To address this gap, we introduce Gene-R1, a data-augmented learning framework that equips lightweight and open-source LLMs with step-by-step reasoning capabilities tailored to GSA. Experiments on 1,508 in-distribution gene sets demonstrate that Gene-R1 achieves substantial performance gains, matching commercial LLMs. On 106 out-of-distribution gene sets, Gene-R1 performs comparably to both commercial and large-scale LLMs, exhibiting robust generalizability across diverse gene sources.",Bioinformatics,http://arxiv.org/abs/2509.10575v1,arXiv,1
"Motivation: Low-complexity (LC) DNA sequences are compositionally repetitive sequences that are often associated with spurious homologous matches and variant calling artifacts. While algorithms for identifying LC sequences exist, they either do not define complexity mathematically or are inefficient with long or variable context windows.   Results: Longdust is a new algorithm that efficiently identifies long LC sequences including centromeric satellite and tandem repeats with moderately long motifs. It defines string complexity by statistically modeling the k-mer count distribution with the parameters: the k-mer length, the context window size and a threshold on complexity. Longdust exhibits high performance on real data and high consistency with existing methods.   Availability and implementation: https://github.com/lh3/longdust",Bioinformatics,http://arxiv.org/abs/2509.07357v2,arXiv,1
"As synthetic genomics scales toward the construction of increasingly larger genomes, computational strategies are needed to address technical feasibility. We introduce an algorithmic framework for the Minimum-Cost Synthetic Genome Planning problem, aiming to identify the most cost-effective strategy to assemble a target genome from a source genome through a combination of reuse, synthesis, and join operations. By comparing dynamic programming and greedy heuristic strategies under diverse cost regimes, we demonstrate how algorithmic choices influence the cost-efficiency of large-scale genome construction. In parallel, solving the Minimum-Cost Synthetic Genome Planning problem can help us better understand genome architecture and evolution. We applied our framework in case studies on viral genomes, including SARS-CoV-2, to examine how source-target genome similarity shapes construction costs. Our analyses revealed that conserved regions such as ORF1ab can be reconstructed cost-effectively from related templates, while highly variable regions such as the S (spike) gene are more reliant on DNA synthesis, highlighting the biological and economic trade-offs of genome design.",Bioinformatics,http://arxiv.org/abs/2509.06234v1,arXiv,1
"The frequency distributions of DNA k-mers are shaped by fundamental biological processes and offer a window into genome structure and evolution. Inspired by analogies to natural language, prior studies have attempted to model genomic k-mer usage using Zipf's law, a rank-frequency law originally formulated for words in human language. However, the extent to which this law accurately captures the distribution of k-mers across diverse species remains unclear. Here, we systematically analyze k-mer frequency spectra across more than 225,000 genome assemblies spanning all three domains of life and viruses. We demonstrate that Zipf's law consistently underperforms in modeling k-mer distributions. In contrast, we propose the truncated power law and Zipf-Mandelbrot distributions, which provide substantially improved fits across taxonomic groups. We show that genome size and GC content influence model performance, with larger and GC-content imbalanced genomes yielding better adherence. Additionally, we perform an extensive analysis on vocabulary expansion and exhaustion across the same organisms using Heaps' law. We apply our modeling framework to evaluate simulated genomes generated by k-let preserving shuffling and deep generative language models. Our results reveal substantial differences between organismal genomes and their synthetic or shuffled counterparts, offering a novel approach to benchmark the biological plausibility of artificial genomes. Collectively, this work establishes new standards for modeling genomic k-mer distributions and provides insights relevant to synthetic biology, and evolutionary sequence analysis.",Bioinformatics,http://arxiv.org/abs/2509.05539v1,arXiv,1
"Prediction of patient survival using high-dimensional multi-omics data requires systematic feature selection methods that ensure predictive performance, sparsity, and reliability for prognostic biomarker discovery. We developed a hybrid ensemble feature selection (hEFS) approach that combines data subsampling with multiple prognostic models, integrating both embedded and wrapper-based strategies for survival prediction. Omics features are ranked using a voting-theory-inspired aggregation mechanism across models and subsamples, while the optimal number of features is selected via a Pareto front, balancing predictive accuracy and model sparsity without any user-defined thresholds. When applied to multi-omics datasets from three pancreatic cancer cohorts, hEFS identifies significantly fewer and more stable biomarkers compared to the conventional, late-fusion CoxLasso models, while maintaining comparable discrimination performance. Implemented within the open-source mlr3fselect R package, hEFS offers a robust, interpretable, and clinically valuable tool for prognostic modelling and biomarker discovery in high-dimensional survival settings.",Bioinformatics,http://arxiv.org/abs/2509.02648v1,arXiv,1
"Single-cell RNA sequencing (scRNA-seq) provides unprecedented insights into cellular heterogeneity, enabling detailed analysis of complex biological systems at single-cell resolution. However, the high dimensionality and technical noise inherent in scRNA-seq data pose significant analytical challenges. While current embedding methods focus primarily on gene expression levels, they often overlook crucial gene-gene interactions that govern cellular identity and function. To address this limitation, we present a novel embedding approach that integrates both gene expression profiles and data-driven gene-gene interactions. Our method first constructs a Cell-Leaf Graph (CLG) using random forest models to capture regulatory relationships between genes, while simultaneously building a K-Nearest Neighbor Graph (KNNG) to represent expression similarities between cells. These graphs are then combined into an Enriched Cell-Leaf Graph (ECLG), which serves as input for a graph neural network to compute cell embeddings. By incorporating both expression levels and gene-gene interactions, our approach provides a more comprehensive representation of cellular states. Extensive evaluation across multiple datasets demonstrates that our method enhances the detection of rare cell populations and improves downstream analyses such as visualization, clustering, and trajectory inference. This integrated approach represents a significant advance in single-cell data analysis, offering a more complete framework for understanding cellular diversity and dynamics.",Bioinformatics,http://arxiv.org/abs/2509.02639v1,arXiv,1
"Single-cell technologies provide an unprecedented opportunity for dissecting the interplay between the cancer cells and the associated tumor microenvironment, and the produced high-dimensional omics data should also augment existing survival modeling approaches for identifying tumor cell type-specific genes predictive of cancer patient survival. However, there is no statistical model to integrate multiscale data including individual-level survival data, multicellular-level cell composition data and cellular-level single-cell omics covariates. We propose a class of Bayesian generalized promotion time cure models (GPTCMs) for the multiscale data integration to identify cell-type-specific genes and improve cancer prognosis. We demonstrate with simulations in both low- and high-dimensional settings that the proposed Bayesian GPTCMs are able to identify cell-type-associated covariates and improve survival prediction.",Bioinformatics,http://arxiv.org/abs/2509.01001v2,arXiv,1
"Understanding the modular structure and central elements of complex biological networks is critical for uncovering system-level mechanisms in disease. Here, we constructed weighted gene co-expression networks from bulk RNA-seq data of rheumatoid arthritis (RA) synovial tissue, using pairwise correlation and a percolation-guided thresholding strategy. Community detection with Louvain and Leiden algorithms revealed robust modules, and node-strength ranking identified the top 50 hub genes globally and within communities. To assess novelty, we integrated genome-wide association studies (GWAS) with literature-based evidence from PubMed, highlighting five high-centrality genes with little to no prior RA-specific association. Functional enrichment confirmed their roles in immune-related processes, including adaptive immune response and lymphocyte regulation. Notably, these hubs showed strong positive correlations with T- and B-cell markers and negative correlations with NK-cell markers, consistent with RA immunopathology. Overall, our framework demonstrates how correlation-based network construction, modularity-driven clustering, and centrality-guided novelty scoring can jointly reveal informative structure in omics-scale data. This generalizable approach offers a scalable path to gene prioritization in RA and other autoimmune conditions.",Bioinformatics,http://arxiv.org/abs/2509.00897v1,arXiv,1
"Accurate prediction of virus-host interactions is critical for understanding viral ecology and developing applications like phage therapy. However, the growing number of computational tools has created a complex landscape, making direct performance comparison challenging due to inconsistent benchmarks and varying usability. Here, we provide a systematic review and a rigorous benchmark of 27 virus-host prediction tools. We formulate the host prediction task into two primary frameworks, link prediction and multi-class classification, and construct two benchmark datasets to evaluate tool performance in distinct scenarios: a database-centric dataset (RefSeq-VHDB) and a metagenomic discovery dataset (MetaHiC-VHDB). Our results reveal that no single tool is universally optimal. Performance is highly context-dependent, with tools like CHERRY and iPHoP demonstrating robust, broad applicability, while others, such as RaFAH and PHIST, excel in specific contexts. We further identify a critical trade-off between predictive accuracy, prediction rate, and computational cost. This work serves as a practical guide for researchers and establishes a standardized benchmark to drive future innovation in deciphering complex virus-host interactions.",Bioinformatics,http://arxiv.org/abs/2509.00349v1,arXiv,1
"Generative modeling of discrete variables is challenging yet crucial for applications in natural language processing and biological sequence design. We introduce the Shortlisting Model (SLM), a novel simplex-based diffusion model inspired by progressive candidate pruning. SLM operates on simplex centroids, reducing generation complexity and enhancing scalability. Additionally, SLM incorporates a flexible implementation of classifier-free guidance, enhancing unconditional generation performance. Extensive experiments on DNA promoter and enhancer design, protein design, character-level and large-vocabulary language modeling demonstrate the competitive performance and strong potential of SLM. Our code can be found at https://github.com/GenSI-THUAIR/SLM",Bioinformatics,http://arxiv.org/abs/2508.17345v1,arXiv,1
"Single-cell multi-omics data contain huge information of cellular states, and analyzing these data can reveal valuable insights into cellular heterogeneity, diseases, and biological processes. However, as cell differentiation \& development is a continuous and dynamic process, it remains challenging to computationally model and infer cell interaction patterns based on single-cell multi-omics data. This paper presents scI2CL, a new single-cell multi-omics fusion framework based on intra- and inter-omics contrastive learning, to learn comprehensive and discriminative cellular representations from complementary multi-omics data for various downstream tasks. Extensive experiments of four downstream tasks validate the effectiveness of scI2CL and its superiority over existing peers. Concretely, in cell clustering, scI2CL surpasses eight state-of-the-art methods on four widely-used real-world datasets. In cell subtyping, scI2CL effectively distinguishes three latent monocyte cell subpopulations, which are not discovered by existing methods. Simultaneously, scI2CL is the only method that correctly constructs the cell developmental trajectory from hematopoietic stem and progenitor cells to Memory B cells. In addition, scI2CL resolves the misclassification of cell types between two subpopulations of CD4+ T cells, while existing methods fail to precisely distinguish the mixed cells. In summary, scI2CL can accurately characterize cross-omics relationships among cells, thus effectively fuses multi-omics data and learns discriminative cellular representations to support various downstream analysis tasks.",Bioinformatics,http://arxiv.org/abs/2508.18304v1,arXiv,1
"Understanding which genes control which traits in an organism remains one of the central challenges in biology. Despite significant advances in data collection technology, our ability to map genes to traits is still limited. This genome-to-phenome (G2P) challenge spans several problem domains, including plant breeding, and requires models capable of reasoning over high-dimensional, heterogeneous, and biologically structured data. Currently, however, many datasets solely capture genetic information or solely capture phenotype information. Additionally, phenotype data is very heterogeneous, which many datasets do not fully capture. The critical drawback is that these datasets are not integrated, that is, they do not link with each other to describe the same biological specimens. This limits machine learning models' ability to be informed on the various aspects of these specimens, impacting the breadth of correlations learned, and therefore their ability to make more accurate predictions. To address this gap, we present the Arabidopsis Genomics-Phenomics (AGP) Dataset, a curated multi-modal dataset linking gene expression profiles with phenotypic trait measurements in Arabidopsis thaliana, a model organism in plant biology. AGP supports tasks such as phenotype prediction and interpretable graph learning. In addition, we benchmark conventional regression and explanatory baselines, including a biologically-informed hypergraph baseline, to validate gene-trait associations. To the best of our knowledge, this is the first dataset that provides multi-modal gene information and heterogeneous trait or phenotype data for the same Arabidopsis thaliana specimens. With AGP, we aim to foster the research community towards accurately understanding the connection between genotypes and phenotypes using gene information, higher-order gene pairings, and trait data from several sources.",Bioinformatics,http://arxiv.org/abs/2508.14934v1,arXiv,1
"Variations in complex traits are influenced by multiple genetic variants, environmental risk factors, and their interactions. Though substantial progress has been made in identifying single genetic variants associated with complex traits, detecting the gene-gene and gene-environment interactions remains a great challenge. When a large number of genetic variants and environmental risk factors are involved, searching for interactions is limited to pair-wise interactions due to the exponentially increased feature space and computational intensity. Alternatively, recursive partitioning approaches, such as random forests, have gained popularity in high-dimensional genetic association studies. In this article, we propose a U-Statistic-based random forest approach, referred to as Forest U-Test, for genetic association studies with quantitative traits. Through simulation studies, we showed that the Forest U-Test outperformed existing methods. The proposed method was also applied to study Cannabis Dependence CD, using three independent datasets from the Study of Addiction: Genetics and Environment. A significant joint association was detected with an empirical p-value less than 0.001. The finding was also replicated in two independent datasets with p-values of 5.93e-19 and 4.70e-17, respectively.",Bioinformatics,http://arxiv.org/abs/2508.14924v1,arXiv,1
"In 2024, NHGRI-funded genomic resource projects completed a Self-Assessment Tool (SAT) and interviews to evaluate their application of FAIR (Findable, Accessible, Interoperable, Reusable) principles and sustainability. Key challenges were identified in metadata tools, data curation, variant identifiers, and data processing. Addressing these needs, we engaged the community through webinars and discussions, leading to a two-day workshop in March 2025. The workshop developed targeted recommendations, including improving transparency, standardizing identifiers, enhancing usability, implementing APIs, leveraging AI/ML for curation, and evaluating impact. These outcomes provide a framework for advancing FAIR practices, fostering collaboration, and strengthening the sustainability of NHGRI resources.",Bioinformatics,http://arxiv.org/abs/2508.13498v1,arXiv,1
"Objective: To investigate the mechanism by which quercetin inhibits triple-negative breast cancer (TNBC) through regulating T-cell-related targets, providing a novel strategy for TNBC immunotherapy.Methods: Single-cell RNA sequencing (GSE161529 dataset) and network pharmacology were integrated. PCA and UMAP clustering identified T-cell subsets and differentially expressed genes in TNBC microenvironment. TNBC-related targets were screened via CTD and OMIM databases, with functional pathways analyzed by GO/KEGG enrichment. Molecular docking and PPI networks validated interactions between quercetin and core targets.Results: Quercetin intersected with 79 TNBC targets, including AKT1, EGFR, and MMP9, enriched in EGFR inhibitor resistance and endocrine resistance pathways. Molecular docking revealed the highest affinity between quercetin and GSK3B (-13.2 kJ/mol). AKT1 and MMP9 expression correlated with patient survival.Conclusion: Quercetin may reverse TNBC immunosuppression by multi-target modulation of T-cell function, but clinical application requires solutions for its low bioavailability, such as delivery systems or combination therapies.",Bioinformatics,http://arxiv.org/abs/2508.12731v1,arXiv,1
"The surge in multimodal single-cell omics data exposes limitations in traditional, manually defined analysis workflows. AI agents offer a paradigm shift, enabling adaptive planning, executable code generation, traceable decisions, and real-time knowledge fusion. However, the lack of a comprehensive benchmark critically hinders progress. We introduce a novel benchmarking evaluation system to rigorously assess agent capabilities in single-cell omics analysis. This system comprises: a unified platform compatible with diverse agent frameworks and LLMs; multidimensional metrics assessing cognitive program synthesis, collaboration, execution efficiency, bioinformatics knowledge integration, and task completion quality; and 50 diverse real-world single-cell omics analysis tasks spanning multi-omics, species, and sequencing technologies. Our evaluation reveals that Grok-3-beta achieves state-of-the-art performance among tested agent frameworks. Multi-agent frameworks significantly enhance collaboration and execution efficiency over single-agent approaches through specialized role division. Attribution analyses of agent capabilities identify that high-quality code generation is crucial for task success, and self-reflection has the most significant overall impact, followed by retrieval-augmented generation (RAG) and planning. This work highlights persistent challenges in code generation, long-context handling, and context-aware knowledge retrieval, providing a critical empirical foundation and best practices for developing robust AI agents in computational biology.",Bioinformatics,http://arxiv.org/abs/2508.13201v1,arXiv,1
"Pre-training large language models on genomic sequences is a powerful approach for learning biologically meaningful representations. Masked language modeling (MLM) methods, such as DNABERT and Nucleotide Transformer (NT), achieve strong performance but suffer from partial token supervision, pre-training/fine-tuning mismatches, and high computational costs. We introduce NucEL, the first ELECTRA-style pre-training framework for genomic foundation models, addressing these limitations. Using a discriminator to identify tokens altered by a generator, NucEL provides comprehensive token-level supervision across all sequence positions, improving efficiency over the partial supervision of MLM. Incorporating ModernBERT's hybrid local-global attention and flash attention, NucEL offers an optimized BERT architecture for genomic modeling. Unlike 6-mer tokenization, NucEL uses single-nucleotide tokens for fine-grained resolution, boosting both efficiency and interpretability. Pre-trained on the human genome, NucEL achieves state-of-the-art results on diverse downstream tasks -- regulatory element identification (e.g., promoters, enhancers), transcription factor binding prediction, open chromatin classification, and histone modification profiling -- surpassing similarly sized MLM-based models and rivaling models 25x larger, such as NT. Ablation studies highlight optimal tokenization and masking strategies for ELECTRA-style DNA pre-training. Attention analysis reveals NucEL's superior capture of biologically relevant motifs compared to NT, providing insights into hierarchical learning and regulatory element modeling. These findings demonstrate ELECTRA-style pre-training as an efficient, effective strategy for genomic representation learning with broad implications for genomic research.",Bioinformatics,http://arxiv.org/abs/2508.13191v1,arXiv,1
"A fundamental limitation of probabilistic deep learning is its predominant reliance on Gaussian priors. This simplistic assumption prevents models from accurately capturing the complex, non-Gaussian landscapes of natural data, particularly in demanding domains like complex biological data, severely hindering the fidelity of the model for scientific discovery. The physically-grounded Boltzmann distribution offers a more expressive alternative, but it is computationally intractable on classical computers. To date, quantum approaches have been hampered by the insufficient qubit scale and operational stability required for the iterative demands of deep learning. Here, we bridge this gap by introducing the Quantum Boltzmann Machine-Variational Autoencoder (QBM-VAE), a large-scale and long-time stable hybrid quantum-classical architecture. Our framework leverages a quantum processor for efficient sampling from the Boltzmann distribution, enabling its use as a powerful prior within a deep generative model. Applied to million-scale single-cell datasets from multiple sources, the QBM-VAE generates a latent space that better preserves complex biological structures, consistently outperforming conventional Gaussian-based deep learning models like VAE and SCVI in essential tasks such as omics data integration, cell-type classification, and trajectory inference. It also provides a typical example of introducing a physics priori into deep learning to drive the model to acquire scientific discovery capabilities that breaks through data limitations. This work provides the demonstration of a practical quantum advantage in deep learning on a large-scale scientific problem and offers a transferable blueprint for developing hybrid quantum AI models.",Bioinformatics,http://arxiv.org/abs/2508.11190v1,arXiv,1
"Motivation: Recent advances in single-cell analysis have introduced new computational challenges. Researchers often need to use multiple analysis tools written in different programming languages while managing version conflicts between related packages within a single workflow. For the research community, minimizing the time spent on environment setup and installation issues is essential. Results: We present ShortCake, a containerized platform that integrates a suite of single-cell analysis tools written in R and Python. ShortCake isolates competing Python tools into separate virtual environments that can be easily accessed within a Jupyter notebook. This enables users to effortlessly transition between various environments, including R, even within a single notebook. Additionally, ShortCake offers multiple ``flavors,'' enabling users to select container images tailored to their specific needs. ShortCake provides a unified environment with fixed versions of various tools, thus streamlining workflows, reducing setup time, and improving reproducibility. Availability and implementation: The ShortCake image is available on DockerHub (https://hub.docker.com/r/rnakato/shortcake). The source code is available on GitHub (https://github.com/rnakato/ShortCake).",Bioinformatics,http://arxiv.org/abs/2508.08014v1,arXiv,1
"Deep generative models open new avenues for simulating realistic genomic data while preserving privacy and addressing data accessibility constraints. While previous studies have primarily focused on generating gene expression or haplotype data, this study explores generating genotype data in both unconditioned and phenotype-conditioned settings, which is inherently more challenging due to the discrete nature of genotype data. In this work, we developed and evaluated commonly used generative models, including Variational Autoencoders (VAEs), Diffusion Models, and Generative Adversarial Networks (GANs), and proposed adaptation tailored to discrete genotype data. We conducted extensive experiments on large-scale datasets, including all chromosomes from cow and multiple chromosomes from human. Model performance was assessed using a well-established set of metrics drawn from both deep learning and quantitative genetics literature. Our results show that these models can effectively capture genetic patterns and preserve genotype-phenotype association. Our findings provide a comprehensive comparison of these models and offer practical guidelines for future research in genotype simulation. We have made our code publicly available at https://github.com/SihanXXX/DiscreteGenoGen.",Bioinformatics,http://arxiv.org/abs/2508.09212v1,arXiv,1
"Integrating multi-omics data, such as DNA methylation, mRNA expression, and microRNA (miRNA) expression, offers a comprehensive view of the biological mechanisms underlying disease. However, the high dimensionality and complex interactions among omics layers present major challenges for predictive modeling. We propose Multi-Omics integration with Tree-generated Graph Neural Network (MOTGNN), a novel and interpretable framework for binary disease classification. MOTGNN employs eXtreme Gradient Boosting (XGBoost) to perform omics-specific supervised graph construction, followed by modality-specific Graph Neural Networks (GNNs) for hierarchical representation learning, and a deep feedforward network for cross-omics integration. On three real-world disease datasets, MOTGNN outperforms state-of-the-art baselines by 5-10% in accuracy, ROC-AUC, and F1-score, and remains robust to severe class imbalance (e.g., 87.2% vs. 33.4% F1 on imbalanced data). The model maintains computational efficiency through sparse graphs (2.1-2.8 edges per node) and provides built-in interpretability, revealing both top-ranked biomarkers and the relative contributions of each omics modality. These results highlight MOTGNN's potential to improve both predictive accuracy and interpretability in multi-omics disease modeling.",Bioinformatics,http://arxiv.org/abs/2508.07465v1,arXiv,1
"MicroRNAs (miRs) are robust regulators of gene expression, implicated in most biological processes. microRNAs predominantly downregulate the expression of genes post-transcriptionally and each miR is predicted to target several hundred genes. The accurate identification and annotation of miR-mRNA target interactions is central to understanding miRs function and their therapeutic potential. However, computational target prediction is challenging due to imperfect complementarity of miRs with their targets and the growing volume and heterogeneity of experimental data present challenges in accessing, integrating, and analysing miR-target interaction information across biological contexts. This creates a need for integrated resources and intelligent query tools.   We present the miRKat Suite, comprising miRKatDB, a comprehensive, curated database of predicted and validated miR-target interactions and associated annotations, and miRKatAI, a multi-agent system powered by large language models (LLMs) and LangGraph. miRKatDB integrates data from multiple publicly available sources, providing a comprehensive foundation for miR studies, including miR target genes and changes in levels of tissue expression previously reported. miRKatAI offers a natural language interface for complex querying of miRKatDB, facilitates grounded information retrieval from established sources in the field, and supports basic data visualisation. The miRKat Suite aims to accelerate miR research by streamlining data access, enhancing exploratory analysis, and supporting hypothesis generation.",Bioinformatics,http://arxiv.org/abs/2508.08331v2,arXiv,1
"Cardiovascular disease (CVD) prediction remains a tremendous challenge due to its multifactorial etiology and global burden of morbidity and mortality. Despite the growing availability of genomic and electrophysiological data, extracting biologically meaningful insights from such high-dimensional, noisy, and sparsely annotated datasets remains a non-trivial task. Recently, LLMs has been applied effectively to predict structural variations in biological sequences. In this work, we explore the potential of fine-tuned LLMs to predict cardiac diseases and SNPs potentially leading to CVD risk using genetic markers derived from high-throughput genomic profiling. We investigate the effect of genetic patterns associated with cardiac conditions and evaluate how LLMs can learn latent biological relationships from structured and semi-structured genomic data obtained by mapping genetic aspects that are inherited from the family tree. By framing the problem as a Chain of Thought (CoT) reasoning task, the models are prompted to generate disease labels and articulate informed clinical deductions across diverse patient profiles and phenotypes. The findings highlight the promise of LLMs in contributing to early detection, risk assessment, and ultimately, the advancement of personalized medicine in cardiac care.",Bioinformatics,http://arxiv.org/abs/2508.07127v1,arXiv,1
"The complexity of modern bioinformatics analysis has created a critical gap between data generation and developing scientific insights. While large language models (LLMs) have shown promise in scientific reasoning, they remain fundamentally limited when dealing with real-world analytical workflows that demand iterative computation, tool integration and rigorous validation. We introduce K-Dense Analyst, a hierarchical multi-agent system that achieves autonomous bioinformatics analysis through a dual-loop architecture. K-Dense Analyst, part of the broader K-Dense platform, couples planning with validated execution using specialized agents to decompose complex objectives into executable, verifiable tasks within secure computational environments. On BixBench, a comprehensive benchmark for open-ended biological analysis, K-Dense Analyst achieves 29.2% accuracy, surpassing the best-performing language model (GPT-5) by 6.3 percentage points, representing nearly 27% improvement over what is widely considered the most powerful LLM available. Remarkably, K-Dense Analyst achieves this performance using Gemini 2.5 Pro, which attains only 18.3% accuracy when used directly, demonstrating that our architectural innovations unlock capabilities far beyond the underlying model's baseline performance. Our insights demonstrate that autonomous scientific reasoning requires more than enhanced language models, it demands purpose-built systems that can bridge the gap between high-level scientific objectives and low-level computational execution. These results represent a significant advance toward fully autonomous computational biologists capable of accelerating discovery across the life sciences.",Bioinformatics,http://arxiv.org/abs/2508.07043v2,arXiv,1
"Understanding gene perturbation effects across diverse cellular contexts is a central challenge in functional genomics, with important implications for therapeutic discovery and precision medicine. Single-cell technologies enable high-resolution measurement of transcriptional responses, but collecting such data is costly and time-consuming, especially when repeated for each cell type. Existing computational methods often require separate models per cell type, limiting scalability and generalization. We present CFM-GP, a method for cell type-agnostic gene perturbation prediction. CFM-GP learns a continuous, time-dependent transformation between unperturbed and perturbed gene expression distributions, conditioned on cell type, allowing a single model to predict across all cell types. Unlike prior approaches that use discrete modeling, CFM-GP employs a flow matching objective to capture perturbation dynamics in a scalable manner. We evaluate on five datasets: SARS-CoV-2 infection, IFN-beta stimulated PBMCs, glioblastoma treated with Panobinostat, lupus under IFN-beta stimulation, and Statefate progenitor fate mapping. CFM-GP consistently outperforms state-of-the-art baselines in R-squared and Spearman correlation, and pathway enrichment analysis confirms recovery of key biological pathways. These results demonstrate the robustness and biological fidelity of CFM-GP as a scalable solution for cross-cell type gene perturbation prediction.",Bioinformatics,http://arxiv.org/abs/2508.08312v3,arXiv,1
"Large pre-trained DNA language models such as DNABERT-2, Nucleotide Transformer, and HyenaDNA have demonstrated strong performance on various genomic benchmarks. However, most applications rely on expensive fine-tuning, which works best when the training and test data share a similar distribution. In this work, we investigate whether task-specific fine-tuning is always necessary. We show that simple embedding-based pipelines that extract fixed representations from these models and feed them into lightweight classifiers can achieve competitive performance. In evaluation settings with different data distributions, embedding-based methods often outperform fine-tuning while reducing inference time by 10x to 20x. Our results suggest that embedding extraction is not only a strong baseline but also a more generalizable and efficient alternative to fine-tuning, especially for deployment in diverse or unseen genomic contexts. For example, in enhancer classification, HyenaDNA embeddings combined with zCurve achieve 0.68 accuracy (vs. 0.58 for fine-tuning), with an 88% reduction in inference time and over 8x lower carbon emissions (0.02 kg vs. 0.17 kg CO2). In non-TATA promoter classification, DNABERT-2 embeddings with zCurve or GC content reach 0.85 accuracy (vs. 0.89 with fine-tuning) with a 22x lower carbon footprint (0.02 kg vs. 0.44 kg CO2). These results show that embedding-based pipelines offer over 10x better carbon efficiency while maintaining strong predictive performance. The code is available here: https://github.com/NIRJHOR-DATTA/EMBEDDING-IS-ALMOST-ALL-YOU-NEED.",Bioinformatics,http://arxiv.org/abs/2508.04757v1,arXiv,1
"microRNA are pivotal post-transcriptional regulators whose single-cell behavior has remained largely inaccessible owing to technical barriers in single-cell small-RNA profiling. We present SiCmiR, a two-layer neural network that predicts miRNA expression profile from only 977 LINCS L1000 landmark genes reducing sensitivity to dropout of single-cell RNA-seq data. Proof-of-concept analyses illustrate how SiCmiR can uncover candidate hub-miRNAs in bulk-seq cell lines and hepatocellular carcinoma, scRNA-seq pancreatic ductal carcinoma and ACTH-secreting pituitary adenoma and extracellular-vesicle-mediated crosstalk in glioblastoma. Trained on 6462 TCGA paired miRNA-mRNA samples, SiCmiR attains state-of-the-art accuracy on held-out cancers and generalizes to unseen cancer types, drug perturbations and scRNA-seq. We next constructed SiCmiR-Atlas, containing 632 public datasets, 9.36 million cells, 726 cell types, which is the first dedicated database of single-cell mature miRNA expression--providing interactive visualization, biomarker identification and cell-type-resolved miRNA-target networks. SiCmiR transforms bulk-derived statistical power into a single-cell view of miRNA biology and provides a community resource SiCmiR Atlas for biomarker discovery. SiCmiR Atlas is avilable at https://awi.cuhk.edu.cn/~SiCmiR/.",Bioinformatics,http://arxiv.org/abs/2508.05692v1,arXiv,1
"Cell type annotation is a fundamental step in the analysis of single-cell RNA sequencing (scRNA-seq) data. In practice, human experts often rely on the structure revealed by principal component analysis (PCA) followed by $k$-nearest neighbor ($k$-NN) graph construction to guide annotation. While effective, this process is labor-intensive and does not scale to large datasets. Recent advances in CLIP-style models offer a promising path toward automating cell type annotation. By aligning scRNA-seq profiles with natural language descriptions, models like LangCell enable zero-shot annotation. While LangCell demonstrates decent zero-shot performance, its predictions remain suboptimal, particularly in achieving consistent accuracy across all cell types. In this paper, we propose to refine the zero-shot logits produced by LangCell through a graph-regularized optimization framework. By enforcing local consistency over the task-specific PCA-based k-NN graph, our method combines the scalability of the pre-trained models with the structural robustness relied upon in expert annotation. We evaluate our approach on 14 annotated human scRNA-seq datasets from 4 distinct studies, spanning 11 organs and over 200,000 single cells. Our method consistently improves zero-shot annotation accuracy, achieving accuracy gains of up to 10%. Further analysis showcase the mechanism by which GRIT effectively propagates correct signals through the graph, pulling back mislabeled cells toward more accurate predictions. The method is training-free, model-agnostic, and serves as a simple yet effective plug-in for enhancing automated cell type annotation in practice.",Bioinformatics,http://arxiv.org/abs/2508.04747v1,arXiv,1
"Understanding the molecular-level mechanisms underpinning Alzheimer's disease (AD) by studying crucial genes associated with the disease remains a challenge. Alzheimer's, being a multifactorial disease, requires understanding the gene-gene interactions underlying it for theranostics and progress. In this article, a novel attempt has been made using a quantum regression to decode how some crucial genes in the AD Amyloid Beta Precursor Protein ($APP$), Sterol regulatory element binding transcription factor 14 ($FGF14$), Yin Yang 1 ($YY1$), and Phospholipase D Family Member 3 ($PLD3$) etc. become influenced by other prominent switching genes during disease progression, which may help in gene expression-based therapy for AD. Our proposed Quantum Regression Network (Alz-QNet) introduces a pioneering approach with insights from the state-of-the-art Quantum Gene Regulatory Networks (QGRN) to unravel the gene interactions involved in AD pathology, particularly within the Entorhinal Cortex (EC), where early pathological changes occur. Using the proposed Alz-QNet framework, we explore the interactions between key genes ($APP$, $FGF14$, $YY1$, $EGR1$, $GAS7$, $AKT3$, $SREBF2$, and $PLD3$) within the CE microenvironment of AD patients, studying genetic samples from the database $GSE138852$, all of which are believed to play a crucial role in the progression of AD. Our investigation uncovers intricate gene-gene interactions, shedding light on the potential regulatory mechanisms that underlie the pathogenesis of AD, which help us to find potential gene inhibitors or regulators for theranostics.",Bioinformatics,http://arxiv.org/abs/2508.04743v1,arXiv,1
"Modern disease classification often overlooks molecular commonalities hidden beneath divergent clinical presentations. This study introduces a transcriptomics-driven framework for discovering disease relationships by analyzing over 1300 disease-condition pairs using GenoMAS, a fully automated agentic AI system. Beyond identifying robust gene-level overlaps, we develop a novel pathway-based similarity framework that integrates multi-database enrichment analysis to quantify functional convergence across diseases. The resulting disease similarity network reveals both known comorbidities and previously undocumented cross-category links. By examining shared biological pathways, we explore potential molecular mechanisms underlying these connections-offering functional hypotheses that go beyond symptom-based taxonomies. We further show how background conditions such as obesity and hypertension modulate transcriptomic similarity, and identify therapeutic repurposing opportunities for rare diseases like autism spectrum disorder based on their molecular proximity to better-characterized conditions. In addition, this work demonstrates how biologically grounded agentic AI can scale transcriptomic analysis while enabling mechanistic interpretation across complex disease landscapes. All results are publicly accessible at github.com/KeeeeChen/Pathway_Similarity_Network.",Bioinformatics,http://arxiv.org/abs/2508.04742v1,arXiv,1
"Genomic language models (gLMs) face a fundamental efficiency challenge: either maintain separate specialized models for each biological modality (DNA and RNA) or develop large multi-modal architectures. Both approaches impose significant computational burdens - modality-specific models require redundant infrastructure despite inherent biological connections, while multi-modal architectures demand massive parameter counts and extensive cross-modality pretraining. To address this limitation, we introduce CodonMoE (Adaptive Mixture of Codon Reformative Experts), a lightweight adapter that transforms DNA language models into effective RNA analyzers without RNA-specific pretraining. Our theoretical analysis establishes CodonMoE as a universal approximator at the codon level, capable of mapping arbitrary functions from codon sequences to RNA properties given sufficient expert capacity. Across four RNA prediction tasks spanning stability, expression, and regulation, DNA models augmented with CodonMoE significantly outperform their unmodified counterparts, with HyenaDNA+CodonMoE series achieving state-of-the-art results using 80% fewer parameters than specialized RNA models. By maintaining sub-quadratic complexity while achieving superior performance, our approach provides a principled path toward unifying genomic language modeling, leveraging more abundant DNA data and reducing computational overhead while preserving modality-specific performance advantages.",Bioinformatics,http://arxiv.org/abs/2508.04739v1,arXiv,1
"Feature selection (FS) is assumed to improve predictive performance and identify meaningful features in high-dimensional datasets. Surprisingly, small random subsets of features (0.02-1%) match or outperform the predictive performance of both full feature sets and FS across 28 out of 30 diverse datasets (microarray, bulk and single-cell RNA-Seq, mass spectrometry, imaging, etc.). In short, any arbitrary set of features is as good as any other (with surprisingly low variance in results) - so how can a particular set of selected features be ""important"" if they perform no better than an arbitrary set? These results challenge the assumption that computationally selected features reliably capture meaningful signals, emphasizing the importance of rigorous validation before interpreting selected features as actionable, particularly in computational genomics.",Bioinformatics,http://arxiv.org/abs/2508.03593v2,arXiv,1
"Network models provide a powerful framework for analysing single-cell count data, facilitating the characterisation of cellular identities, disease mechanisms, and developmental trajectories. However, uncertainty modeling in unsupervised learning with genomic data remains insufficiently explored. Conventional clustering methods assign a singular identity to each cell, potentially obscuring transitional states during differentiation or mutation. This study introduces a variational Bayesian framework for clustering and analysing single-cell genomic data, employing a Bayesian Gaussian mixture model to estimate the probabilistic association of cells with distinct clusters. This approach captures cellular transitions, yielding biologically coherent insights into neurogenesis and breast cancer progression. The inferred clustering probabilities enable further analyses, including Differential Expression Analysis and pseudotime analysis. Furthermore, we propose utilising the misclustering rate and Area Under the Curve in clustering scRNA-seq data as an innovative metric to quantitatively evaluate overall clustering performance. This methodological advancement enhances the resolution of single-cell data analysis, enabling a more nuanced characterisation of dynamic cellular identities in development and disease.",Bioinformatics,http://arxiv.org/abs/2508.02061v1,arXiv,1
"Spatial transcriptomics enables simultaneous measurement of gene expression and tissue morphology, offering unprecedented insights into cellular organization and disease mechanisms. However, the field lacks comprehensive benchmarks for evaluating multimodal learning methods that leverage both histology images and gene expression data. Here, we present HESCAPE, a large-scale benchmark for cross-modal contrastive pretraining in spatial transcriptomics, built on a curated pan-organ dataset spanning 6 different gene panels and 54 donors. We systematically evaluated state-of-the-art image and gene expression encoders across multiple pretraining strategies and assessed their effectiveness on two downstream tasks: gene mutation classification and gene expression prediction. Our benchmark demonstrates that gene expression encoders are the primary determinant of strong representational alignment, and that gene models pretrained on spatial transcriptomics data outperform both those trained without spatial data and simple baseline approaches. However, downstream task evaluation reveals a striking contradiction: while contrastive pretraining consistently improves gene mutation classification performance, it degrades direct gene expression prediction compared to baseline encoders trained without cross-modal objectives. We identify batch effects as a key factor that interferes with effective cross-modal alignment. Our findings highlight the critical need for batch-robust multimodal learning approaches in spatial transcriptomics. To accelerate progress in this direction, we release HESCAPE, providing standardized datasets, evaluation protocols, and benchmarking tools for the community",Bioinformatics,http://arxiv.org/abs/2508.01490v2,arXiv,1
"Pan-cancer classification using transcriptomic (RNA-Seq) data can inform tumor subtyping and therapy selection, but is challenging due to extremely high dimensionality and limited sample sizes. In this study, we propose a novel deep learning framework that uses a class-conditional variational autoencoder (cVAE) to augment training data for pan-cancer gene expression classification. Using 801 tumor RNA-Seq samples spanning 5 cancer types from The Cancer Genome Atlas (TCGA), we first perform feature selection to reduce 20,531 gene expression features to the 500 most variably expressed genes. A cVAE is then trained on this data to learn a latent representation of gene expression conditioned on cancer type, enabling the generation of synthetic gene expression samples for each tumor class. We augment the training set with these cVAE-generated samples (doubling the dataset size) to mitigate overfitting and class imbalance. A two-layer multilayer perceptron (MLP) classifier is subsequently trained on the augmented dataset to predict tumor type. The augmented framework achieves high classification accuracy (~98%) on a held-out test set, substantially outperforming a classifier trained on the original data alone. We present detailed experimental results, including VAE training curves, classifier performance metrics (ROC curves and confusion matrix), and architecture diagrams to illustrate the approach. The results demonstrate that cVAE-based synthetic augmentation can significantly improve pan-cancer prediction performance, especially for underrepresented cancer classes.",Bioinformatics,http://arxiv.org/abs/2508.02743v1,arXiv,1
"CARTEpigenoQC is an R-based toolkit designed to streamline quality control (QC) for single-cell epigenomic datasets involving Chimeric Antigen Receptor (CAR)-engineered T cells. With the growing application of scATAC-seq, scCUT&Tag, and scBS-seq to characterize CAR-T cell states, it has become critical to perform customized QC that not only addresses standard metrics like FRiP (Fraction of Reads in Peaks) and TSS enrichment, but also directly detects signal from CAR vector insertion sites. CARTEpigenoQC supports both 10x Genomics and non-10x data formats and produces HTML and PNG summary outputs suited for exploratory analysis and regulatory-grade preclinical reporting. It is intended to assist researchers, core facilities, and translational immunologists in ensuring the validity of single-cell epigenomic profiling of engineered T cells.",Bioinformatics,http://arxiv.org/abs/2507.23048v1,arXiv,1
"Distinguishing pathogenic mutations from benign polymorphisms remains a critical challenge in precision medicine. EnTao-GPM, developed by Fudan University and BioMap, addresses this through three innovations: (1) Cross-species targeted pre-training on disease-relevant mammalian genomes (human, pig, mouse), leveraging evolutionary conservation to enhance interpretation of pathogenic motifs, particularly in non-coding regions; (2) Germline mutation specialization via fine-tuning on ClinVar and HGMD, improving accuracy for both SNVs and non-SNVs; (3) Interpretable clinical framework integrating DNA sequence embeddings with LLM-based statistical explanations to provide actionable insights. Validated against ClinVar, EnTao-GPM demonstrates superior accuracy in mutation classification. It revolutionizes genetic testing by enabling faster, more accurate, and accessible interpretation for clinical diagnostics (e.g., variant assessment, risk identification, personalized treatment) and research, advancing personalized medicine.",Bioinformatics,http://arxiv.org/abs/2507.21706v1,arXiv,1
"Gene expression analysis holds the key to many biomedical discoveries, yet extracting insights from raw transcriptomic data remains formidable due to the complexity of multiple large, semi-structured files and the need for extensive domain expertise. Current automation approaches are often limited by either inflexible workflows that break down in edge cases or by fully autonomous agents that lack the necessary precision for rigorous scientific inquiry. GenoMAS charts a different course by presenting a team of LLM-based scientists that integrates the reliability of structured workflows with the adaptability of autonomous agents. GenoMAS orchestrates six specialized LLM agents through typed message-passing protocols, each contributing complementary strengths to a shared analytic canvas. At the heart of GenoMAS lies a guided-planning framework: programming agents unfold high-level task guidelines into Action Units and, at each juncture, elect to advance, revise, bypass, or backtrack, thereby maintaining logical coherence while bending gracefully to the idiosyncrasies of genomic data.   On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation of 89.13% for data preprocessing and an F$_1$ of 60.48% for gene identification, surpassing the best prior art by 10.61% and 16.85% respectively. Beyond metrics, GenoMAS surfaces biologically plausible gene-phenotype associations corroborated by the literature, all while adjusting for latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.",Bioinformatics,http://arxiv.org/abs/2507.21035v2,arXiv,1
"Differential expression (DE) analysis is a key task in RNA-seq studies, aiming to identify genes with expression differences across conditions. A central challenge is balancing false discovery rate (FDR) control with statistical power. Parametric methods such as DESeq2 and edgeR achieve high power by modeling gene-level counts using negative binomial distributions and applying empirical Bayes shrinkage. However, these methods may suffer from FDR inflation when model assumptions are mildly violated, especially in large-sample settings. In contrast, non-parametric tests like Wilcoxon offer more robust FDR control but often lack power and do not support covariate adjustment. We propose Nullstrap-DE, a general add-on framework that combines the strengths of both approaches. Designed to augment tools like DESeq2 and edgeR, Nullstrap-DE calibrates FDR while preserving power, without modifying the original method's implementation. It generates synthetic null data from a model fitted under the gene-specific null (no DE), applies the same test statistic to both observed and synthetic data, and derives a threshold that satisfies the target FDR level. We show theoretically that Nullstrap-DE asymptotically controls FDR while maintaining power consistency. Simulations confirm that it achieves reliable FDR control and high power across diverse settings, where DESeq2, edgeR, or Wilcoxon often show inflated FDR or low power. Applications to real datasets show that Nullstrap-DE enhances statistical rigor and identifies biologically meaningful genes.",Bioinformatics,http://arxiv.org/abs/2507.20598v1,arXiv,1
"Multi-omics data offer unprecedented insights into complex biological systems, yet their high dimensionality, sparsity, and intricate interactions pose significant analytical challenges. Network-based approaches have advanced multi-omics research by effectively capturing biologically relevant relationships among molecular entities. While these methods are powerful for representing molecular interactions, there remains a need for tools specifically designed to effectively utilize these network representations across diverse downstream analyses. To fulfill this need, we introduce BioNeuralNet, a flexible and modular Python framework tailored for end-to-end network-based multi-omics data analysis. BioNeuralNet leverages Graph Neural Networks (GNNs) to learn biologically meaningful low-dimensional representations from multi-omics networks, converting these complex molecular networks into versatile embeddings. BioNeuralNet supports all major stages of multi-omics network analysis, including several network construction techniques, generation of low-dimensional representations, and a broad range of downstream analytical tasks. Its extensive utilities, including diverse GNN architectures, and compatibility with established Python packages (e.g., scikit-learn, PyTorch, NetworkX), enhance usability and facilitate quick adoption. BioNeuralNet is an open-source, user-friendly, and extensively documented framework designed to support flexible and reproducible multi-omics network analysis in precision medicine.",Bioinformatics,http://arxiv.org/abs/2507.20440v1,arXiv,1
"The modeling of genomic sequences presents unique challenges due to their length and structural complexity. Traditional sequence models struggle to capture long-range dependencies and biological features inherent in DNA. In this work, we propose TrinityDNA, a novel DNA foundational model designed to address these challenges. The model integrates biologically informed components, including Groove Fusion for capturing DNA's structural features and Gated Reverse Complement (GRC) to handle the inherent symmetry of DNA sequences. Additionally, we introduce a multi-scale attention mechanism that allows the model to attend to varying levels of sequence dependencies, and an evolutionary training strategy that progressively adapts the model to both prokaryotic and eukaryotic genomes. TrinityDNA provides a more accurate and efficient approach to genomic sequence modeling, offering significant improvements in gene function prediction, regulatory mechanism discovery, and other genomics applications. Our model bridges the gap between machine learning techniques and biological insights, paving the way for more effective analysis of genomic data. Additionally, we introduced a new DNA long-sequence CDS annotation benchmark to make evaluations more comprehensive and oriented toward practical applications.",Bioinformatics,http://arxiv.org/abs/2507.19229v1,arXiv,1
"The exponential growth of DNA sequencing data has outpaced traditional heuristic-based methods, which struggle to scale effectively. Efficient computational approaches are urgently needed to support large-scale similarity search, a foundational task in bioinformatics for detecting homology, functional similarity, and novelty among genomic and proteomic sequences. Although tools like BLAST have been widely used and remain effective in many scenarios, they suffer from limitations such as high computational cost and poor performance on divergent sequences.   In this work, we explore embedding-based similarity search methods that learn latent representations capturing deeper structural and functional patterns beyond raw sequence alignment. We systematically evaluate two state-of-the-art vector search libraries, FAISS and ScaNN, on biologically meaningful gene embeddings. Unlike prior studies, our analysis focuses on bioinformatics-specific embeddings and benchmarks their utility for detecting novel sequences, including those from uncharacterized taxa or genes lacking known homologs. Our results highlight both computational advantages (in memory and runtime efficiency) and improved retrieval quality, offering a promising alternative to traditional alignment-heavy tools.",Bioinformatics,http://arxiv.org/abs/2507.16978v1,arXiv,1
"Functional annotation of microbial genomes is often biased toward protein-coding genes, leaving a vast, unexplored landscape of non-coding RNAs (ncRNAs) that are critical for regulating bacterial and archaeal physiology, stress response and metabolism. Identifying ncRNAs directly from genomic sequence is a paramount challenge in bioinformatics and biology, essential for understanding the complete regulatory potential of an organism. This paper presents RNAMunin, a machine learning (ML) model that is capable of finding ncRNAs using genomic sequence alone. It is also computationally viable for large sequence datasets such as long read metagenomic assemblies with contigs totaling multiple Gbp. RNAMunin is trained on Rfam sequences extracted from approximately 60 Gbp of long read metagenomes from 16 San Francisco Estuary samples. We know of no other model that can detect ncRNAs based solely on genomic sequence at this scale. Since RNAMunin only requires genomic sequence as input, we do not need for an ncRNA to be transcribed to find it, i.e., we do not need transcriptomics data. We wrote this manuscript in a narrative style in order to best convey how RNAMunin was developed and how it works in detail. Unlike almost all current ML models, at approximately 1M parameters, RNAMunin is very small and very fast.",Bioinformatics,http://arxiv.org/abs/2507.11950v1,arXiv,1
"Spatial Transcriptomics (ST) technologies provide biologists with rich insights into single-cell biology by preserving spatial context of cells. Building foundational models for ST can significantly enhance the analysis of vast and complex data sources, unlocking new perspectives on the intricacies of biological tissues. However, modeling ST data is inherently challenging due to the need to extract multi-scale information from tissue slices containing vast numbers of cells. This process requires integrating macro-scale tissue morphology, micro-scale cellular microenvironment, and gene-scale gene expression profile. To address this challenge, we propose SToFM, a multi-scale Spatial Transcriptomics Foundation Model. SToFM first performs multi-scale information extraction on each ST slice, to construct a set of ST sub-slices that aggregate macro-, micro- and gene-scale information. Then an SE(2) Transformer is used to obtain high-quality cell representations from the sub-slices. Additionally, we construct \textbf{SToCorpus-88M}, the largest high-resolution spatial transcriptomics corpus for pretraining. SToFM achieves outstanding performance on a variety of downstream tasks, such as tissue region semantic segmentation and cell type annotation, demonstrating its comprehensive understanding of ST data through capturing and integrating multi-scale information.",Bioinformatics,http://arxiv.org/abs/2507.11588v2,arXiv,1
"In this paper, we consider a tree inference problem motivated by the critical problem in single-cell genomics of reconstructing dynamic cellular processes from sequencing data. In particular, given a population of cells sampled from such a process, we are interested in the problem of ordering the cells according to their progression in the process. This is known as trajectory inference. If the process is differentiation, this amounts to reconstructing the corresponding differentiation tree. One way of doing this in practice is to estimate the shortest-path distance between nodes based on cell similarities observed in sequencing data. Recent sequencing techniques make it possible to measure two types of data: gene expression levels, and RNA velocity, a vector that predicts changes in gene expression. The data then consist of a discrete vector field on a (subset of a) Euclidean space of dimension equal to the number of genes under consideration. By integrating this velocity field, we trace the evolution of gene expression levels in each single cell from some initial stage to its current stage. Eventually, we assume that we have a faithful embedding of the differentiation tree in a Euclidean space, but which we only observe through the curves representing the paths from the root to the nodes. Using varifold distances between such curves, we define a similarity measure between nodes which we prove approximates the shortest-path distance in a tree that is isomorphic to the target tree.",Bioinformatics,http://arxiv.org/abs/2507.11313v1,arXiv,1
"Single-cell foundation models such as scGPT represent a significant advancement in single-cell omics, with an ability to achieve state-of-the-art performance on various downstream biological tasks. However, these models are inherently limited in that a vast amount of information in biology exists as text, which they are unable to leverage. There have therefore been several recent works that propose the use of LLMs as an alternative to single-cell foundation models, achieving competitive results. However, there is little understanding of what factors drive this performance, along with a strong focus on using LLMs as an alternative, rather than complementary approach to single-cell foundation models. In this study, we therefore investigate what biological insights contribute toward the performance of LLMs when applied to single-cell data, and introduce scMPT; a model which leverages synergies between scGPT, and single-cell representations from LLMs that capture these insights. scMPT demonstrates stronger, more consistent performance than either of its component models, which frequently have large performance gaps between each other across datasets. We also experiment with alternate fusion methods, demonstrating the potential of combining specialized reasoning models with scGPT to improve performance. This study ultimately showcases the potential for LLMs to complement single-cell foundation models and drive improvements in single-cell analysis.",Bioinformatics,http://arxiv.org/abs/2507.10039v1,arXiv,1
"SimOmics is an R package designed to generate realistic, multivariate, and multi-omics synthetic datasets. It is intended for use in benchmarking, method development, and reproducibility in bioinformatics, particularly in the context of omics integration tasks such as those encountered in transcriptomics, proteomics, and metabolomics. SimOmics supports latent factor simulation, sparsity structures, block-wise covariance modeling, and biologically inspired noise models and feature dimensions.",Bioinformatics,http://arxiv.org/abs/2507.09967v1,arXiv,1
"Clustering analysis is fundamental in single-cell RNA sequencing (scRNA-seq) data analysis for elucidating cellular heterogeneity and diversity. Recent graph-based scRNA-seq clustering methods, particularly graph neural networks (GNNs), have significantly improved in tackling the challenges of high-dimension, high-sparsity, and frequent dropout events that lead to ambiguous cell population boundaries. However, their reliance on hard graph constructions derived from thresholded similarity matrices presents challenges:(i) The simplification of intercellular relationships into binary edges (0 or 1) by applying thresholds, which restricts the capture of continuous similarity features among cells and leads to significant information loss.(ii) The presence of significant inter-cluster connections within hard graphs, which can confuse GNN methods that rely heavily on graph structures, potentially causing erroneous message propagation and biased clustering outcomes. To tackle these challenges, we introduce scSGC, a Soft Graph Clustering for single-cell RNA sequencing data, which aims to more accurately characterize continuous similarities among cells through non-binary edge weights, thereby mitigating the limitations of rigid data structures. The scSGC framework comprises three core components: (i) a zero-inflated negative binomial (ZINB)-based feature autoencoder; (ii) a dual-channel cut-informed soft graph embedding module; and (iii) an optimal transport-based clustering optimization module. Extensive experiments across ten datasets demonstrate that scSGC outperforms 13 state-of-the-art clustering models in clustering accuracy, cell type annotation, and computational efficiency. These results highlight its substantial potential to advance scRNA-seq data analysis and deepen our understanding of cellular heterogeneity.",Bioinformatics,http://arxiv.org/abs/2507.09890v1,arXiv,1
"Transcription Factor Binding Site (TFBS) prediction is crucial for understanding gene regulation and various biological processes. This study introduces a novel Mixture of Experts (MoE) approach for TFBS prediction, integrating multiple pre-trained Convolutional Neural Network (CNN) models, each specializing in different TFBS patterns. We evaluate the performance of our MoE model against individual expert models on both in-distribution and out-of-distribution (OOD) datasets, using six randomly selected transcription factors (TFs) for OOD testing. Our results demonstrate that the MoE model achieves competitive or superior performance across diverse TF binding sites, particularly excelling in OOD scenarios. The Analysis of Variance (ANOVA) statistical test confirms the significance of these performance differences. Additionally, we introduce ShiftSmooth, a novel attribution mapping technique that provides more robust model interpretability by considering small shifts in input sequences. Through comprehensive explainability analysis, we show that ShiftSmooth offers superior attribution for motif discovery and localization compared to traditional Vanilla Gradient methods. Our work presents an efficient, generalizable, and interpretable solution for TFBS prediction, potentially enabling new discoveries in genome biology and advancing our understanding of transcriptional regulation.",Bioinformatics,http://arxiv.org/abs/2507.09754v2,arXiv,1
"Transformers have revolutionized nucleotide sequence analysis, yet capturing long-range dependencies remains challenging. Recent studies show that autoregressive transformers often exhibit Markovian behavior by relying on fixed-length context windows for next-token prediction. However, standard self-attention mechanisms are computationally inefficient for long sequences due to their quadratic complexity and do not explicitly enforce global transition consistency.   We introduce CARMANIA (Context-Aware Regularization with Markovian Integration for Attention-Based Nucleotide Analysis), a self-supervised pretraining framework that augments next-token (NT) prediction with a transition-matrix (TM) loss. The TM loss aligns predicted token transitions with empirically derived n-gram statistics from each input sequence, encouraging the model to capture higher-order dependencies beyond local context. This integration enables CARMANIA to learn organism-specific sequence structures that reflect both evolutionary constraints and functional organization.   We evaluate CARMANIA across diverse genomic tasks, including regulatory element prediction, functional gene classification, taxonomic inference, antimicrobial resistance detection, and biosynthetic gene cluster classification. CARMANIA outperforms the previous best long-context model by at least 7 percent, matches state-of-the-art on shorter sequences (exceeding prior results on 20 out of 40 tasks while running approximately 2.5 times faster), and shows particularly strong improvements on enhancer and housekeeping gene classification tasks, including up to a 34 percent absolute gain in Matthews correlation coefficient (MCC) for enhancer prediction. The TM loss boosts accuracy in 33 of 40 tasks, especially where local motifs or regulatory patterns drive prediction.",Bioinformatics,http://arxiv.org/abs/2507.09378v3,arXiv,1
"AMRScan is a hybrid bioinformatics toolkit implemented in both R and [Nextflow](https://www.nextflow.io/) for the rapid and reproducible detection of antimicrobial resistance (AMR) genes from next-generation sequencing (NGS) data. The toolkit enables users to identify AMR gene hits in sequencing reads by aligning them against reference databases such as CARD using BLAST.   The R implementation provides a concise, script-based approach suitable for single-sample analysis, teaching, and rapid prototyping. In contrast, the Nextflow implementation enables reproducible, scalable workflows for multi-sample batch processing in high-performance computing (HPC) and containerized environments. It leverages modular pipeline design with support for automated database setup, quality control, conversion, BLAST alignment, and results parsing.   AMRScan helps bridge the gap between lightweight exploratory analysis and production-ready surveillance pipelines, making it suitable for both research and public health genomics applications.",Bioinformatics,http://arxiv.org/abs/2507.08062v1,arXiv,1
"MicroTrace is an open-source R tool that performs SNP-based hierarchical clustering to detect potential transmission clusters from pathogen whole-genome sequencing (WGS) data. Designed for epidemiologists, microbiologists, and genomic surveillance teams, it processes SNP distance matrices and outputs dendrograms and cluster tables with optional metadata integration. MicroTrace enables reproducible outbreak detection workflows with minimal setup.",Bioinformatics,http://arxiv.org/abs/2507.08060v1,arXiv,1
"HybridQC is an R package that streamlines quality control (QC) of single-cell RNA sequencing (scRNA-seq) data by combining traditional threshold-based filtering with machine learning-based outlier detection. It provides an efficient and adaptive framework to identify low-quality cells in noisy or shallow-depth datasets using techniques such as Isolation Forest, while remaining compatible with widely adopted formats such as Seurat objects.   The package is lightweight, easy to install, and suitable for small-to-medium scRNA-seq datasets in research settings. HybridQC is especially useful for projects involving non-model organisms, rare samples, or pilot studies, where automated and flexible QC is critical for reproducibility and downstream analysis.",Bioinformatics,http://arxiv.org/abs/2507.08058v1,arXiv,1
"Genetic transfers are pervasive across both prokaryotes and eukaryotes, encompassing canonical genomic introgression between species or genera and horizontal gene transfer (HGT) across kingdoms. However, DNA transfer between phylogenetically distant species, here defined as remote introgression (RI), has remained poorly explored in evolutionary genomics. In this study, we present RIFinder, a novel phylogeny-based method for RI event detection, and apply it to a comprehensive dataset of 122 grass genomes. Our analysis identifies 622 RI events originating from 543 distinct homologous genes, revealing distinct characteristics among grass subfamilies. Specifically, the subfamily Pooideae exhibits the highest number of introgressed genes while Bambusoideae contains the lowest. Comparisons among accepted genes, their donor copies and native homologs demonstrate that introgressed genes undergo post-transfer localized adaptation, with significant functional enrichment in stress-response pathways. Notably, we identify a large Triticeae-derived segment in a Chloridoideae species Cleistogenes songorica, which is potentially associated with its exceptional drought tolerance. Furthermore, we provide compelling evidence that RI has contributed to the origin and diversification of biosynthetic gene clusters of gramine, a defensive alkaloid chemical, across grass species. Collectively, our study establishes a robust method for RI detection and highlights its critical role in adaptive evolution.",Bioinformatics,http://arxiv.org/abs/2507.07761v1,arXiv,1
"Single-cell RNA sequencing (scRNA-seq) enables single-cell transcriptomic profiling, revealing cellular heterogeneity and rare populations. Recent deep learning models like Geneformer and Mouse-Geneformer perform well on tasks such as cell-type classification and in silico perturbation. However, their species-specific design limits cross-species generalization and translational applications, which are crucial for advancing translational research and drug discovery. We present Mix-Geneformer, a novel Transformer-based model that integrates human and mouse scRNA-seq data into a unified representation via a hybrid self-supervised approach combining Masked Language Modeling (MLM) and SimCSE-based contrastive loss to capture both shared and species-specific gene patterns. A rank-value encoding scheme further emphasizes high-variance gene signals during training. Trained on about 50 million cells from diverse human and mouse organs, Mix-Geneformer matched or outperformed state-of-the-art baselines in cell-type classification and in silico perturbation tasks, achieving 95.8% accuracy on mouse kidney data versus 94.9% from the best existing model. It also successfully identified key regulatory genes validated by in vivo studies. By enabling scalable cross-species transcriptomic modeling, Mix-Geneformer offers a powerful tool for comparative transcriptomics and translational applications. While our results demonstrate strong performance, we also acknowledge limitations, such as the computational cost and variability in zero-shot transfer.",Bioinformatics,http://arxiv.org/abs/2507.07454v1,arXiv,1
"Sequence data, such as DNA, RNA, and protein sequences, exhibit intricate, multi-scale structures that pose significant challenges for conventional analysis methods, particularly those relying on alignment or purely statistical representations. In this work, we introduce category-based topological sequence analysis (CTSA ) of genomes. CTSA models a sequence as a resolution category, capturing its hierarchical structure through a categorical construction. Substructure complexes are then derived from this categorical representation, and their persistent homology is computed to extract multi-scale topological features. Our models depart from traditional alignment-free approaches by incorporating structured mathematical formalisms rooted in sequence topology. The resulting topological signatures provide informative representations across a variety of tasks, including the phylogenetic analysis of SARS-CoV-2 variants and the prediction of protein-nucleic acid binding affinities. Comparative studies were carried out against six state-of-the-art methods. Experimental results demonstrate that CTSA achieves excellent and consistent performance in these tasks, suggesting its general applicability and robustness. Beyond sequence analysis, the proposed framework opens new directions for the integration of categorical and homological theories for biological sequence analysis.",Bioinformatics,http://arxiv.org/abs/2507.08043v2,arXiv,1
"Single-cell RNA sequencing (scRNA-seq) has revolutionized the study of cellular heterogeneity, enabling detailed molecular profiling at the individual cell level. However, integrating high-dimensional single-cell data into causal mediation analysis remains challenging due to zero inflation and complex mediator structures. We propose a novel mediation framework leveraging zero-inflated negative binomial models to characterize cell-level mediator distributions and beta regression for zero-inflation proportions. The model can identify expression level as well as expressed proportion that could mediate disease-leading causal pathway. Extensive simulation studies demonstrate improved power and controlled false discovery rates. We further illustrate the utility of this approach through application to ROSMAP single-cell transcriptomic data, uncovering biologically meaningful mediation effects that enhance understanding of disease mechanisms.",Bioinformatics,http://arxiv.org/abs/2507.06113v3,arXiv,1
"T cell receptor (TCR) repertoires encode critical immunological signatures for autoimmune diseases, yet their clinical application remains limited by sequence sparsity and low witness rates. We developed EAMil, a multi-instance deep learning framework that leverages TCR sequencing data to diagnose systemic lupus erythematosus (SLE) and rheumatoid arthritis (RA) with exceptional accuracy. By integrating PrimeSeq feature extraction with ESMonehot encoding and enhanced gate attention mechanisms, our model achieved state-of-the-art performance with AUCs of 98.95% for SLE and 97.76% for RA. EAMil successfully identified disease-associated genes with over 90% concordance with established differential analyses and effectively distinguished disease-specific TCR genes. The model demonstrated robustness in classifying multiple disease categories, utilizing the SLEDAI score to stratify SLE patients by disease severity as well as to diagnose the site of damage in SLE patients, and effectively controlling for confounding factors such as age and gender. This interpretable framework for immune receptor analysis provides new insights for autoimmune disease detection and classification with broad potential clinical applications across immune-mediated conditions.",Bioinformatics,http://arxiv.org/abs/2507.04981v4,arXiv,1
"Graph Neural Networks (GNNs) and Transformers share significant similarities in their encoding strategies for interacting with features from nodes of interest, where Transformers use query-key scores and GNNs use edges. Compared to GNNs, which are unable to encode relative positions, Transformers leverage dynamic attention capabilities to better represent relative relationships, thereby becoming the standard backbones in large-scale sequential pre-training. However, the subtle difference prompts us to consider: if positions are no longer crucial, could we substitute Transformers with Graph Neural Networks in some fields such as Single-Cell Transcriptomics? In this paper, we first explore the similarities and differences between GNNs and Transformers, specifically in terms of relative positions. Additionally, we design a synthetic example to illustrate their equivalence where there are no relative positions between tokens in the sample. Finally, we conduct extensive experiments on a large-scale position-agnostic dataset-single-cell transcriptomics-finding that GNNs achieve competitive performance compared to Transformers while consuming fewer computation resources. These findings provide novel insights for researchers in the field of single-cell transcriptomics, challenging the prevailing notion that the Transformer is always the optimum choice.",Bioinformatics,http://arxiv.org/abs/2507.04125v1,arXiv,1
"Background: While benchmarks on short-read variant calling suggest low error rate below 0.5%, they are only applicable to predefined confident regions. For a human sample without such regions, the error rate could be 10 times higher. Although multiple sets of easy regions have been identified to alleviate the issue, they fail to consider non-reference samples or are biased towards existing short-read data or aligners.   Results: Here, using hundreds of high-quality human assemblies, we derived a set of sample-agnostic easy regions where short-read variant calling reaches high accuracy. These regions cover 88.2% of GRCh38, 92.2% of coding regions and 96.3% of ClinVar pathogenic variants. They achieve a good balance between coverage and easiness and can be generated for other human assemblies or species with multiple well assembled genomes.   Conclusion: This resource provides a convient and powerful way to filter spurious variant calls for clinical or research human samples.",Bioinformatics,http://arxiv.org/abs/2507.03718v2,arXiv,1
"Predicting drug responses using genetic and transcriptomic features is crucial for enhancing personalized medicine. In this study, we implemented an ensemble of machine learning algorithms to analyze the correlation between genetic and transcriptomic features of cancer cell lines and IC50 values, a reliable metric for drug efficacy. Our analysis involved a reduction of the feature set from an original pool of 38,977 features, demonstrating a strong linear relationship between genetic features and drug responses across various algorithms, including SVR, Linear Regression, and Ridge Regression. Notably, copy number variations (CNVs) emerged as more predictive than mutations, suggesting a significant reevaluation of biomarkers for drug response prediction. Through rigorous statistical methods, we identified a highly reduced set of 421 critical features. This set offers a novel perspective that contrasts with traditional cancer driver genes, underscoring the potential for these biomarkers in designing targeted therapies. Furthermore, our findings advocate for IC50 values as a predictable measurement of drug responses and underscore the need for more data that can represent the dimensionality of genomic data in drug response prediction. Future work will aim to expand the dataset and refine feature selection to enhance the generalizability of the predictive model in clinical settings.",Bioinformatics,http://arxiv.org/abs/2507.02818v1,arXiv,1
"We train a neural network to predict distributional responses in gene expression following genetic perturbations. This is an essential task in early-stage drug discovery, where such responses can offer insights into gene function and inform target identification. Existing methods only predict changes in the mean expression, overlooking stochasticity inherent in single-cell data. In contrast, we offer a more realistic view of cellular responses by modeling expression distributions. Our model predicts gene-level histograms conditioned on perturbations and outperforms baselines in capturing higher-order statistics, such as variance, skewness, and kurtosis, at a fraction of the training cost. To generalize to unseen perturbations, we incorporate prior knowledge via gene embeddings from large language models (LLMs). While modeling a richer output space, the method remains competitive in predicting mean expression changes. This work offers a practical step towards more expressive and biologically informative models of perturbation effects.",Bioinformatics,http://arxiv.org/abs/2507.02980v1,arXiv,1
"Recent multi-omic microbiome studies enable integrative analysis of microbes and metabolites, uncovering their associations with various host conditions. Such analyses require multivariate models capable of accounting for the complex correlation structures between microbes and metabolites. However, existing multivariate models often suffer from low statistical power for detecting microbiome-metabolome interactions due to small sample sizes and weak biological signals. To address these challenges, we introduce CoMMiT, Co-informed inference of Microbiome-Metabolome Interactions via novel Transfer learning models. Unlike conventional transfer-learning methods that borrow information from external datasets, CoMMiT leverages similarities across metabolites within a single cohort, reducing the risk of negative transfer often caused by differences in sequencing platforms and bioinformatic pipelines across studies. CoMMiT operates under the flexible assumption that auxiliary metabolites are collectively informative for the target metabolite, without requiring individual auxiliary metabolites to be informative. CoMMiT uses a novel data-driven approach to selecting the optimal set of auxiliary metabolites. Using this optimal set, CoMMiT employs a de-biasing framework to enable efficient calculation of p-values, facilitating the identification of statistically significant microbiome-metabolome interactions. Applying CoMMiT to a feeding study reveals biologically meaningful microbiome-metabolome interactions under a low glycemic load diet, demonstrating the diet-host link through gut metabolism.",Bioinformatics,http://arxiv.org/abs/2506.24013v1,arXiv,1
"Cancer is a genetic disorder whose clonal evolution can be monitored by tracking noisy genome-wide copy number variants. We introduce the Copy Number Stochastic Block Model (CN-SBM), a probabilistic framework that jointly clusters samples and genomic regions based on discrete copy number states using a bipartite categorical block model. Unlike models relying on Gaussian or Poisson assumptions, CN-SBM respects the discrete nature of CNV calls and captures subpopulation-specific patterns through block-wise structure. Using a two-stage approach, CN-SBM decomposes CNV data into primary and residual components, enabling detection of both large-scale chromosomal alterations and finer aberrations. We derive a scalable variational inference algorithm for application to large cohorts and high-resolution data. Benchmarks on simulated and real datasets show improved model fit over existing methods. Applied to TCGA low-grade glioma data, CN-SBM reveals clinically relevant subtypes and structured residual variation, aiding patient stratification in survival analysis. These results establish CN-SBM as an interpretable, scalable framework for CNV analysis with direct relevance for tumor heterogeneity and prognosis.",Bioinformatics,http://arxiv.org/abs/2506.22963v1,arXiv,1
"A key challenge in learning from multimodal biological data is missing modalities, where all data from some modalities are missing for some patients. Current fusion methods address this by excluding patients with missing modalities, imputing missing modalities, or making predictions directly with partial modalities. However, they often struggle with diverse missing-modality patterns and the exponential growth of the number of such patterns as the number of modalities increases. To address these limitations, we propose MAGNET (Missing-modality-Aware Graph neural NETwork) for direct prediction with partial modalities, which introduces a patient-modality multi-head attention mechanism to fuse lower-dimensional modality embeddings based on their importance and missingness. MAGNET's complexity increases linearly with the number of modalities while adapting to missing-pattern variability. To generate predictions, MAGNET further constructs a patient graph with fused multimodal embeddings as node features and the connectivity determined by the modality missingness, followed by a conventional graph neural network. Experiments on three public multiomics datasets for cancer classification, with real-world instead of artificial missingness, show that MAGNET outperforms the state-of-the-art fusion methods. The data and code are available at https://github.com/SinaTabakhi/MAGNET.",Bioinformatics,http://arxiv.org/abs/2506.22901v1,arXiv,1
"Recent benchmarks reveal that models for single-cell perturbation response are often outperformed by simply predicting the dataset mean. We trace this anomaly to a metric artifact: control-referenced deltas and unweighted error metrics reward mode collapse whenever the control is biased or the biological signal is sparse. Large-scale \textit{in silico} simulations and analysis of two real-world perturbation datasets confirm that shared reference shifts, not genuine biological change, drives high performance in these evaluations. We introduce differentially expressed gene (DEG)-aware metrics, weighted mean-squared error (WMSE) and weighted delta $R^{2}$ ($R^{2}_{w}(Î)$) with respect to all perturbations, that measure error in niche signals with high sensitivity. We further introduce negative and positive performance baselines to calibrate these metrics. With these improvements, the mean baseline sinks to null performance while genuine predictors are correctly rewarded. Finally, we show that using WMSE as a loss function reduces mode collapse and improves model performance.",Bioinformatics,http://arxiv.org/abs/2506.22641v1,arXiv,1
"Hepatocellular carcinoma (HCC) is one of the leading causes of cancer-related deaths worldwide. Several diagnostic methods, such as imaging modalities and Serum Alpha-Fetoprotein (AFP) testing, have been used for HCC detection; however, their effectiveness is limited to later stages of the disease. In contrast, transcriptomic analysis of biposy samples has shown promise for early detection. While machine learning techniques have been applied to transcriptomic data for cancer detection, their clinical adoption remains limited due to challenges such as poor generalizability across different datasets, lack of interpretability, and high computational complexity. To address these limitations, we developed a novel predictive formula for HCC detection using the Kolmogorov-Arnold Network (KAN). This formula is based on the expression levels of five genes: VIPR1, CYP1A2, FCN3, ECM1, and LIFR. Derived from the GSE25097 dataset, the formula offers a simple, interpretable, efficient, and accessible approach for HCC identification. It achieves 99% accuracy on the GSE25097 test set and demonstrates robust performance on six additional independent datasets, achieving accuracies of above 90% in all cases. These findings highlight the critical role of these five genes as biomarkers for HCC detection, offering a foundation for future research and clinical applications to improve HCC diagnostic approaches.",Bioinformatics,http://arxiv.org/abs/2507.00154v1,arXiv,1
"Single-cell sequencing is revolutionizing biology by enabling detailed investigations of cell-state transitions. Many biological processes unfold along continuous trajectories, yet it remains challenging to extract smooth, low-dimensional representations from inherently noisy, high-dimensional single-cell data. Neighbor embedding (NE) algorithms, such as t-SNE and UMAP, are widely used to embed high-dimensional single-cell data into low dimensions. But they often introduce undesirable distortions, resulting in misleading interpretations. Existing evaluation methods for NE algorithms primarily focus on separating discrete cell types rather than capturing continuous cell-state transitions, while dynamic modeling approaches rely on strong assumptions about cellular processes and specialized data. To address these challenges, we build on the Predictability-Computability-Stability (PCS) framework for reliable and reproducible data-driven discoveries. First, we systematically evaluate popular NE algorithms through empirical analysis, simulation, and theory, and reveal their key shortcomings, such as artifacts and instability. We then introduce NESS, a principled and interpretable machine learning approach to improve NE representations by leveraging algorithmic stability and to enable robust inference of smooth biological structures. NESS offers useful concepts, quantitative stability metrics, and efficient computational workflows to uncover developmental trajectories and cell-state transitions in single-cell data. Finally, we apply NESS to six single-cell datasets, spanning pluripotent stem cell differentiation, organoid development, and multiple tissue-specific lineage trajectories. Across these diverse contexts, NESS consistently yields useful biological insights, such as identification of transitional and stable cell states and quantification of transcriptional dynamics during development.",Bioinformatics,http://arxiv.org/abs/2506.22228v1,arXiv,1
"Large language models (LLMs) trained on text demonstrated remarkable results on natural language processing (NLP) tasks. These models have been adapted to decipher the language of DNA, where sequences of nucleotides act as ""words"" that encode genomic functions. However, the genome differs fundamentally from natural language, as it lacks clearly defined words or a consistent grammar. Although DNA language models (DNALMs) such as DNABERT, GENA-LM have achieved high level of performance on genome-related biological tasks, these models do not encode biological functions in the presence of sequence variations. To address this problem, we pre-train foundation models that effectively integrate sequence variations, in particular Single Nucleotide Polymorphisms (SNPs), as they underlie important biological functions. Specifically, we use ModernBERT to pre-train two different Biomedical Foundation Models (BMFM), namely, BMFM-DNA-REF in which the model is trained with sequences of varying lengths along with their reverse complements derived from the reference genome and BMFM-DNA-SNP in which the model is trained with sequences created using a novel representation scheme that encodes sequence variations. Our findings indicate that integrating sequence variations into DNALMs helps capture the biological functions as seen in improvements on all fine-tuning tasks. To explore the model's practical utility, we experimented with various strategies for SNP imputation on promoter detection task introduced in DNABERT-2. However, we acknowledge that the current benchmarks are limited in their ability to fully evaluate these models. To enable more comprehensive assessment in the future and encourage community contributions, we release our models through HuggingFace and the code to reproduce the results at https://github.com/BiomedSciAI/biomed-multi-omic",Bioinformatics,http://arxiv.org/abs/2507.05265v1,arXiv,1
"The accurate development, assessment, interpretation, and benchmarking of bioinformatics frameworks for analyzing transcriptional regulatory grammars rely on controlled simulations to validate the underlying methods. However, existing simulators often lack end-to-end flexibility or ease of integration, which limits their practical use. We present inMOTIFin, a lightweight, modular, and user-friendly Python-based software that addresses these gaps by providing versatile and efficient simulation and modification of DNA regulatory sequences. inMOTIFin enables users to simulate or modify regulatory sequences efficiently for the customizable generation of motifs and insertion of motif instances with precise control over their positions, co-occurrences, and spacing, as well as direct modification of real sequences, facilitating a comprehensive evaluation of motif-based methods and interpretation tools. We demonstrate inMOTIFin applications for the assessment of de novo motif discovery prediction, the analysis of transcription factor cooperativity, and the support of explainability analyses for deep learning models. inMOTIFin ensures robust and reproducible analyses for studying transcriptional regulatory grammars.   inMOTIFin is available at PyPI https://pypi.org/project/inMOTIFin/ and Docker Hub https://hub.docker.com/r/cbgr/inmotifin. Detailed documentation is available at https://inmotifin.readthedocs.io/en/latest/. The code for use case analyses is available at https://bitbucket.org/CBGR/inmotifin_evaluation/src/main/.",Bioinformatics,http://arxiv.org/abs/2506.20769v1,arXiv,1
This paper presents a novel quantum-enhanced prototype for drug repurposing and addresses the challenge of managing massive genomics data in precision medicine.,Bioinformatics,http://arxiv.org/abs/2506.19097v1,arXiv,1
"Extrachromosomal circular DNA (eccDNA) plays key regulatory roles and contributes to oncogene overexpression in cancer through high-copy amplification and long-range interactions. Despite advances in modeling, no pre-trained models currently support full-length circular eccDNA for downstream analysis. Existing genomic models are either limited to single-nucleotide resolution or hindered by the inefficiency of the quadratic attention mechanism. Here, we introduce eccDNAMamba, the first bidirectional state-space encoder tailored for circular DNA sequences. It combines forward and reverse passes for full-context representation learning with linear-time complexity, and preserves circular structure through a novel augmentation strategy. Tested on two real-world datasets, eccDNAMamba achieves strong classification performance and scales to sequences up to 200 Kbp, offering a robust and efficient framework for modeling circular genomes. Our codes are available at https://github.com/zzq1zh/GenAI-Lab.",Bioinformatics,http://arxiv.org/abs/2506.18940v1,arXiv,1
"Pretraining DNA language models (DNALMs) on the full human genome is resource-intensive, yet often considered necessary for strong downstream performance. Inspired by recent findings in NLP and long-context modeling, we explore an alternative: self-pretraining on task-specific, unlabeled data. Using the BEND benchmark, we show that DNALMs trained with self-pretraining match or exceed the performance of models trained from scratch under identical compute. While genome-scale pretraining may still offer higher absolute performance, task-specific self-pretraining provides a practical and compute-efficient strategy for building stronger supervised baselines.",Bioinformatics,http://arxiv.org/abs/2506.17766v1,arXiv,1
"Understanding the properties of biological systems is an exciting avenue for applying advanced approaches to solving corresponding computational tasks. A specific class of problems that arises in the resolution of biological challenges is optimization. In this work, we present the results of a proof-of-concept study that applies a quantum-inspired optimization algorithm to simulate a viral response. We formulate an Ising-type model to describe the patterns of gene activity in host responses. Reducing the problem to the Ising form allows the use of available quantum and quantum-inspired optimization tools. We demonstrate the application of a quantum-inspired optimization algorithm to this problem. Our study paves the way for exploring the full potential of quantum and quantum-inspired optimization tools in biological applications.",Bioinformatics,http://arxiv.org/abs/2506.15671v2,arXiv,1
"We studied a generalized question: chronic diseases like ME/CFS and long COVID exhibit high heterogeneity with multifactorial etiology and progression, complicating diagnosis and treatment. To address this, we developed BioMapAI, an explainable Deep Learning framework using the richest longitudinal multi-omics dataset for ME/CFS to date. This dataset includes gut metagenomics, plasma metabolome, immune profiling, blood labs, and clinical symptoms. By connecting multi-omics to a symptom matrix, BioMapAI identified both disease- and symptom-specific biomarkers, reconstructed symptoms, and achieved state-of-the-art precision in disease classification. We also created the first connectivity map of these omics in both healthy and disease states and revealed how microbiome-immune-metabolome crosstalk shifted from healthy to ME/CFS.",Bioinformatics,http://arxiv.org/abs/2506.15761v1,arXiv,1
"Optimal transport provides a robust framework for comparing probability distributions. Its effectiveness is significantly influenced by the choice of the underlying ground metric. Traditionally, the ground metric has either been (i) predefined, e.g., as the Euclidean distance, or (ii) learned in a supervised way, by utilizing labeled data to learn a suitable ground metric for enhanced task-specific performance. Yet, predefined metrics typically cannot account for the inherent structure and varying importance of different features in the data, and existing supervised approaches to ground metric learning often do not generalize across multiple classes or are restricted to distributions with shared supports. To address these limitations, we propose a novel approach for learning metrics for arbitrary distributions over a shared metric space. Our method provides a distance between individual points like a global metric, but requires only class labels on a distribution-level for training. The learned global ground metric enables more accurate optimal transport distances, leading to improved performance in embedding, clustering and classification tasks. We demonstrate the effectiveness and interpretability of our approach using patient-level scRNA-seq data spanning multiple diseases.",Bioinformatics,http://arxiv.org/abs/2506.15383v1,arXiv,1
"Circular genome visualizations are essential for exploring structural variants and gene regulation. However, existing tools often require complex scripting and manual configuration, making the process time-consuming, error-prone, and difficult to learn. To address these challenges, we introduce AuraGenome, an LLM-powered framework for rapid, reusable, and scalable generation of multi-layered circular genome visualizations. AuraGenome combines a semantic-driven multi-agent workflow with an interactive visual analytics system. The workflow employs seven specialized LLM-driven agents, each assigned distinct roles such as intent recognition, layout planning, and code generation, to transform raw genomic data into tailored visualizations. The system supports multiple coordinated views tailored for genomic data, offering ring, radial, and chord-based layouts to represent multi-layered circular genome visualizations. In addition to enabling interactions and configuration reuse, the system supports real-time refinement and high-quality report export. We validate its effectiveness through two case studies and a comprehensive user study. AuraGenome is available at: https://github.com/Darius18/AuraGenome.",Bioinformatics,http://arxiv.org/abs/2507.02877v1,arXiv,1
"Transcriptomic foundation models (TFMs) have recently emerged as powerful tools for analyzing gene expression in cells and tissues, supporting key tasks such as cell-type annotation, batch correction, and perturbation prediction. However, the diversity of model implementations and training strategies across recent TFMs, though promising, makes it challenging to isolate the contribution of individual design choices or evaluate their potential synergies. This hinders the field's ability to converge on best practices and limits the reproducibility of insights across studies. We present BMFM-RNA, an open-source, modular software package that unifies diverse TFM pretraining and fine-tuning objectives within a single framework. Leveraging this capability, we introduce a novel training objective, whole cell expression decoder (WCED), which captures global expression patterns using an autoencoder-like CLS bottleneck representation. In this paper, we describe the framework, supported input representations, and training objectives. We evaluated four model checkpoints pretrained on CELLxGENE using combinations of masked language modeling (MLM), WCED and multitask learning. Using the benchmarking capabilities of BMFM-RNA, we show that WCED-based models achieve performance that matches or exceeds state-of-the-art approaches like scGPT across more than a dozen datasets in both zero-shot and fine-tuning tasks. BMFM-RNA, available as part of the biomed-multi-omics project ( https://github.com/BiomedSciAI/biomed-multi-omic ), offers a reproducible foundation for systematic benchmarking and community-driven exploration of optimal TFM training strategies, enabling the development of more effective tools to leverage the latest advances in AI for understanding cell biology.",Bioinformatics,http://arxiv.org/abs/2506.14861v1,arXiv,1
"Generating high-fidelity and biologically plausible synthetic single-cell RNA sequencing (scRNA-seq) data, especially with conditional control, is challenging due to its high dimensionality, sparsity, and complex biological variations. Existing generative models often struggle to capture these unique characteristics and ensure robustness to structural noise in cellular networks. We introduce LapDDPM, a novel conditional Graph Diffusion Probabilistic Model for robust and high-fidelity scRNA-seq generation. LapDDPM uniquely integrates graph-based representations with a score-based diffusion model, enhanced by a novel spectral adversarial perturbation mechanism on graph edge weights. Our contributions are threefold: we leverage Laplacian Positional Encodings (LPEs) to enrich the latent space with crucial cellular relationship information; we develop a conditional score-based diffusion model for effective learning and generation from complex scRNA-seq distributions; and we employ a unique spectral adversarial training scheme on graph edge weights, boosting robustness against structural variations. Extensive experiments on diverse scRNA-seq datasets demonstrate LapDDPM's superior performance, achieving high fidelity and generating biologically-plausible, cell-type-specific samples. LapDDPM sets a new benchmark for conditional scRNA-seq data generation, offering a robust tool for various downstream biological applications.",Bioinformatics,http://arxiv.org/abs/2506.13344v1,arXiv,1
"Identifying causative genes from patient phenotypes remains a significant challenge in precision medicine, with important implications for the diagnosis and treatment of genetic disorders. We propose a novel graph-based approach for predicting causative genes from patient phenotypes, with or without an available list of candidate genes, by integrating a rare disease knowledge graph (KG). Our model, combining graph neural networks and transformers, achieves substantial improvements over the current state-of-the-art. On the real-world MyGene2 dataset, it attains a mean reciprocal rank (MRR) of 24.64\% and nDCG@100 of 33.64\%, surpassing the best baseline (SHEPHERD) at 19.02\% MRR and 30.54\% nDCG@100. We perform extensive ablation studies to validate the contribution of each model component. Notably, the approach generalizes to cases where only phenotypic data are available, addressing key challenges in clinical decision support when genomic information is incomplete.",Bioinformatics,http://arxiv.org/abs/2506.13119v1,arXiv,1
"Motivation: Spliced alignment refers to the alignment of messenger RNA (mRNA) or protein sequences to eukaryotic genomes. It plays a critical role in gene annotation and the study of gene functions. Accurate spliced alignment demands sophisticated modeling of splice sites, but current aligners use simple models, which may affect their accuracy given dissimilar sequences.   Results: We implemented minisplice to learn splice signals with a one-dimensional convolutional neural network (1D-CNN) and trained a model with 7,026 parameters for vertebrate and insect genomes. It captures conserved splice signals across phyla and reveals GC-rich introns specific to mammals and birds. We used this model to estimate the empirical splicing probability for every GT and AG in genomes, and modified minimap2 and miniprot to leverage pre-computed splicing probability during alignment. Evaluation on human long-read RNA-seq data and cross-species protein datasets showed our method greatly improves the junction accuracy especially for noisy long RNA-seq reads and proteins of distant homology.   Availability and implementation: https://github.com/lh3/minisplice",Bioinformatics,http://arxiv.org/abs/2506.12986v2,arXiv,1
"Generative AI foundation models offer transformative potential for processing structured biological data, particularly in single-cell RNA sequencing, where datasets are rapidly scaling toward billions of cells. We propose the use of agentic foundation models with real-time web search to automate the labeling of experimental data, achieving up to 82.5% accuracy. This addresses a key bottleneck in supervised learning for structured omics data by increasing annotation throughput without manual curation and human error. Our approach enables the development of virtual cell foundation models capable of downstream tasks such as cell-typing and perturbation prediction. As data volume grows, these models may surpass human performance in labeling, paving the way for reliable inference in large-scale perturbation screens. This application demonstrates domain-specific innovation in health monitoring and diagnostics, aligned with efforts like the Human Cell Atlas and Human Tumor Atlas Network.",Bioinformatics,http://arxiv.org/abs/2506.13817v1,arXiv,1
"Viruses are the most abundant biological entities on Earth and play central roles in shaping microbiomes and influencing ecosystem functions. Yet, most viral genes remain uncharacterized, comprising what is commonly referred to as ""viral dark matter."" Metagenomic studies across diverse environments consistently show that 40-90% of viral genes lack known homologs or annotated functions. This persistent knowledge gap limits our ability to interpret viral sequence data, understand virus-host interactions, and assess the ecological or applied significance of viral genes. Among the most intriguing components of viral dark matter are auxiliary viral genes (AVGs), including auxiliary metabolic genes (AMGs), regulatory genes (AReGs), and host physiology-modifying genes (APGs), which may alter host function during infection and contribute to microbial metabolism, stress tolerance, or resistance. In this review, we explore recent advances in the discovery and functional characterization of viral dark matter. We highlight representative examples of novel viral proteins across diverse ecosystems including human microbiomes, soil, oceans, and extreme environments, and discuss what is known, and still unknown, about their roles. We then examine the bioinformatic and experimental challenges that hinder functional characterization, and present emerging strategies to overcome these barriers. Finally, we highlight both the fundamental and applied benefits that multidisciplinary efforts to characterize viral proteins can bring. By integrating computational predictions with experimental validation, and fostering collaboration across disciplines, we emphasize that illuminating viral dark matter is both feasible and essential for advancing microbial ecology and unlocking new tools for biotechnology.",Bioinformatics,http://arxiv.org/abs/2506.11942v1,arXiv,1
"Over the past years, substantial numbers of microbial species' genomes have been deposited outside of conventional INSDC databases. The GlobDB aggregates 14 independent genomic catalogues to provide a comprehensive database of species-dereplicated microbial genomes, with consistent taxonomy, annotations, and additional analysis resources. The GlobDB is available at https://globdb.org/.",Bioinformatics,http://arxiv.org/abs/2506.11896v1,arXiv,1
"Spatial transcriptomics (ST) technologies enable gene expression profiling with spatial resolution, offering unprecedented insights into tissue organization and disease heterogeneity. However, current analysis methods often struggle with noisy data, limited scalability, and inadequate modelling of complex cellular relationships. We present SemanticST, a biologically informed, graph-based deep learning framework that models diverse cellular contexts through multi-semantic graph construction. SemanticST builds multiple context-specific graphs capturing spatial proximity, gene expression similarity, and tissue domain structure, and learns disentangled embeddings for each. These are fused using an attention-inspired strategy to yield a unified, biologically meaningful representation. A community-aware min-cut loss improves robustness over contrastive learning, particularly in sparse ST data. SemanticST supports mini-batch training, making it the first graph neural network scalable to large-scale datasets such as Xenium (500,000 cells). Benchmarking across four platforms (Visium, Slide-seq, Stereo-seq, Xenium) and multiple human and mouse tissues shows consistent 20 percentage gains in ARI, NMI, and trajectory fidelity over DeepST, GraphST, and IRIS. In re-analysis of breast cancer Xenium data, SemanticST revealed rare and clinically significant niches, including triple receptor-positive clusters, spatially distinct DCIS-to-IDC transition zones, and FOXC2 tumour-associated myoepithelial cells, suggesting non-canonical EMT programs with stem-like features. SemanticST thus provides a scalable, interpretable, and biologically grounded framework for spatial transcriptomics analysis, enabling robust discovery across tissue types and diseases, and paving the way for spatially resolved tissue atlases and next-generation precision medicine.",Bioinformatics,http://arxiv.org/abs/2506.11491v2,arXiv,1
"Raw signal genome analysis (RSGA) has emerged as a promising approach to enable real-time genome analysis by directly analyzing raw electrical signals. However, rapid advancements in sequencing technologies make it increasingly difficult for software-based RSGA to match the throughput of raw signal generation. This paper demonstrates that while hardware acceleration techniques can significantly accelerate RSGA, the high volume of genomic data shifts the performance and energy bottleneck from computation to I/O data movement. As sequencing throughput increases, I/O overhead becomes the main contributor to both runtime and energy consumption. Therefore, there is a need to design a high-performance, energy-efficient system for RSGA that can both alleviate the data movement bottleneck and provide large acceleration capabilities. We propose MARS, a storage-centric system that leverages the heterogeneous resources within modern storage systems (e.g., storage-internal DRAM, storage controller, flash chips) alongside their large storage capacity to tackle both data movement and computational overheads of RSGA in an area-efficient and low-cost manner. MARS accelerates RSGA through a novel hardware/software co-design approach. First, MARS modifies the RSGA pipeline via two filtering mechanisms and a quantization scheme, reducing hardware demands and optimizing for in-storage execution. Second, MARS accelerates the RSGA steps directly within the storage by leveraging both Processing-Near-Memory and Processing-Using-Memory paradigms. Third, MARS orchestrates the execution of all steps to fully exploit in-storage parallelism and minimize data movement. Our evaluation shows that MARS outperforms basecalling-based software and hardware-accelerated state-of-the-art read mapping pipelines by 93x and 40x, on average across different datasets, while reducing their energy consumption by 427x and 72x.",Bioinformatics,http://arxiv.org/abs/2506.10931v2,arXiv,1
"To meet the needs of a large pharmaceutical organization, we set out to create S3Mirror - an application for transferring large genomic sequencing datasets between S3 buckets quickly, reliably, and observably. We used the DBOS Transact durable execution framework to achieve these goals and benchmarked the performance and cost of the application. S3Mirror is an open source DBOS Python application that can run in a variety of environments, including DBOS Cloud Pro, where it runs as much as 40x faster than AWS DataSync at a fraction of the cost. Moreover, S3Mirror is resilient to failures and allows for real-time filewise observability of ongoing and past transfers.",Bioinformatics,http://arxiv.org/abs/2506.10886v2,arXiv,1
"Predicting guide RNA (gRNA) activity is critical for effective CRISPR-Cas12 genome editing but remains challenging due to limited data, variation across protospacer adjacent motifs (PAMs-short sequence requirements for Cas binding), and reliance on large-scale training. We investigate whether pre-trained biological foundation model originally trained on transcriptomic data can improve gRNA activity estimation even without domain-specific pre-training. Using embeddings from existing RNA foundation model as input to lightweight regressor, we show substantial gains over traditional baselines. We also integrate chromatin accessibility data to capture regulatory context, improving performance further. Our results highlight the effectiveness of pre-trained foundation models and chromatin accessibility data for gRNA activity prediction.",Bioinformatics,http://arxiv.org/abs/2506.11182v1,arXiv,1
"Genomic language models (gLMs) hold promise for generating novel, functional DNA sequences for synthetic biology. However, realizing this potential requires models to go beyond evolutionary plausibility and understand how DNA sequence encodes gene expression and regulation. We introduce a benchmark called Nullsettes, which assesses how well models can predict in silico loss-of-function (LOF) mutations, in synthetic expression cassettes with little evolutionary precedent. Testing 12 state-of-the-art gLMs, we find that most fail to consistently detect these strong LOF mutations. All models show a sharp drop in predictive accuracy as the likelihood assigned to the original (nonmutant) sequence decreases, suggesting that gLMs rely heavily on pattern-matching to their evolutionary prior rather than on any mechanistic understanding of gene expression. Our findings highlight fundamental limitations in how gLMs generalize to engineered, non-natural sequences, and underscore the need for benchmarks and modeling strategies that prioritize functional understanding.",Bioinformatics,http://arxiv.org/abs/2506.10271v3,arXiv,1
"In this paper, we study the efficacy and utility of recent advances in non-local, non-linear image interpolation and extrapolation algorithms, specifically, ideas based on Implicit Neural Representations (INR), as a tool for analysis of spatial transcriptomics data. We seek to utilize the microarray gene expression data sparsely sampled in the healthy human brain, and produce fully resolved spatial maps of any given gene across the whole brain at a voxel-level resolution. To do so, we first obtained the 100 top AD risk genes, whose baseline spatial transcriptional profiles were obtained from the Allen Human Brain Atlas (AHBA). We adapted Implicit Neural Representation models so that the pipeline can produce robust voxel-resolution quantitative maps of all genes. We present a variety of experiments using interpolations obtained from Abagen as a baseline/reference.",Bioinformatics,http://arxiv.org/abs/2506.11158v1,arXiv,1
"Single-cell transcriptomics and proteomics have become a great source for data-driven insights into biology, enabling the use of advanced deep learning methods to understand cellular heterogeneity and gene expression at the single-cell level. With the advent of spatial-omics data, we have the promise of characterizing cells within their tissue context as it provides both spatial coordinates and intra-cellular transcriptional or protein counts. Proteomics offers a complementary view by directly measuring proteins, which are the primary effectors of cellular function and key therapeutic targets. However, existing models either ignore the spatial information or the complex genetic and proteomic programs within cells. Thus they cannot infer how cell internal regulation adapts to microenvironmental cues. Furthermore, these models often utilize fixed gene vocabularies, hindering their generalizability unseen genes. In this paper, we introduce HEIST, a hierarchical graph transformer foundation model for spatial transcriptomics and proteomics. HEIST models tissues as hierarchical graphs. The higher level graph is a spatial cell graph, and each cell in turn, is represented by its lower level gene co-expression network graph. HEIST achieves this by performing both intra-level and cross-level message passing to utilize the hierarchy in its embeddings and can thus generalize to novel datatypes including spatial proteomics without retraining. HEIST is pretrained on 22.3M cells from 124 tissues across 15 organs using spatially-aware contrastive and masked autoencoding objectives. Unsupervised analysis of HEIST embeddings reveals spatially informed subpopulations missed by prior models. Downstream evaluations demonstrate generalizability to proteomics data and state-of-the-art performance in clinical outcome prediction, cell type annotation, and gene imputation across multiple technologies.",Bioinformatics,http://arxiv.org/abs/2506.11152v3,arXiv,1
"Pathogen genome data offers valuable structure for spatial models, but its utility is limited by incomplete sequencing coverage. We propose a probabilistic framework for inferring genetic distances between unsequenced cases and known sequences within defined transmission chains, using time-aware evolutionary distance modeling. The method estimates pairwise divergence from collection dates and observed genetic distances, enabling biologically plausible imputation grounded in observed divergence patterns, without requiring sequence alignment or known transmission chains. Applied to highly pathogenic avian influenza A/H5 cases in wild birds in the United States, this approach supports scalable, uncertainty-aware augmentation of genomic datasets and enhances the integration of evolutionary information into spatiotemporal modeling workflows.",Bioinformatics,http://arxiv.org/abs/2506.09076v3,arXiv,1
"As a core mechanism of epigenetic regulation in eukaryotes, protein post-translational modifications (PTMs) require precise prediction to decipher dynamic life activity networks. To address the limitations of existing deep learning models in cross-modal feature fusion, domain generalization, and architectural optimization, this study proposes UniPTMs: the first unified framework for multi-type PTM prediction. The framework innovatively establishes a ""Master-Slave"" dual-path collaborative architecture: The master path dynamically integrates high-dimensional representations of protein sequences, structures, and evolutionary information through a Bidirectional Gated Cross-Attention (BGCA) module, while the slave path optimizes feature discrepancies and recalibration between structural and traditional features using a Low-Dimensional Fusion Network (LDFN). Complemented by a Multi-scale Adaptive convolutional Pyramid (MACP) for capturing local feature patterns and a Bidirectional Hierarchical Gated Fusion Network (BHGFN) enabling multi-level feature integration across paths, the framework employs a Hierarchical Dynamic Weighting Fusion (HDWF) mechanism to intelligently aggregate multimodal features. Enhanced by a novel Hierarchical Contrastive loss function for feature consistency optimization, UniPTMs demonstrates significant performance improvements (3.2%-11.4% MCC and 4.2%-14.3% AP increases) over state-of-the-art models across five modification types and transcends the Single-Type Prediction Paradigm. To strike a balance between model complexity and performance, we have also developed a lightweight variant named UniPTMs-mini.",Bioinformatics,http://arxiv.org/abs/2506.05443v1,arXiv,1
"Gene set analysis (GSA) is a foundational approach for interpreting genomic data of diseases by linking genes to biological processes. However, conventional GSA methods overlook clinical context of the analyses, often generating long lists of enriched pathways with redundant, nonspecific, or irrelevant results. Interpreting these requires extensive, ad-hoc manual effort, reducing both reliability and reproducibility. To address this limitation, we introduce cGSA, a novel AI-driven framework that enhances GSA by incorporating context-aware pathway prioritization. cGSA integrates gene cluster detection, enrichment analysis, and large language models to identify pathways that are not only statistically significant but also biologically meaningful. Benchmarking on 102 manually curated gene sets across 19 diseases and ten disease-related biological mechanisms shows that cGSA outperforms baseline methods by over 30%, with expert validation confirming its increased precision and interpretability. Two independent case studies in melanoma and breast cancer further demonstrate its potential to uncover context-specific insights and support targeted hypothesis generation.",Bioinformatics,http://arxiv.org/abs/2506.04303v1,arXiv,1
"Learning on small data is a challenge frequently encountered in many real-world applications. In this work we study how effective quantum ensemble models are when trained on small data problems in healthcare and life sciences. We constructed multiple types of quantum ensembles for binary classification using up to 26 qubits in simulation and 56 qubits on quantum hardware. Our ensemble designs use minimal trainable parameters but require long-range connections between qubits. We tested these quantum ensembles on synthetic datasets and gene expression data from renal cell carcinoma patients with the task of predicting patient response to immunotherapy. From the performance observed in simulation and initial hardware experiments, we demonstrate how quantum embedding structure affects performance and discuss how to extract informative features and build models that can learn and generalize effectively. We present these exploratory results in order to assist other researchers in the design of effective learning on small data using ensembles. Incorporating quantum computing in these data constrained problems offers hope for a wide range of studies in healthcare and life sciences where biological samples are relatively scarce given the feature space to be explored.",Bioinformatics,http://arxiv.org/abs/2506.02213v1,arXiv,1
"Natural Language Processing (NLP) has transformed various fields beyond linguistics by applying techniques originally developed for human language to the analysis of biological sequences. This review explores the application of NLP methods to biological sequence data, focusing on genomics, transcriptomics, and proteomics. We examine how various NLP methods, from classic approaches like word2vec to advanced models employing transformers and hyena operators, are being adapted to analyze DNA, RNA, protein sequences, and entire genomes. The review also examines tokenization strategies and model architectures, evaluating their strengths, limitations, and suitability for different biological tasks. We further cover recent advances in NLP applications for biological data, such as structure prediction, gene expression, and evolutionary analysis, highlighting the potential of these methods for extracting meaningful insights from large-scale genomic data. As language models continue to advance, their integration into bioinformatics holds immense promise for advancing our understanding of biological processes in all domains of life.",Bioinformatics,http://arxiv.org/abs/2506.02212v1,arXiv,1
"Modern single-cell datasets now comprise hundreds of millions of cells, presenting significant challenges for training deep learning models that require shuffled, memory-efficient data loading. While the AnnData format is the community standard for storing single-cell datasets, existing data loading solutions for AnnData are often inadequate: some require loading all data into memory, others convert to dense formats that increase storage demands, and many are hampered by slow random disk access. We present scDataset, a PyTorch IterableDataset that operates directly on one or more AnnData files without the need for format conversion. The core innovation is a combination of block sampling and batched fetching, which together balance randomness and I/O efficiency. On the Tahoe 100M dataset, scDataset achieves up to a 48$\times$ speed-up over AnnLoader, a 27$\times$ speed-up over HuggingFace Datasets, and an 18$\times$ speed-up over BioNeMo in single-core settings. These advances democratize large-scale single-cell model training for the broader research community.",Bioinformatics,http://arxiv.org/abs/2506.01883v1,arXiv,1
"Inspired by the success of unsupervised pre-training paradigms, researchers have applied these approaches to DNA pre-training. However, we argue that these approaches alone yield suboptimal results because pure DNA sequences lack sufficient information, since their functions are regulated by genomic profiles like chromatin accessibility. Here, we demonstrate that supervised training for genomic profile prediction serves as a more effective alternative to pure sequence pre-training. Furthermore, considering the multi-species and multi-profile nature of genomic profile prediction, we introduce our $\textbf{S}$pecies-$\textbf{P}$rofile $\textbf{A}$daptive $\textbf{C}$ollaborative $\textbf{E}$xperts (SPACE) that leverages Mixture of Experts (MoE) to better capture the relationships between DNA sequences across different species and genomic profiles, thereby learning more effective DNA representations. Through extensive experiments across various tasks, our model achieves state-of-the-art performance, establishing that DNA models trained with supervised genomic profiles serve as powerful DNA representation learners. The code is available at https://github.com/ZhuJiwei111/SPACE.",Bioinformatics,http://arxiv.org/abs/2506.01833v1,arXiv,1
"Climate change is a major threat to crop potential and is characterized by both long-term shifts in temperature and precipitation patterns as well as increased occurrence of extreme weather events, these extreme weather events are the most immediate and intractable threat to agriculture. Crop resilience in the face of stress depends upon the speed and effectiveness with which plants and cropping systems sense and respond to that stress. A variety of agronomic practices including breeding, exogenous inputs (nutrients, water, biostimulants and others) and shifts in cultivation practice have been used to influence plant stress response to achieve the goal of increased plant and cropping system resilience. Traditional breeding is a powerful tool that has resulted in stable and long-term cultivar improvements but is often too slow and complex to meet the diverse, complex and unpredictable challenges of climate induced stresses. Increased inputs (water, nutrients, pesticides etc.) and management strategies (cropping system choice, soil management etc.) can alleviate stress but are often constrained by cost and availability of inputs. Exogenous biostimulants, microbials and plant hormones have shown great promise as mechanisms to optimize natural plant resilience resulting in immediate but non-permanent improvements in plant responses to climate induced stresses. The failure to modernize regulatory frameworks for the use of biostimulants in agriculture will constrain the development of safe effective tools and deprive growers of means to respond to the vagaries of climate change. Here we discuss the scientific rationale for eliminating the regulatory barriers that constrain the potential for biostimulants or products that modulate plant regulatory networks to address climate change challenges and propose a framework for enabling legislation to strengthen cropping system resilience.",Bioinformatics,http://arxiv.org/abs/2506.01714v2,arXiv,1
"Multiplexed immunofluorescence microscopy captures detailed measurements of spatially resolved, multiple biomarkers simultaneously, revealing tissue composition and cellular interactions in situ among single cells. The growing scale and dimensional complexity of these datasets demand reproducible, comprehensive and user-friendly computational tools. To address this need, we developed SPAC (SPAtial single-Cell analysis), a Python-based package and a corresponding shiny application within an integrated, modular SPAC ecosystem (Liu et al., 2025) designed specifically for biologists without extensive coding expertise. Following image segmentation and extraction of spatially resolved single-cell data, SPAC streamlines downstream phenotyping and spatial analysis, facilitating characterization of cellular heterogeneity and spatial organization within tissues. Through scalable performance, specialized spatial statistics, highly customizable visualizations, and seamless workflows from dataset to insights, SPAC significantly lowers barriers to sophisticated spatial analyses.",Bioinformatics,http://arxiv.org/abs/2506.01560v1,arXiv,1
"Recent studies have shown that integrating multimodal data fusion techniques for imaging and genetic features is beneficial for the etiological analysis and predictive diagnosis of Alzheimer's disease (AD). However, there are several critical flaws in current deep learning methods. Firstly, there has been insufficient discussion and exploration regarding the selection and encoding of genetic information. Secondly, due to the significantly superior classification value of AD imaging features compared to genetic features, many studies in multimodal fusion emphasize the strengths of imaging features, actively mitigating the influence of weaker features, thereby diminishing the learning of the unique value of genetic features. To address this issue, this study proposes the dynamic multimodal role-swapping network (GenDMR). In GenDMR, we develop a novel approach to encode the spatial organization of single nucleotide polymorphisms (SNPs), enhancing the representation of their genomic context. Additionally, to adaptively quantify the disease risk of SNPs and brain region, we propose a multi-instance attention module to enhance model interpretability. Furthermore, we introduce a dominant modality selection module and a contrastive self-distillation module, combining them to achieve a dynamic teacher-student role exchange mechanism based on dominant and auxiliary modalities for bidirectional co-updating of different modal data. Finally, GenDMR achieves state-of-the-art performance on the ADNI public dataset and visualizes attention to different SNPs, focusing on confirming 12 potential high-risk genes related to AD, including the most classic APOE and recently highlighted significant risk genes. This demonstrates GenDMR's interpretable analytical capability in exploring AD genetic features, providing new insights and perspectives for the development of multimodal data fusion techniques.",Bioinformatics,http://arxiv.org/abs/2506.01456v1,arXiv,1
"Alzheimer's disease (AD) dementia is the most common form of dementia. With the emergence of disease-modifying therapies, predicting disease risk before symptom onset has become critical. We introduce DuAL-Net, a hybrid deep learning framework for AD dementia prediction using whole genome sequencing (WGS) data. DuAL-Net integrates two components: local probability modeling, which segments the genome into non-overlapping windows, and global annotation-based modeling, which annotates SNPs and reorganizes WGS input to capture long-range functional relationships. Both employ out-of-fold stacking with TabNet and Random Forest classifiers. Final predictions combine local and global probabilities using an optimized weighting parameter alpha. We analyzed WGS data from 1,050 individuals (443 cognitively normal, 607 AD dementia) using five-fold cross-validation. DuAL-Net achieved an AUC of 0.671 using top-ranked SNPs, representing 35.0% and 20.3% higher performance than bottom-ranked and randomly selected SNPs, respectively. ROC analysis demonstrated strong positive correlation between SNP prioritization rank and predictive power. The model identified known AD-associated SNPs as top contributors alongside potentially novel variants. DuAL-Net presents a promising framework improving both predictive accuracy and biological interpretability. The framework and web implementation offer an accessible platform for broader research applications.",Bioinformatics,http://arxiv.org/abs/2506.00673v1,arXiv,1
"INTRODUCTION: Alzheimer's disease (AD) is genetically complex, complicating robust classification from genomic data. METHODS: We developed a transformer-based ensemble model (TrUE-Net) using Monte Carlo Dropout for uncertainty estimation in AD classification from whole-genome sequencing (WGS). We combined a transformer that preserves single-nucleotide polymorphism (SNP) sequence structure with a concurrent random forest using flattened genotypes. An uncertainty threshold separated samples into an uncertain (high-variance) group and a more certain (low-variance) group. RESULTS: We analyzed 1050 individuals, holding out half for testing. Overall accuracy and area under the receiver operating characteristic (ROC) curve (AUC) were 0.6514 and 0.6636, respectively. Excluding the uncertain group improved accuracy from 0.6263 to 0.7287 (10.24% increase) and F1 from 0.5843 to 0.8205 (23.62% increase). DISCUSSION: Monte Carlo Dropout-driven uncertainty helps identify ambiguous cases that may require further clinical evaluation, thus improving reliability in AD genomic classification.",Bioinformatics,http://arxiv.org/abs/2506.00662v1,arXiv,1
"Low-cost, high-throughput DNA and RNA sequencing (HTS) data is the main workforce for the life sciences. Genome sequencing is now becoming a part of Predictive, Preventive, Personalized, and Participatory (termed 'P4') medicine. All genomic data are currently processed in energy-hungry computer clusters and centers, necessitating data transfer, consuming substantial energy, and wasting valuable time. Therefore, there is a need for fast, energy-efficient, and cost-efficient technologies that enable genomics research without requiring data centers and cloud platforms. We recently started the BioPIM Project to leverage the emerging processing-in-memory (PIM) technologies to enable energy and cost-efficient analysis of bioinformatics workloads. The BioPIM Project focuses on co-designing algorithms and data structures commonly used in genomics with several PIM architectures for the highest cost, energy, and time savings benefit.",Bioinformatics,http://arxiv.org/abs/2506.00597v1,arXiv,1
"Single-cell RNA sequencing (scRNA-seq) has revolutionized our understanding of cellular processes by enabling gene expression analysis at the individual cell level. Clustering allows for the identification of cell types and the further discovery of intrinsic patterns in single-cell data. However, the high dimensionality and sparsity of scRNA-seq data continue to challenge existing clustering models. In this paper, we introduce JojoSCL, a novel self-supervised contrastive learning framework for scRNA-seq clustering. By incorporating a shrinkage estimator based on hierarchical Bayesian estimation, which adjusts gene expression estimates towards more reliable cluster centroids to reduce intra-cluster dispersion, and optimized using Stein's Unbiased Risk Estimate (SURE), JojoSCL refines both instance-level and cluster-level contrastive learning. Experiments on ten scRNA-seq datasets substantiate that JojoSCL consistently outperforms prevalent clustering methods, with further validation of its practicality through robustness analysis and ablation studies. JojoSCL's code is available at: https://github.com/ziwenwang28/JojoSCL.",Bioinformatics,http://arxiv.org/abs/2506.00410v1,arXiv,1
"Cell-free DNA (cfDNA) analysis is a powerful, minimally invasive tool for monitoring disease progression, treatment response, and early detection. A major challenge, however, is accurately determining the tissue of origin, especially in complex or heterogeneous disease contexts. To address this, we developed a machine learning framework that leverages tissue-specific DNA methylation signatures to classify both tissue and disease origin from cfDNA data. Our model integrates methylation datasets across diverse epigenomic platforms, including Whole Genome Bisulfite Sequencing (WGBS), Illumina Infinium Bead Arrays, and Enzymatic Methyl-seq (EM-seq). To account for platform variability and data sparsity, we applied imputation strategies and harmonized CpG features to enable cross-platform learning. Dimensionality reduction revealed clear tissue-specific clustering of methylation profiles. A random forest classifier trained on these features achieved consistent classification performance (accuracy 0.75-0.8 across test sets and platforms). Notably, our model distinguished clinically relevant tissues such as inflamed synovium and peripheral blood mononuclear cells (PBMCs) in arthritis patients and deconvoluted synthetic cfDNA mixtures mimicking real-world liquid biopsy samples. The predicted tissue proportions closely matched the true values, demonstrating the model's potential for both classification and quantitative inference. These results support the feasibility of using cross-platform methylation data and machine learning for scalable, generalizable cfDNA diagnostics and lay the groundwork for future integration of disease-specific epigenetic features to guide clinical decision-making in precision medicine.",Bioinformatics,http://arxiv.org/abs/2506.00146v1,arXiv,1
"Accurately predicting gene mutations, mutation subtypes and their exons in lung cancer is critical for personalized treatment planning and prognostic assessment. Faced with regional disparities in medical resources and the high cost of genomic assays, using artificial intelligence to infer these mutations and exon variants from routine histopathology images could greatly facilitate precision therapy. Although some prior studies have shown that deep learning can accelerate the prediction of key gene mutations from lung cancer pathology slides, their performance remains suboptimal and has so far been limited mainly to early screening tasks. To address these limitations, we have assembled PathGene, which comprises histopathology images paired with next-generation sequencing reports from 1,576 patients at the Second Xiangya Hospital, Central South University, and 448 TCGA-LUAD patients. This multi-center dataset links whole-slide images to driver gene mutation status, mutation subtypes, exon, and tumor mutational burden (TMB) status, with the goal of leveraging pathology images to predict mutations, subtypes, exon locations, and TMB for early genetic screening and to advance precision oncology. Unlike existing datasets, we provide molecular-level information related to histopathology images in PathGene to facilitate the development of biomarker prediction models. We benchmarked 11 multiple-instance learning methods on PathGene for mutation, subtype, exon, and TMB prediction tasks. These experimental methods provide valuable alternatives for early genetic screening of lung cancer patients and assisting clinicians to quickly develop personalized precision targeted treatment plans for patients. Code and data are available at https://github.com/panliangrui/NIPS2025/.",Bioinformatics,http://arxiv.org/abs/2506.00096v3,arXiv,1
"Background: Platelet proteomics offers valuable insights for clinical research, yet isolating high-purity platelets remains a challenge. Current methods often lead to contamination or platelet loss, compromising data quality and reproducibility.   Objectives: This study aimed to optimize a platelet isolation technique that yields high-purity samples with minimal loss and to identify the most effective mass spectrometry-based proteomic method for analyzing platelet proteins with optimal coverage and sensitivity.   Methods: We refined an isolation protocol by adjusting centrifugation time to reduce blood volume requirements while preserving platelet yield and purity. Using this optimized method, we evaluated three proteomic approaches: Label-free Quantification with Data-Independent Acquisition (LFQ-DIA), Label-free Quantification with Data-Dependent Acquisition (LFQ-DDA), and Tandem Mass Tag labeling with DDA (TMT-DDA).   Results: LFQ-DIA demonstrated superior protein coverage and sensitivity compared to LFQ-DDA and TMT-DDA. The refined isolation protocol effectively minimized contamination and platelet loss. Additionally, age-related differences in platelet protein composition were observed, highlighting the importance of using age-matched controls in biomarker discovery studies.   Conclusions: The optimized platelet isolation protocol provides a cost-effective and reliable method for preparing high-purity samples for proteomics. LFQ-DIA is the most suitable approach for comprehensive platelet protein analysis. Age-related variation in platelet proteomes underscores the need for demographic matching in clinical proteomic research.",Bioinformatics,http://arxiv.org/abs/2505.24394v1,arXiv,1
"Potato functional genomics lags due to unsystematic gene information curation, gene identifier inconsistencies across reference genome versions, and the increasing volume of research publications. To address these limitations, we developed the Potato Knowledge Hub (http://www.potato-ai.top), leveraging Large Language Models (LLMs) and a systematically curated collection of over 3,200 high-quality potato research papers spanning over 120 years. This platform integrates two key modules: a functional gene database containing 2,571 literature-reported genes, meticulously mapped to the latest DMv8.1 reference genome with resolved nomenclature discrepancies and links to original publications; and a potato knowledge base. The knowledge base, built using a Retrieval-Augmented Generation (RAG) architecture, accurately answers research queries with literature citations, mitigating LLM ""hallucination."" Users can interact with the hub via a natural language AI agent, ""Potato Research Assistant,"" for querying specialized knowledge, retrieving gene information, and extracting sequences. The continuously updated Potato Knowledge Hub aims to be a comprehensive resource, fostering advancements in potato functional genomics and supporting breeding programs.",Bioinformatics,http://arxiv.org/abs/2506.00082v1,arXiv,1
"Topologically Associating Chromatic Domains are spatially distinct chromatin regions that regulate transcription by segregating active and inactive genomic elements. Empirical studies show that their formation correlates with local patterns of epigenetic markers, yet the precise mechanisms linking 1D epigenetic landscapes to 3D chromatin folding remain unclear. Recent models represent chromatin as a spin system, where nucleosomes are treated as discrete-state variables coupled by interaction strengths derived from genomic and epigenomic data. Classical samplers struggle with these models due to high frustration and dense couplings. Here, we present a quantum annealing (QA) approach to efficiently sample chromatin states, embedding an epigenetic Ising model into the topology of D-Wave quantum processors.",Bioinformatics,http://arxiv.org/abs/2505.23289v1,arXiv,1
"DNA, encoding genetic instructions for almost all living organisms, fuels groundbreaking advances in genomics and synthetic biology. Recently, DNA Foundation Models have achieved success in designing synthetic functional DNA sequences, even whole genomes, but their susceptibility to jailbreaking remains underexplored, leading to potential concern of generating harmful sequences such as pathogens or toxin-producing genes. In this paper, we introduce GeneBreaker, the first framework to systematically evaluate jailbreak vulnerabilities of DNA foundation models. GeneBreaker employs (1) an LLM agent with customized bioinformatic tools to design high-homology, non-pathogenic jailbreaking prompts, (2) beam search guided by PathoLM and log-probability heuristics to steer generation toward pathogen-like sequences, and (3) a BLAST-based evaluation pipeline against a curated Human Pathogen Database (JailbreakDNABench) to detect successful jailbreaks. Evaluated on our JailbreakDNABench, GeneBreaker successfully jailbreaks the latest Evo series models across 6 viral categories consistently (up to 60\% Attack Success Rate for Evo2-40B). Further case studies on SARS-CoV-2 spike protein and HIV-1 envelope protein demonstrate the sequence and structural fidelity of jailbreak output, while evolutionary modeling of SARS-CoV-2 underscores biosecurity risks. Our findings also reveal that scaling DNA foundation models amplifies dual-use risks, motivating enhanced safety alignment and tracing mechanisms. Our code is at https://github.com/zaixizhang/GeneBreaker.",Bioinformatics,http://arxiv.org/abs/2505.23839v1,arXiv,1
"Inspired by the great success of Masked Language Modeling (MLM) in the natural language domain, the paradigm of self-supervised pre-training and fine-tuning has also achieved remarkable progress in the field of DNA sequence modeling. However, previous methods often relied on massive pre-training data or large-scale base models with huge parameters, imposing a significant computational burden. To address this, many works attempted to use more compact models to achieve similar outcomes but still fell short by a considerable margin. In this work, we propose a Hybrid Architecture Distillation (HAD) approach, leveraging both distillation and reconstruction tasks for more efficient and effective pre-training. Specifically, we employ the NTv2-500M as the teacher model and devise a grouping masking strategy to align the feature embeddings of visible tokens while concurrently reconstructing the invisible tokens during MLM pre-training. To validate the effectiveness of our proposed method, we conducted comprehensive experiments on the Nucleotide Transformer Benchmark and Genomic Benchmark. Compared to models with similar parameters, our model achieved excellent performance. More surprisingly, it even surpassed the distillation ceiling-teacher model on some sub-tasks, which is more than 500 $\times$ larger. Lastly, we utilize t-SNE for more intuitive visualization, which shows that our model can gain a sophisticated understanding of the intrinsic representation pattern in genomic sequences.",Bioinformatics,http://arxiv.org/abs/2505.20836v1,arXiv,1
"Designing regulatory DNA sequences that achieve precise cell-type-specific gene expression is crucial for advancements in synthetic biology, gene therapy and precision medicine. Although transformer-based language models (LMs) can effectively capture patterns in regulatory DNA, their generative approaches often struggle to produce novel sequences with reliable cell-specific activity. Here, we introduce Ctrl-DNA, a novel constrained reinforcement learning (RL) framework tailored for designing regulatory DNA sequences with controllable cell-type specificity. By formulating regulatory sequence design as a biologically informed constrained optimization problem, we apply RL to autoregressive genomic LMs, enabling the models to iteratively refine sequences that maximize regulatory activity in targeted cell types while constraining off-target effects. Our evaluation on human promoters and enhancers demonstrates that Ctrl-DNA consistently outperforms existing generative and RL-based approaches, generating high-fitness regulatory sequences and achieving state-of-the-art cell-type specificity. Moreover, Ctrl-DNA-generated sequences capture key cell-type-specific transcription factor binding sites (TFBS), short DNA motifs recognized by regulatory proteins that control gene expression, demonstrating the biological plausibility of the generated sequences.",Bioinformatics,http://arxiv.org/abs/2505.20578v1,arXiv,1
"In the crowded eukaryotic nucleus, euchromatin and heterochromatin segregate into distinct compartments, a phenomenon often attributed to homotypic interactions mediated by liquid liquid phase separation of chromatin associated proteins. Here, we revisit genome compartmentalization by examining the role of in vivo DNA packing density fluctuations driven by ATP dependent chromatin remodelers. Leveraging DNA accessibility data, we develop a polymer based model that captures these fluctuations and successfully reproduces genome wide compartment patterns observed in HiC data, without invoking homotypic interactions. Further analysis reveals that density fluctuations in a crowded nuclear environment elevate the system energy, while euchromatin heterochromatin segregation facilitates energy dissipation, offering a thermodynamic advantage for spontaneous compartment formation. These findings suggest that euchromatin heterochromatin segregation may arise through a non equilibrium, self organizing process, providing new insights into genome organization.",Bioinformatics,http://arxiv.org/abs/2505.19461v1,arXiv,1
"Brain aging trajectories differ between males and females, yet the genetic factors underlying these differences remain underexplored. Using structural MRI and genotyping data from 40,940 UK Biobank participants (aged 45-83), we computed Brain Age Gap Estimates (BrainAGE) for total brain, hippocampal, and ventricular volumes. We conducted sex-stratified genome-wide association studies (GWAS) and Post-GWAS analyses to identify genetic variants associated with accelerated brain aging. Distinct gene sets emerged by sex: in females, neurotransmitter transport and mitochondrial stress response genes were implicated; in males, immune and inflammation-related genes dominated. Shared genes, including GMNC and OSTN, were consistently linked to brain volumes across sexes, suggesting core roles in neurostructural maintenance. Tissue expression analyses revealed sex-specific enrichment in pathways tied to neurodegeneration. These findings highlight the importance of sex-stratified approaches in aging research and suggest genetic targets for personalized interventions against age-related cognitive decline.",Bioinformatics,http://arxiv.org/abs/2505.20344v1,arXiv,1
"Spatial transcriptomics (ST) has emerged as a powerful technology for bridging histology imaging with gene expression profiling. However, its application has been limited by low throughput and the need for specialized experimental facilities. Prior works sought to predict ST from whole-slide histology images to accelerate this process, but they suffer from two major limitations. First, they do not explicitly model cell-cell interaction as they factorize the joint distribution of whole-slide ST data and predict the gene expression of each spot independently. Second, their encoders struggle with memory constraints due to the large number of spots (often exceeding 10,000) in typical ST datasets. Herein, we propose STFlow, a flow matching generative model that considers cell-cell interaction by modeling the joint distribution of gene expression of an entire slide. It also employs an efficient slide-level encoder with local spatial attention, enabling whole-slide processing without excessive memory overhead. On the recently curated HEST-1k and STImage-1K4M benchmarks, STFlow substantially outperforms state-of-the-art baselines and achieves over 18% relative improvements over the pathology foundation models.",Bioinformatics,http://arxiv.org/abs/2506.05361v1,arXiv,1
"Large language models (LLMs) have revolutionized natural language processing and are increasingly applied to other sequential data types, including genetic sequences. However, adapting LLMs to genomics presents significant challenges. Capturing complex genomic interactions requires modeling long-range dependencies within DNA sequences, where interactions often span over 10,000 base pairs, even within a single gene, posing substantial computational burdens under conventional model architectures and training paradigms. Moreover, standard LLM training approaches are suboptimal for DNA: autoregressive training, while efficient, supports only unidirectional understanding. However, DNA is inherently bidirectional, e.g., bidirectional promoters regulate transcription in both directions and account for nearly 11% of human gene expression. Masked language models (MLMs) allow bidirectional understanding but are inefficient, as only masked tokens contribute to the loss per step. To address these limitations, we introduce JanusDNA, the first bidirectional DNA foundation model built upon a novel pretraining paradigm that combines the optimization efficiency of autoregressive modeling with the bidirectional comprehension of masked modeling. JanusDNA adopts a hybrid Mamba, Attention and Mixture of Experts (MoE) architecture, combining long-range modeling of Attention with efficient sequential learning of Mamba. MoE layers further scale model capacity via sparse activation while keeping computational cost low. Notably, JanusDNA processes up to 1 million base pairs at single nucleotide resolution on a single 80GB GPU. Extensive experiments and ablations show JanusDNA achieves new SOTA results on three genomic representation benchmarks, outperforming models with 250x more activated parameters. Code: https://github.com/Qihao-Duan/JanusDNA",Bioinformatics,http://arxiv.org/abs/2505.17257v4,arXiv,1
"Sequencing a genome to determine an individual's DNA produces an enormous number of short nucleotide subsequences known as reads, which must be reassembled to reconstruct the full genome. We present a method for analyzing this type of data using contrastive learning, in which an encoder model is trained to produce embeddings that cluster together sequences from the same genomic region. The sequential nature of genomic regions is preserved in the form of trajectories through this embedding space. Trained solely to reflect the structure of the genome, the resulting model provides a general representation of $k$-mer sequences, suitable for a range of downstream tasks involving read data. We apply our framework to learn the structure of the $E.\ coli$ genome, and demonstrate its use in simulated ancient DNA (aDNA) read mapping and identification of structural variations. Furthermore, we illustrate the potential of using this type of model for metagenomic species identification. We show how incorporating a domain-specific noise model can enhance embedding robustness, and how a supervised contrastive learning setting can be adopted when a linear reference genome is available, by introducing a distance thresholding parameter $Î$. The model can also be trained fully self-supervised on read data, enabling analysis without the need to construct a full genome assembly using specialized algorithms. Small prediction heads based on a pre-trained embedding are shown to perform on par with BWA-aln, the current gold standard approach for aDNA mapping, in terms of accuracy and runtime for short genomes. Given the method's favorable scaling properties with respect to total genome size, inference using our approach is highly promising for metagenomic applications and for mapping to genomes comparable in size to the human genome.",Bioinformatics,http://arxiv.org/abs/2505.16680v1,arXiv,1
"Causal discovery in multi-omic datasets is crucial for understanding the bigger picture of gene regulatory mechanisms, but remains challenging due to high dimensionality, differentiation of direct from indirect relationships, and hidden confounders. We introduce GENESIS (GEne Network inference from Expression SIgnals and SNPs), a constraint-based algorithm that leverages the natural causal precedence of genotypes to infer ancestral relationships in transcriptomic data. Unlike traditional causal discovery methods that start with a fully connected graph, GENESIS initialises an empty ancestrality matrix and iteratively populates it with direct, indirect or non-causal relationships using a series of provably sound marginal and conditional independence tests. By integrating genotypes as fixed causal anchors, GENESIS provides a principled ``head start'' to classical causal discovery algorithms, restricting the search space to biologically plausible edges. We test GENESIS on synthetic and real-world genomic datasets. This framework offers a powerful avenue for uncovering causal pathways in complex traits, with promising applications to functional genomics, drug discovery, and precision medicine.",Bioinformatics,http://arxiv.org/abs/2505.15866v1,arXiv,1
"The code of nature, embedded in DNA and RNA genomes since the origin of life, holds immense potential to impact both humans and ecosystems through genome modeling. Genomic Foundation Models (GFMs) have emerged as a transformative approach to decoding the genome. As GFMs scale up and reshape the landscape of AI-driven genomics, the field faces an urgent need for rigorous and reproducible evaluation. We present OmniGenBench, a modular benchmarking platform designed to unify the data, model, benchmarking, and interpretability layers across GFMs. OmniGenBench enables standardized, one-command evaluation of any GFM across five benchmark suites, with seamless integration of over 31 open-source models. Through automated pipelines and community-extensible features, the platform addresses critical reproducibility challenges, including data transparency, model interoperability, benchmark fragmentation, and black-box interpretability. OmniGenBench aims to serve as foundational infrastructure for reproducible genomic AI research, accelerating trustworthy discovery and collaborative innovation in the era of genome-scale modeling.",Bioinformatics,http://arxiv.org/abs/2505.14402v1,arXiv,1
"Respiratory viral infections pose a global health burden, yet the cellular immune responses driving protection or pathology remain unclear. Natural infection cohorts often lack pre-exposure baseline data and structured temporal sampling. In contrast, inoculation and vaccination trials generate insightful longitudinal transcriptomic data. However, the scattering of these datasets across platforms, along with inconsistent metadata and preprocessing procedure, hinders AI-driven discovery. To address these challenges, we developed the Human Respiratory Viral Immunization LongitudinAl Gene Expression (HR-VILAGE-3K3M) repository: an AI-ready, rigorously curated dataset that integrates 14,136 RNA-seq profiles from 3,178 subjects across 66 studies encompassing over 2.56 million cells. Spanning vaccination, inoculation, and mixed exposures, the dataset includes microarray, bulk RNA-seq, and single-cell RNA-seq from whole blood, PBMCs, and nasal swabs, sourced from GEO, ImmPort, and ArrayExpress. We harmonized subject-level metadata, standardized outcome measures, applied unified preprocessing pipelines with rigorous quality control, and aligned all data to official gene symbols. To demonstrate the utility of HR-VILAGE-3K3M, we performed predictive modeling of vaccine responders and evaluated batch-effect correction methods. Beyond these initial demonstrations, it supports diverse systems immunology applications and benchmarking of feature selection and transfer learning algorithms. Its scale and heterogeneity also make it ideal for pretraining foundation models of the human immune response and for advancing multimodal learning frameworks. As the largest longitudinal transcriptomic resource for human respiratory viral immunization, it provides an accessible platform for reproducible AI-driven research, accelerating systems immunology and vaccine development against emerging viral threats.",Bioinformatics,http://arxiv.org/abs/2505.14725v1,arXiv,1
"The advent of single-cell Assay for Transposase-Accessible Chromatin using sequencing (scATAC-seq) offers an innovative perspective for deciphering regulatory mechanisms by assembling a vast repository of single-cell chromatin accessibility data. While foundation models have achieved significant success in single-cell transcriptomics, there is currently no foundation model for scATAC-seq that supports zero-shot high-quality cell identification and comprehensive multi-omics analysis simultaneously. Key challenges lie in the high dimensionality and sparsity of scATAC-seq data, as well as the lack of a standardized schema for representing open chromatin regions (OCRs). Here, we present ChromFound, a foundation model tailored for scATAC-seq. ChromFound utilizes a hybrid architecture and genome-aware tokenization to effectively capture genome-wide long contexts and regulatory signals from dynamic chromatin landscapes. Pretrained on 1.97 million cells from 30 tissues and 6 disease conditions, ChromFound demonstrates broad applicability across 6 diverse tasks. Notably, it achieves robust zero-shot performance in generating universal cell representations and exhibits excellent transferability in cell type annotation and cross-omics prediction. By uncovering enhancer-gene links undetected by existing computational methods, ChromFound offers a promising framework for understanding disease risk variants in the noncoding genome.",Bioinformatics,http://arxiv.org/abs/2505.12638v3,arXiv,1
"Single-cell RNA sequencing (scRNA-seq) reveals cell heterogeneity, with cell clustering playing a key role in identifying cell types and marker genes. Recent advances, especially graph neural networks (GNNs)-based methods, have significantly improved clustering performance. However, the analysis of scRNA-seq data remains challenging due to noise, sparsity, and high dimensionality. Compounding these challenges, GNNs often suffer from over-smoothing, limiting their ability to capture complex biological information. In response, we propose scSiameseClu, a novel Siamese Clustering framework for interpreting single-cell RNA-seq data, comprising of 3 key steps: (1) Dual Augmentation Module, which applies biologically informed perturbations to the gene expression matrix and cell graph relationships to enhance representation robustness; (2) Siamese Fusion Module, which combines cross-correlation refinement and adaptive information fusion to capture complex cellular relationships while mitigating over-smoothing; and (3) Optimal Transport Clustering, which utilizes Sinkhorn distance to efficiently align cluster assignments with predefined proportions while maintaining balance. Comprehensive evaluations on seven real-world datasets demonstrate that scSiameseClu outperforms state-of-the-art methods in single-cell clustering, cell type annotation, and cell type classification, providing a powerful tool for scRNA-seq data interpretation.",Bioinformatics,http://arxiv.org/abs/2505.12626v3,arXiv,1
"This paper surveys foundation models for AI-enabled biological design, focusing on recent developments in applying large-scale, self-supervised models to tasks such as protein engineering, small molecule design, and genomic sequence design. Though this domain is evolving rapidly, this survey presents and discusses a taxonomy of current models and methods. The focus is on challenges and solutions in adapting these models for biological applications, including biological sequence modeling architectures, controllability in generation, and multi-modal integration. The survey concludes with a discussion of open problems and future directions, offering concrete next-steps to improve the quality of biological sequence generation.",Bioinformatics,http://arxiv.org/abs/2505.11610v1,arXiv,1
"Compendium Manager is a command-line tool written in Python to automate the provisioning, launch, and evaluation of bioinformatics pipelines. Although workflow management tools such as Snakemake and Nextflow enable users to automate the processing of samples within a single sequencing project, integrating many datasets in bulk requires launching and monitoring hundreds or thousands of pipelines. We present the Compendium Manager, a lightweight command-line tool to enable launching and monitoring analysis pipelines at scale. The tool can gauge progress through a list of projects, load results into a shared database, and record detailed processing metrics for later evaluation and reproducibility.",Bioinformatics,http://arxiv.org/abs/2505.11385v1,arXiv,1
"Colorectal cancer remains a major global health concern, with early detection being pivotal for improving patient outcomes. In this study, we leveraged high throughput methylation profiling of cellfree DNA to identify and validate diagnostic biomarkers for CRC. The GSE124600 study data were downloaded from the Gene Expression Omnibus, as the discovery cohort, comprising 142 CRC and 132 normal cfDNA methylation profiles obtained via MCTA seq. After preprocessing and filtering, 97,863 CpG sites were retained for further analysis. Differential methylation analysis using statistical tests identified 30,791 CpG sites as significantly altered in CRC samples, where p is less than 0.05. Univariate scoring enabled the selection of top ranking features, which were further refined using multiple feature selection algorithms, including Recursive Feature Elimination, Sequential Feature Selection, and SVC L1. Various machine learning models such as Logistic Regression, Support Vector Machines, Random Forest, and Multi layer Perceptron were trained and tested using independent validation datasets. The best performance was achieved with an MLP model trained on 25 features selected by RFE, reaching an AUROC of 0.89 and MCC of 0.78 on validation data. Additionally, a deep learning based convolutional neural network achieved an AUROC of 0.78. Functional annotation of the most predictive CpG sites identified several genes involved in key cellular processes, some of which were validated for differential expression in CRC using the GEPIA2 platform. Our study highlights the potential of cfDNA methylation markers combined with ML and DL models for noninvasive and accurate CRC detection, paving the way for clinically relevant diagnostic tools.",Bioinformatics,http://arxiv.org/abs/2505.11041v1,arXiv,1
"Obesity prevalence in Indonesian adults increased from 10.5% in 2007 to 23.4% in 2023. Studies showed that genetic predisposition significantly influences obesity susceptibility. To aid this, polygenic risk scores (PRS) help aggregate the effects of numerous genetic variants to assess genetic risk. However, 91% of genome-wide association studies (GWAS) involve European populations, limiting their applicability to Indonesians due to genetic diversity. This study aims to develop and validate an ancestry adjusted PRS for obesity in the Indonesian population using principal component analysis (PCA) method constructed from the 1000 Genomes Project data and our own genomic data from approximately 2,800 Indonesians. We calculate PRS for obesity using all races, then determine the first four principal components using ancestry-informative SNPs and develop a linear regression model to predict PRS based on these principal components. The raw PRS is adjusted by subtracting the predicted score to obtain an ancestry adjusted PRS for the Indonesian population. Our results indicate that the ancestry-adjusted PRS improves obesity risk prediction. Compared to the unadjusted PRS, the adjusted score improved classification performance with a 5% increase in area under the ROC curve (AUC). This approach underscores the importance of population-specific adjustments in genetic risk assessments to enable more effective personalized healthcare and targeted intervention strategies for diverse populations.",Bioinformatics,http://arxiv.org/abs/2505.13503v1,arXiv,1
"The investigation of plant transcriptional regulation constitutes a fundamental basis for crop breeding, where cis-regulatory elements (CREs), as the key factor determining gene expression, have become the focus of crop genetic improvement research. Deep learning techniques, leveraging their exceptional capacity for high-dimensional feature extraction and nonlinear regulatory relationship modeling, have been extensively employed in this field. However, current methodologies present notable limitations: single CNN-based architectures struggle to capture long-range regulatory interactions, while existing CNN-Transformer hybrid models demonstrate proneness to overfitting and inadequate generalization in cross-species prediction contexts. To address these challenges, this study proposes DeepPlantCRE, a deep-learning framework for plant gene expression prediction and CRE Extraction. The model employs a Transformer-CNN hybrid architecture that achieves enhanced Accuracy, AUC-ROC, and F1-score metrics over existing baselines (DeepCRE and PhytoExpr), with improved generalization performance and overfitting inhibiting. Cross-species validation experiments conducted on gene expression datasets from \textit{Gossypium}, \textit{Arabidopsis thaliana}, \textit{Solanum lycopersicum}, \textit{Sorghum bicolor}, and \textit{Arabidopsis thaliana} reveal that the model achieves peak prediction accuracy of 92.3\%, particularly excelling in complex genomic data analysis. Furthermore, interpretability investigations using DeepLIFT and Transcription Factor Motif Discovery from the importance scores algorithm (TF-MoDISco) demonstrate that the derived motifs from our model exhibit high concordance with known transcription factor binding sites (TFBSs) such as MYR2, TSO1 in JASPAR plant database, substantiating the potential of biological interpretability and practical agricultural application of DeepPlantCRE.",Bioinformatics,http://arxiv.org/abs/2505.09883v1,arXiv,1
"Deep learning-based AI models have been extensively applied in genomics, achieving remarkable success across diverse applications. As these models gain prominence, there exists an urgent need for interpretability methods to establish trustworthiness in model-driven decisions. For genetic researchers, interpretable insights derived from these models hold significant value in providing novel perspectives for understanding biological processes. Current interpretability analyses in genomics predominantly rely on intuition and experience rather than rigorous theoretical foundations. In this review, we systematically categorize interpretability methods into input-based and model-based approaches, while critically evaluating their limitations through concrete biological application scenarios. Furthermore, we establish theoretical underpinnings to elucidate the origins of these constraints through formal mathematical demonstrations, aiming to assist genetic researchers in better understanding and designing models in the future. Finally, we provide feasible suggestions for future research on interpretability in the field of genetics.",Bioinformatics,http://arxiv.org/abs/2505.09873v1,arXiv,1
"The emergence of telomere-to-telomere (T2T) genome assemblies has opened new avenues for comparative genomics, yet effective tokenization strategies for genomic sequences remain underexplored. In this pilot study, we apply Byte Pair Encoding (BPE) to nine T2T primate genomes including three human assemblies by training independent BPE tokenizers with a fixed vocabulary of 512,000 tokens using our custom tool, dnaBPE. Our analysis reveals that only 11,569 tokens are shared across all assemblies, while nearly 991,854 tokens are unique to a single genome, indicating a rapid decline in shared vocabulary with increasing assembly comparisons. Moreover, phylogenetic trees derived from token overlap failed to recapitulate established primate relationships, a discrepancy attributed to the disproportionate influence of species-specific high-copy repetitive elements. These findings underscore the dual nature of BPE tokenization: while it effectively compresses repetitive sequences, its sensitivity to high-copy elements limits its utility as a universal tool for comparative genomics. We discuss potential hybrid strategies and repeat-masking approaches to refine genomic tokenization, emphasizing the need for domain-specific adaptations in the development of large-scale genomic language models. The dnaBPE tool used in this study is open-source and available at https://github.com/aglabx/dnaBPE.",Bioinformatics,http://arxiv.org/abs/2505.08918v1,arXiv,1
"All cells respond to changes in both their internal milieu and the environment around them through the regulation of their genes. Despite decades of effort, there remain huge gaps in our knowledge of both the function of many genes (the so-called y-ome) and how they adapt to changing environments via regulation. Here we describe a joint experimental and theoretical dissection of the regulation of a broad array of over 100 biologically interesting genes in E. coli across 39 diverse environments, permitting us to discover the binding sites and transcription factors that mediate regulatory control. Using a combination of mutagenesis, massively parallel reporter assays, mass spectrometry and tools from information theory and statistical physics, we go from complete ignorance of a promoter's environment-dependent regulatory architecture to predictive models of its behavior. As a proof of principle of the biological insights to be gained from such a study, we chose a combination of genes from the y-ome, toxin-antitoxin pairs, and genes hypothesized to be part of regulatory modules; in all cases, we discovered a host of new insights into their underlying regulatory landscape and resulting biological function.",Bioinformatics,http://arxiv.org/abs/2505.08764v1,arXiv,1
"Cell type annotation is a critical yet laborious step in single-cell RNA sequencing analysis. We present a trustworthy large language model (LLM)-agent, CellTypeAgent, which integrates LLMs with verification from relevant databases. CellTypeAgent achieves higher accuracy than existing methods while mitigating hallucinations. We evaluated CellTypeAgent across nine real datasets involving 303 cell types from 36 tissues. This combined approach holds promise for more efficient and reliable cell type annotation.",Bioinformatics,http://arxiv.org/abs/2505.08844v1,arXiv,1
"The rise of large language models and multi-agent systems has sparked growing interest in AI scientists capable of autonomous biological research. However, existing benchmarks either focus on reasoning without data or on data analysis with predefined statistical answers, lacking realistic, data-driven evaluation settings. Here, we introduce the Biological AI Scientist Benchmark (BaisBench), a benchmark designed to assess AI scientists' ability to generate biological discoveries through data analysis and reasoning with external knowledge. BaisBench comprises two tasks: cell type annotation on 31 expert-labeled single-cell datasets, and scientific discovery through answering 198 multiple-choice questions derived from the biological insights of 41 recent single-cell studies. Systematic experiments on state-of-the-art AI scientists and LLM agents showed that while promising, current models still substantially underperform human experts on both tasks. We hope BaisBench will fill this gap and serve as a foundation for advancing and evaluating AI models for scientific discovery. The benchmark can be found at: https://github.com/EperLuo/BaisBench.",Bioinformatics,http://arxiv.org/abs/2505.08341v1,arXiv,1
"De novo assembly enables investigations of unknown genomes, paving the way for personalized medicine and disease management. However, it faces immense computational challenges arising from the excessive data volumes and algorithmic complexity.   While state-of-the-art de novo assemblers utilize distributed systems for extreme-scale genome assembly, they demand substantial computational and memory resources. They also fail to address the inherent challenges of de novo assembly, including a large memory footprint, memory-bound behavior, and irregular data patterns stemming from complex, interdependent data structures. Given these challenges, de novo assembly merits a custom hardware solution, though existing approaches have not fully addressed the limitations.   We propose NMP-PaK, a hardware-software co-design that accelerates scalable de novo genome assembly through near-memory processing (NMP). Our channel-level NMP architecture addresses memory bottlenecks while providing sufficient scratchpad space for processing elements. Customized processing elements maximize parallelism while efficiently handling large data structures that are both dynamic and interdependent. Software optimizations include customized batch processing to reduce the memory footprint and hybrid CPU-NMP processing to address hardware underutilization caused by irregular data patterns.   NMP-PaK conducts the same genome assembly while incurring a 14X smaller memory footprint compared to the state-of-the-art de novo assembly. Moreover, NMP-PaK delivers a 16X performance improvement over the CPU baseline, with a 2.4X reduction in memory operations. Consequently, NMP-PaK achieves 8.3X greater throughput than state-of-the-art de novo assembly under the same resource constraints, showcasing its superior computational efficiency.",Bioinformatics,http://arxiv.org/abs/2505.08071v1,arXiv,1
"Chloroplast sequences from the Lamiales order were analyzed using the Pangenome Research Toolkit (PGR-TK). Overall, most genera and families exhibited a high degree of sequence uniformity. However, at the genus level, Utricularia, Incarvillea, and Orobanche stood out as particularly divergent. At the family level, Orobanchaceae, Bignoniaceae and Lentibulariaceae displayed notably complex patterns in the generated plots. The PGR-TK algorithm successfully distinguished most genera within their respective families and often recognized misclassified plants.",Bioinformatics,http://arxiv.org/abs/2505.07740v1,arXiv,1
"A significant advancement in bioinformatics is using genome graph techniques to improve variation discovery across organisms. Traditional approaches, such as bwa mem, rely on linear reference genomes for genomic analyses but may introduce biases when applied to highly diverse bacterial genomes of the same species. Pangenome graphs provide an alternative paradigm for evaluating structural and minor variations within a graphical framework, including insertions, deletions, and single nucleotide polymorphisms. Pangenome graphs enhance the detection and interpretation of complex genetic variants by representing the full genetic diversity of a species. In this study, we present a robust and reliable bioinformatics pipeline utilising the PanGenome Graph Builder (PGGB) and the Variation Graph toolbox (vg giraffe) to align whole-genome sequencing data, call variants against a graph reference, and construct pangenomes from assembled genomes. Our results demonstrate that leveraging pangenome graphs over a single linear reference genome significantly improves mapping rates and variant calling accuracy for simulated and actual bacterial pathogens datasets.",Bioinformatics,http://arxiv.org/abs/2505.07919v1,arXiv,1
"Understanding cell identity and function through single-cell level sequencing data remains a key challenge in computational biology. We present a novel framework that leverages gene-specific textual annotations from the NCBI Gene database to generate biologically contextualized cell embeddings. For each cell in a single-cell RNA sequencing (scRNA-seq) dataset, we rank genes by expression level, retrieve their NCBI Gene descriptions, and transform these descriptions into vector embedding representations using large language models (LLMs). The models used include OpenAI text-embedding-ada-002, text-embedding-3-small, and text-embedding-3-large (Jan 2024), as well as domain-specific models BioBERT and SciBERT. Embeddings are computed via an expression-weighted average across the top N most highly expressed genes in each cell, providing a compact, semantically rich representation. This multimodal strategy bridges structured biological data with state-of-the-art language modeling, enabling more interpretable downstream applications such as cell-type clustering, cell vulnerability dissection, and trajectory inference.",Bioinformatics,http://arxiv.org/abs/2505.07896v1,arXiv,1
"Duplicate marking is a critical preprocessing step in gene sequence analysis to flag redundant reads arising from polymerase chain reaction(PCR) amplification and sequencing artifacts. Although Picard MarkDuplicates is widely recognized as the gold-standard tool, its single-threaded implementation and reliance on global sorting result in significant computational and resource overhead, limiting its efficiency on large-scale datasets. Here, we introduce FastDup: a high-performance, scalable solution that follows the speculation-and-test mechanism. FastDup achieves up to 20x throughput speedup and guarantees 100\% identical output compared to Picard MarkDuplicates. FastDup is a C++ program available from GitHub (https://github.com/zzhofict/FastDup.git) under the MIT license.",Bioinformatics,http://arxiv.org/abs/2505.06127v1,arXiv,1
"This paper describes a new asynchronous algorithm and implementation for the problem of k-mer counting (KC), which concerns quantifying the frequency of length k substrings in a DNA sequence. This operation is common to many computational biology workloads and can take up to 77% of the total runtime of de novo genome assembly. The performance and scalability of the current state-of-the-art distributed-memory KC algorithm are hampered by multiple rounds of Many-To-Many collectives. Therefore, we develop an asynchronous algorithm (DAKC) that uses fine-grained, asynchronous messages to obviate most of this global communication while utilizing network bandwidth efficiently via custom message aggregation protocols. DAKC can perform strong scaling up to 256 nodes (512 sockets / 6K cores) and can count k-mers up to 9x faster than the state-of-the-art distributed-memory algorithm, and up to 100x faster than the shared-memory alternative. We also provide an analytical model to understand the hardware resource utilization of our asynchronous KC algorithm and provide insights on the performance.",Bioinformatics,http://arxiv.org/abs/2505.04431v1,arXiv,1
"Gene finding is the task of identifying the locations of coding sequences within the vast amount of genetic code contained in the genome. With an ever increasing quantity of raw genome sequences, gene finding is an important avenue towards understanding the genetic information of (novel) organisms, as well as learning shared patterns across evolutionarily diverse species. The current state of the art are graphical models usually trained per organism and requiring manually curated datasets. However, these models lack the flexibility to incorporate deep learning representation learning techniques that have in recent years been transformative in the analysis of pro tein sequences, and which could potentially help gene finders exploit the growing number of the sequenced genomes to expand performance across multiple organisms. Here, we propose a novel approach, combining learned embeddings of raw genetic sequences with exact decoding using a latent conditional random field. We show that the model achieves performance matching the current state of the art, while increasing training robustness, and removing the need for manually fitted length distributions. As language models for DNA improve, this paves the way for more performant cross-organism gene-finders.",Bioinformatics,http://arxiv.org/abs/2505.03377v1,arXiv,1
"Predicting genetic perturbations enables the identification of potentially crucial genes prior to wet-lab experiments, significantly improving overall experimental efficiency. Since genes are the foundation of cellular life, building gene regulatory networks (GRN) is essential to understand and predict the effects of genetic perturbations. However, current methods fail to fully leverage gene-related information, and solely rely on simple evaluation metrics to construct coarse-grained GRN. More importantly, they ignore functional differences between biotypes, limiting the ability to capture potential gene interactions. In this work, we leverage pre-trained large language model and DNA sequence model to extract features from gene descriptions and DNA sequence data, respectively, which serve as the initialization for gene representations. Additionally, we introduce gene biotype information for the first time in genetic perturbation, simulating the distinct roles of genes with different biotypes in regulating cellular processes, while capturing implicit gene relationships through graph structure learning (GSL). We propose GRAPE, a heterogeneous graph neural network (HGNN) that leverages gene representations initialized with features from descriptions and sequences, models the distinct roles of genes with different biotypes, and dynamically refines the GRN through GSL. The results on publicly available datasets show that our method achieves state-of-the-art performance.",Bioinformatics,http://arxiv.org/abs/2505.03853v1,arXiv,1
"DNA microarray technology enables the simultaneous measurement of expression levels of thousands of genes, thereby facilitating the understanding of the molecular mechanisms underlying complex diseases such as brain tumors and the identification of diagnostic genetic signatures. To derive meaningful biological insights from the high-dimensional and complex gene features obtained through this technology and to analyze gene properties in detail, classical AI-based approaches such as machine learning and deep learning are widely employed. However, these methods face various limitations in managing high-dimensional vector spaces and modeling the intricate relationships among genes. In particular, challenges such as hyperparameter tuning, computational costs, and high processing power requirements can hinder their efficiency. To overcome these limitations, quantum computing and quantum AI approaches are gaining increasing attention. Leveraging quantum properties such as superposition and entanglement, quantum methods enable more efficient parallel processing of high-dimensional data and offer faster and more effective solutions to problems that are computationally demanding for classical methods. In this study, a novel model called ""Deep VQC"" is proposed, based on the Variational Quantum Classifier approach. Developed using microarray data containing 54,676 gene features, the model successfully classified four different types of brain tumors-ependymoma, glioblastoma, medulloblastoma, and pilocytic astrocytoma-alongside healthy samples with high accuracy. Furthermore, compared to classical ML algorithms, our model demonstrated either superior or comparable classification performance. These results highlight the potential of quantum AI methods as an effective and promising approach for the analysis and classification of complex structures such as brain tumors based on gene expression features.",Bioinformatics,http://arxiv.org/abs/2505.02033v1,arXiv,1
"Integrating heterogeneous biomedical data including imaging, omics, and clinical records supports accurate diagnosis and personalised care. Graph-based models fuse such non-Euclidean data by capturing spatial and relational structure, yet clinical uptake requires regulator-ready interpretability. We present the first technical survey of interpretable graph based models for multimodal biomedical data, covering 26 studies published between Jan 2019 and Sep 2024. Most target disease classification, notably cancer and rely on static graphs from simple similarity measures, while graph-native explainers are rare; post-hoc methods adapted from non-graph domains such as gradient saliency, and SHAP predominate. We group existing approaches into four interpretability families, outline trends such as graph-in-graph hierarchies, knowledge-graph edges, and dynamic topology learning, and perform a practical benchmark. Using an Alzheimer disease cohort, we compare Sensitivity Analysis, Gradient Saliency, SHAP and Graph Masking. SHAP and Sensitivity Analysis recover the broadest set of known AD pathways and Gene-Ontology terms, whereas Gradient Saliency and Graph Masking surface complementary metabolic and transport signatures. Permutation tests show all four beat random gene sets, but with distinct trade-offs: SHAP and Graph Masking offer deeper biology at higher compute cost, while Gradient Saliency and Sensitivity Analysis are quicker though coarser. We also provide a step-by-step flowchart covering graph construction, explainer choice and resource budgeting to help researchers balance transparency and performance. This review synthesises the state of interpretable graph learning for multimodal medicine, benchmarks leading techniques, and charts future directions, from advanced XAI tools to under-studied diseases, serving as a concise reference for method developers and translational scientists.",Bioinformatics,http://arxiv.org/abs/2505.01696v1,arXiv,1
"Mixture interpretation is a central challenge in forensic science, where evidence often contains contributions from multiple sources. In the context of DNA analysis, biological samples recovered from crime scenes may include genetic material from several individuals, necessitating robust statistical tools to assess whether a specific person of interest (POI) is among the contributors. Methods based on capillary electrophoresis (CE) are currently in use worldwide, but offer limited resolution in complex mixtures. Advancements in massively parallel sequencing (MPS) technologies provide a richer, more detailed representation of DNA mixtures, but require new analytical strategies to fully leverage this information. In this work, we present a Bayesian framework for evaluating whether a POIs DNA is present in an MPS-based forensic sample. The model accommodates known contributors, such as the victim, and uses a novel string edit distance to quantify similarity between observed alleles and sequencing artifacts. The resulting Bayes factors enable effective discrimination between samples that do and do not contain the POIs DNA, demonstrating strong performance in both hypothesis testing and classification settings.",Bioinformatics,http://arxiv.org/abs/2505.00934v1,arXiv,1
"Unsupervised learning of disease subtypes from multi-omics data presents a significant opportunity for advancing personalized medicine. We introduce OmicsCL, a modular contrastive learning framework that jointly embeds heterogeneous omics modalities-such as gene expression, DNA methylation, and miRNA expression-into a unified latent space. Our method incorporates a survival-aware contrastive loss that encourages the model to learn representations aligned with survival-related patterns, without relying on labeled outcomes. Evaluated on the TCGA BRCA dataset, OmicsCL uncovers clinically meaningful clusters and achieves strong unsupervised concordance with patient survival. The framework demonstrates robustness across hyperparameter configurations and can be tuned to prioritize either subtype coherence or survival stratification. Ablation studies confirm that integrating survival-aware loss significantly enhances the predictive power of learned embeddings. These results highlight the promise of contrastive objectives for biological insight discovery in high-dimensional, heterogeneous omics data.",Bioinformatics,http://arxiv.org/abs/2505.00650v1,arXiv,1
"Aim: This in silico study sought to identify specific biomarkers for mild traumatic brain injury (mTBI) through the analysis of publicly available gene and miRNA databases, hypothesizing their influence on neuronal structure, axonal integrity, and regeneration. Methods: This study implemented a three-step process: (1) Data searching for mTBI-related genes in Gene and MalaCard databases and literature review ; (2) Data analysis involved performing functional annotation through GO and KEGG, identifying hub genes using Cytoscape, mapping protein-protein interactions via DAVID and STRING, and predicting miRNA targets using miRSystem, miRWalk2.0, and mirDIP (3) RNA-sequencing analysis applied to the mTBI dataset GSE123336. Results: Eleven candidate hub genes associated with mTBI outcome were identified: APOE, S100B, GFAP, BDNF, AQP4, COMT, MBP, UCHL1, DRD2, ASIC1, and CACNA1A. Enrichment analysis linked these genes to neuron projection regeneration and synaptic plasticity. miRNAs linked to the mTBI candidate genes were hsa-miR-9-5p, hsa-miR-204-5p, hsa-miR-1908-5p, hsa-miR-16-5p, hsa-miR-10a-5p, has-miR-218-5p, has-miR-34a-5p, and has-miR-199b-5p. The RNA sequencing revealed 2664 differentially expressed miRNAs post-mTBI, with 17 showing significant changes at the time of injury and 48 hours post-injury. Two miRNAs were positively correlated with direct head hits. Conclusion: Our study indicates that specific genes and miRNAs, particularly hsa-miR-10a-5p, may influence mTBI outcomes. Our research may guide future mTBI diagnostics, emphasizing the need to measure and track these specific genes and miRNAs in diverse cohorts.",Bioinformatics,http://arxiv.org/abs/2505.00572v1,arXiv,1
"Background: Praying mantises, members of the order Mantodea, play important roles in agriculture, medicine, bionics, and entertainment. However, the scarcity of genomic resources has hindered extensive studies on mantis evolution and behaviour. Results: Here, we present the chromosome-scale reference genomes of five mantis species: the European mantis (Mantis religiosa), Chinese mantis (Tenodera sinensis), triangle dead leaf mantis (Deroplatys truncata), orchid mantis (Hymenopus coronatus), and metallic mantis (Metallyticus violaceus). We found that transposable element expansion is the major force governing genome size in Mantodea. Based on whole-alignments, we deduced that the Mantodea ancestor may have had only one X chromosome and that translocations between the X chromosome and an autosome may have occurred in the lineage of the superfamily Mantoidea. Furthermore, we found a lower evolutionary rate for the metallic mantis than for the other mantises. We also found that Mantodea underwent rapid radiation after the K-Pg mass extinction event, which could have contributed to the confusion in species classification. Conclusions: We present the chromosome-scale reference genomes of five mantis species to reveal the X-chromosome evolution, clarify the phylogeny relationship, and transposable element expansion.",Bioinformatics,http://arxiv.org/abs/2504.20328v1,arXiv,1
"We present a novel approach for taxonomic analysis of chloroplast genomes in angiosperms using the Pan-genome Research Toolkit (PGR-TK). Comparative plots generated by PGR-TK across diverse angiosperm genera reveal a wide range of structural complexity, from straightforward to highly intricate patterns. Notably, the characteristic quadripartite plastome structure, comprising the large single copy (LSC), small single copy (SSC), and inverted repeat (IR) regions, is clearly identifiable in over 75% of the genera analyzed. Our findings also underscore several occurrences of species mis-annotations in public genomic databases, which are readily detected through visual anomalies in the PGR-TK plots. While more complex plot patterns remain difficult to interpret, they likely reflect underlying biological variation or technical inconsistencies in genome assembly. Overall, this approach effectively integrates classical botanical visualization with modern molecular taxonomy, providing a powerful tool for genome-based classification in plant systematics.",Bioinformatics,http://arxiv.org/abs/2504.20034v1,arXiv,1
"Mappings from biological sequences (DNA, RNA, protein) to quantitative measures of sequence functionality play an important role in contemporary biology. We are interested in the related tasks of (i) inferring predictive sequence-to-function maps and (ii) decomposing sequence-function maps to elucidate the contributions of individual subsequences. Because each sequence-function map can be written as a weighted sum over subsequences in multiple ways, meaningfully interpreting these weights requires ``gauge-fixing,'' i.e., defining a unique representation for each map. Recent work has established that most existing gauge-fixed representations arise as the unique solutions to $L_2$-regularized regression in an overparameterized ``weight space'' where the choice of regularizer defines the gauge. Here, we establish the relationship between regularized regression in overparameterized weight space and Gaussian process approaches that operate in ``function space,'' i.e.~the space of all real-valued functions on a finite set of sequences. We disentangle how weight space regularizers both impose an implicit prior on the learned function and restrict the optimal weights to a particular gauge. We show how to construct regularizers that correspond to arbitrary explicit Gaussian process priors combined with a wide variety of gauges and characterize the implicit function space priors associated with the most common weight space regularizers. Finally, we derive the posterior distribution of a broad class of sequence-to-function statistics, including gauge-fixed weights and multiple systems for expressing higher-order epistatic coefficients. We show that such distributions can be efficiently computed for product-kernel priors using a kernel trick.",Bioinformatics,http://arxiv.org/abs/2504.19034v3,arXiv,1
"Next-generation sequencing (NGS) is a pivotal technique in genome sequencing due to its high throughput, rapid results, cost-effectiveness, and enhanced accuracy. Its significance extends across various domains, playing a crucial role in identifying genetic variations and exploring genomic complexity. NGS finds applications in diverse fields such as clinical genomics, comparative genomics, functional genomics, and metagenomics, contributing substantially to advancements in research, medicine, and scientific disciplines. Within the sphere of genomics data science, the execution of read simulation, mapping, and variant calling holds paramount importance for obtaining precise and dependable results. Given the plethora of tools available for these purposes, each employing distinct methodologies and options, a nuanced understanding of their intricacies becomes imperative for optimization. This research, situated at the intersection of data science and genomics, involves a meticulous assessment of various tools, elucidating their individual strengths and weaknesses through rigorous experimentation and analysis. This comprehensive evaluation has enabled the researchers to pinpoint the most accurate tools, reinforcing the alignment between the established workflow and the demonstrated efficacy of specific tools in the context of genomics data analysis. To meet these requirements, ""VarFind"", an open-source and freely accessible pipeline tool designed to automate the entire process has been introduced (VarFind GitHub repository: https://github.com/shanikawm/varfinder)",Bioinformatics,http://arxiv.org/abs/2504.17860v1,arXiv,1
"The subcellular localization of RNAs, including long non-coding RNAs (lncRNAs), messenger RNAs (mRNAs), microRNAs (miRNAs) and other smaller RNAs, plays a critical role in determining their biological functions. For instance, lncRNAs are predominantly associated with chromatin and act as regulators of gene transcription and chromatin structure, while mRNAs are distributed across the nucleus and cytoplasm, facilitating the transport of genetic information for protein synthesis. Understanding RNA localization sheds light on processes like gene expression regulation with spatial and temporal precision. However, traditional wet lab methods for determining RNA localization, such as in situ hybridization, are often time-consuming, resource-demanding, and costly. To overcome these challenges, computational methods leveraging artificial intelligence (AI) and machine learning (ML) have emerged as powerful alternatives, enabling large-scale prediction of RNA subcellular localization. This paper provides a comprehensive review of the latest advancements in AI-based approaches for RNA subcellular localization prediction, covering various RNA types and focusing on sequence-based, image-based, and hybrid methodologies that combine both data types. We highlight the potential of these methods to accelerate RNA research, uncover molecular pathways, and guide targeted disease treatments. Furthermore, we critically discuss the challenges in AI/ML approaches for RNA subcellular localization, such as data scarcity and lack of benchmarks, and opportunities to address them. This review aims to serve as a valuable resource for researchers seeking to develop innovative solutions in the field of RNA subcellular localization and beyond.",Bioinformatics,http://arxiv.org/abs/2504.17162v1,arXiv,1
"The inference of gene regulatory networks (GRNs) is a foundational stride towards deciphering the fundamentals of complex biological systems. Inferring a possible regulatory link between two genes can be formulated as a link prediction problem. Inference of GRNs via gene coexpression profiling data may not always reflect true biological interactions, as its susceptibility to noise and misrepresenting true biological regulatory relationships. Most GRN inference methods face several challenges in the network reconstruction phase. Therefore, it is important to encode gene expression values, leverege the prior knowledge gained from the available inferred network structures and positional informations of the input network nodes towards inferring a better and more confident GRN network reconstruction. In this paper, we explore the integration of multiple inferred networks to enhance the inference of Gene Regulatory Networks (GRNs). Primarily, we employ autoencoder embeddings to capture gene expression patterns directly from raw data, preserving intricate biological signals. Then, we embed the prior knowledge from GRN structures transforming them into a text-like representation using random walks, which are then encoded with a masked language model, BERT, to generate global embeddings for each gene across all networks. Additionally, we embed the positional encodings of the input gene networks to better identify the position of each unique gene within the graph. These embeddings are integrated into graph transformer-based model, termed GT-GRN, for GRN inference. The GT-GRN model effectively utilizes the topological structure of the ground truth network while incorporating the enriched encoded information. Experimental results demonstrate that GT-GRN significantly outperforms existing GRN inference methods, achieving superior accuracy and highlighting the robustness of our approach.",Bioinformatics,http://arxiv.org/abs/2504.16961v1,arXiv,1
"Single-cell RNA sequencing (scRNA-seq) enables high-resolution analysis of cellular heterogeneity, but its complexity, which is marked by high dimensionality, sparsity, and batch effects, which poses major computational challenges. Transformer-based models have made significant advances in this domain but are often limited by their quadratic complexity and suboptimal handling of long-range dependencies. In this work, we introduce GeneMamba, a scalable and efficient foundation model for single-cell transcriptomics built on state space modeling. Leveraging the Bi-Mamba architecture, GeneMamba captures bidirectional gene context with linear-time complexity, offering substantial computational gains over transformer baselines. The model is pretrained on nearly 30 million cells and incorporates biologically informed objectives, including pathway-aware contrastive loss and rank-based gene encoding. We evaluate GeneMamba across diverse tasks, including multi-batch integration, cell type annotation, and gene-gene correlation, demonstrating strong performance, interpretability, and robustness. These results position GeneMamba as a practical and powerful alternative to transformer-based methods, advancing the development of biologically grounded, scalable tools for large-scale single-cell data analysis.",Bioinformatics,http://arxiv.org/abs/2504.16956v2,arXiv,1
"Recent advances in applying deep learning in genomics include DNA-language and single-cell foundation models. However, these models take only one data type as input. We introduce dynamic token adaptation and demonstrate how it combines these models to predict gene regulation at the single-cell level in different genetic contexts. Although the method is generalisable, we focus on an illustrative example by training an adapter from DNA-sequence embeddings to a single-cell foundation model's token embedding space. As a qualitative evaluation, we assess the impact of DNA sequence changes on the model's learned gene regulatory networks by mutating the transcriptional start site of the transcription factor GATA4 in silico, observing predicted expression changes in its target genes in fetal cardiomyocytes.",Bioinformatics,http://arxiv.org/abs/2504.13049v1,arXiv,1
"Background: Spatial transcriptomics have emerged as a powerful tool in biomedical research because of its ability to capture both the spatial contexts and abundance of the complete RNA transcript profile in organs of interest. However, limitations of the technology such as the relatively low resolution and comparatively insufficient sequencing depth make it difficult to reliably extract real biological signals from these data. To alleviate this challenge, we propose a novel transfer learning framework, referred to as TransST, to adaptively leverage the cell-labeled information from external sources in inferring cell-level heterogeneity of a target spatial transcriptomics data.   Results: Applications in several real studies as well as a number of simulation settings show that our approach significantly improves existing techniques. For example, in the breast cancer study, TransST successfully identifies five biologically meaningful cell clusters, including the two subgroups of cancer in situ and invasive cancer; in addition, only TransST is able to separate the adipose tissues from the connective issues among all the studied methods.   Conclusions: In summary, the proposed method TransST is both effective and robust in identifying cell subclusters and detecting corresponding driving biomarkers in spatial transcriptomics data.",Bioinformatics,http://arxiv.org/abs/2504.12353v1,arXiv,1
"Predicting phenotype from genotype is a central challenge in genetics. Traditional approaches in quantitative genetics typically analyze this problem using methods based on linear regression. These methods generally assume that the genetic architecture of complex traits can be parameterized in terms of an additive model, where the effects of loci are independent, plus (in some cases) pairwise epistatic interactions between loci. However, these models struggle to analyze more complex patterns of epistasis or subtle gene-environment interactions. Recent advances in machine learning, particularly attention-based models, offer a promising alternative. Initially developed for natural language processing, attention-based models excel at capturing context-dependent interactions and have shown exceptional performance in predicting protein structure and function. Here, we apply attention-based models to quantitative genetics. We analyze the performance of this attention-based approach in predicting phenotype from genotype using simulated data across a range of models with increasing epistatic complexity, and using experimental data from a recent quantitative trait locus mapping study in budding yeast. We find that our model demonstrates superior out-of-sample predictions in epistatic regimes compared to standard methods. We also explore a more general multi-environment attention-based model to jointly analyze genotype-phenotype maps across multiple environments and show that such architectures can be used for ""transfer learning"" - predicting phenotypes in novel environments with limited training data.",Bioinformatics,http://arxiv.org/abs/2504.10388v1,arXiv,1
"Thalassemia, a blood disorder and one of the most prevalent hereditary genetic disorders worldwide, is often caused by copy number variations (CNVs) in the hemoglobin genes. This disorder has incredible diversity, with a large number of distinct profiles corresponding to alterations of different regions in the genes. Correctly classifying an individual's profile is critical as it impacts treatment, prognosis, and genetic counseling. However, genetic classification is challenging due to the large number of profiles worldwide, and often requires a large number of sequential tests. Targeted next generation sequencing (NGS), which characterizes segments of an individual's genome, has the potential to dramatically reduce the cost of testing and increase accuracy. In this work, we introduce a probabilistic state space model for profiling thalassemia from targeted NGS data, which naturally characterize the spatial ordering of the genes along the chromosome. We then use decision theory to choose the best profile among the different options. Due to our use of Bayesian methodology, we are also able to detect low-quality samples to be excluded from consideration, an important component of clinical screening. We evaluate our model on a dataset of 57 individuals, including both controls and cases with a variety of thalassemia profiles. Our model has a sensitivity of 0.99 and specificity of 0.93 for thalassemia detection, and accuracy of 91.5\% for characterizing subtypes. Furthermore, the specificity and accuracy rise to $0.96$ and 93.9\% when low-quality samples are excluded using our automated quality control method. This approach outperforms alternative methods, particularly in specificity, and is broadly applicable to other disorders.",Bioinformatics,http://arxiv.org/abs/2504.10338v1,arXiv,1
"Deep learning techniques have driven significant progress in various analytical tasks within 3D genomics in computational biology. However, a holistic understanding of 3D genomics knowledge remains underexplored. Here, we propose MIX-HIC, the first multimodal foundation model of 3D genome that integrates both 3D genome structure and epigenomic tracks, which obtains unified and comprehensive semantics. For accurate heterogeneous semantic fusion, we design the cross-modal interaction and mapping blocks for robust unified representation, yielding the accurate aggregation of 3D genome knowledge. Besides, we introduce the first large-scale dataset comprising over 1 million pairwise samples of Hi-C contact maps and epigenomic tracks for high-quality pre-training, enabling the exploration of functional implications in 3D genomics. Extensive experiments show that MIX-HIC can significantly surpass existing state-of-the-art methods in diverse downstream tasks. This work provides a valuable resource for advancing 3D genomics research.",Bioinformatics,http://arxiv.org/abs/2504.09060v2,arXiv,1
"Accurate diagnosis of Mendelian diseases is crucial for precision therapy and assistance in preimplantation genetic diagnosis. However, existing methods often fall short of clinical standards or depend on extensive datasets to build pretrained machine learning models. To address this, we introduce an innovative LLM-Driven multi-agent debate system (MD2GPS) with natural language explanations of the diagnostic results. It utilizes a language model to transform results from data-driven and knowledge-driven agents into natural language, then fostering a debate between these two specialized agents. This system has been tested on 1,185 samples across four independent datasets, enhancing the TOP1 accuracy from 42.9% to 66% on average. Additionally, in a challenging cohort of 72 cases, MD2GPS identified potential pathogenic genes in 12 patients, reducing the diagnostic time by 90%. The methods within each module of this multi-agent debate system are also replaceable, facilitating its adaptation for diagnosing and researching other complex diseases.",Bioinformatics,http://arxiv.org/abs/2504.07881v2,arXiv,1
"Advancements in biomedical research have driven continuous innovations in sensing and diagnostic technologies. Among these, nanopore based single molecule sensing and sequencing is rapidly emerging as a powerful and versatile sensing methodology. Advancements in nanopore based approaches require concomitant improvements in the electronic readout methods employed, from the point of low noise, bandwidth and form factor. This article focuses on current sensing circuits designed and employed for ultra low noise nanopore signal readout, addressing the fundamental limitations of traditional off chip transimpedance amplifiers (TIAs), which suffer from high input parasitic capacitance, bandwidth constraints, and increased noise at high frequencies. This review explores the latest design schemes and circuit structures classified into on-chip and off-chip TIA designs, highlighting their design implementation, performance, respective challenges and explores the interplay between noise performance, capacitance, and bandwidth across diverse transimpedance amplifier (TIA) configurations. Emphasis is placed on characterizing noise response under varying parasitic capacitance and operational frequencies, a systematic evaluation not extensively addressed in prior literature while also considering the allowable input current compliance range limitations. The review also compares the widely used Axopatch 200B system to the designs reported in literature. The findings offer valuable insights into optimizing TIA designs for enhanced signal integrity in high speed and high sensitivity applications focusing on noise reduction, impedance matching, DC blocking, and offset cancellation techniques.",Bioinformatics,http://arxiv.org/abs/2504.07734v1,arXiv,1
"As genome sequencing is finding utility in a wide variety of domains beyond the confines of traditional medical settings, its computational pipeline faces two significant challenges. First, the creation of up to 0.5 GB of data per minute imposes substantial communication and storage overheads. Second, the sequencing pipeline is bottlenecked at the basecalling step, consuming >40% of genome analysis time. A range of proposals have attempted to address these challenges, with limited success. We propose to address these challenges with a Compute-in-Memory Basecalling Accelerator (CiMBA), the first embedded ($\sim25$mm$^2$) accelerator capable of real-time, on-device basecalling, coupled with AnaLog (AL)-Dorado, a new family of analog focused basecalling DNNs. Our resulting hardware/software co-design greatly reduces data communication overhead, is capable of a throughput of 4.77 million bases per second, 24x that required for real-time operation, and achieves 17x/27x power/area efficiency over the best prior basecalling embedded accelerator while maintaining a high accuracy comparable to state-of-the-art software basecallers.",Bioinformatics,http://arxiv.org/abs/2504.07298v1,arXiv,1
"The ability to quickly and accurately identify microbial species in a sample, known as metagenomic profiling, is critical across various fields, from healthcare to environmental science. This paper introduces a novel method to profile signals coming from sequencing devices in parallel with determining their nucleotide sequences, a process known as basecalling, via a multi-objective deep neural network for simultaneous basecalling and multi-class genome classification. We introduce a new loss strategy where losses for basecalling and classification are back-propagated separately, with model weights combined for the shared layers, and a pre-configured ranking strategy allowing top-K species accuracy, giving users flexibility to choose between higher accuracy or higher speed at identifying the species. We achieve state-of-the-art basecalling accuracies, while classification accuracies meet and exceed the results of state-of-the-art binary classifiers, attaining an average of 92.5%/98.9% accuracy at identifying the top-1/3 species among a total of 17 genomes in the Wick bacterial dataset. The work presented here has implications for future studies in metagenomic profiling by accelerating the bottleneck step of matching the DNA sequence to the correct genome.",Bioinformatics,http://arxiv.org/abs/2504.07065v1,arXiv,1
"Motivation: Viruses represent the most abundant biological entities on the planet and play vital roles in diverse ecosystems. Cataloging viruses across various environments is essential for understanding their properties and functions. Metagenomic sequencing has emerged as the most comprehensive method for virus discovery, enabling the sequencing of all genetic materials, including viruses, from host or environmental samples. However, distinguishing viral sequences from the vast background of cellular organism-derived reads in metagenomic data remains a significant challenge. While several learning-based tools, such as VirSorter2 and geNomad, have shown promise in identifying viral contigs, they often experience varying degrees of false positive rates due to noise in sequencing and assembly, shared genes between viruses and their hosts, and the formation of proviruses within host genomes. This highlights the urgent need for an accurate and efficient method to evaluate the quality of viral contigs. Results: To address these challenges, we introduce ViralQC, a tool designed to assess the quality of reported viral contigs or bins. ViralQC identifies contamination regions within putative viral sequences using foundation models trained on viral and cellular genomes and estimates viral completeness through protein organization alignment. We evaluate ViralQC on multiple datasets and compare its performance against CheckV, the state-of-the-art in virus quality assessment. Notably, ViralQC correctly identifies 38% more contamination than CheckV, while maintaining a median absolute error of only 3%. In addition, ViralQC delivers more accurate results for medium- to high-quality (>50% completeness) contigs, demonstrating its superior performance in completeness estimation.",Bioinformatics,http://arxiv.org/abs/2504.05790v1,arXiv,1
"Explainability is necessary for many tasks in biomedical research. Recent explainability methods have focused on attention, gradient, and Shapley value. These do not handle data with strong associated prior knowledge and fail to constrain explainability results based on known relationships between predictive features.   We propose GraphPINE, a graph neural network (GNN) architecture leveraging domain-specific prior knowledge to initialize node importance optimized during training for drug response prediction. Typically, a manual post-prediction step examines literature (i.e., prior knowledge) to understand returned predictive features. While node importance can be obtained for gradient and attention after prediction, node importance from these methods lacks complementary prior knowledge; GraphPINE seeks to overcome this limitation. GraphPINE differs from other GNN gating methods by utilizing an LSTM-like sequential format. We introduce an importance propagation layer that unifies 1) updates for feature matrix and node importance and 2) uses GNN-based graph propagation of feature values. This initialization and updating mechanism allows for informed feature learning and improved graph representation.   We apply GraphPINE to cancer drug response prediction using drug screening and gene data collected for over 5,000 gene nodes included in a gene-gene graph with a drug-target interaction (DTI) graph for initial importance. The gene-gene graph and DTIs were obtained from curated sources and weighted by article count discussing relationships between drugs and genes. GraphPINE achieves a PR-AUC of 0.894 and ROC-AUC of 0.796 across 952 drugs. Code is available at https://anonymous.4open.science/r/GraphPINE-40DE.",Bioinformatics,http://arxiv.org/abs/2504.05454v1,arXiv,1
"Long-range dependencies are critical for understanding genomic structure and function, yet most conventional methods struggle with them. Widely adopted transformer-based models, while excelling at short-context tasks, are limited by the attention module's quadratic computational complexity and inability to extrapolate to sequences longer than those seen in training. In this work, we explore State Space Models (SSMs) as a promising alternative by benchmarking two SSM-inspired architectures, Caduceus and Hawk, on long-range genomics modeling tasks under conditions parallel to a 50M parameter transformer baseline. We discover that SSMs match transformer performance and exhibit impressive zero-shot extrapolation across multiple tasks, handling contexts 10 to 100 times longer than those seen during training, indicating more generalizable representations better suited for modeling the long and complex human genome. Moreover, we demonstrate that these models can efficiently process sequences of 1M tokens on a single GPU, allowing for modeling entire genomic regions at once, even in labs with limited compute. Our findings establish SSMs as efficient and scalable for long-context genomic analysis.",Bioinformatics,http://arxiv.org/abs/2504.06304v2,arXiv,1
"OLAF (Open Life Science Analysis Framework) is an open-source platform that enables researchers to perform bioinformatics analyses using natural language. By combining large language models (LLMs) with a modular agent-pipe-router architecture, OLAF generates and executes bioinformatics code on real scientific data, including formats like .h5ad. The system includes an Angular front end and a Python/Firebase backend, allowing users to run analyses such as single-cell RNA-seq workflows, gene annotation, and data visualization through a simple web interface. Unlike general-purpose AI tools, OLAF integrates code execution, data handling, and scientific libraries in a reproducible, user-friendly environment. It is designed to lower the barrier to computational biology for non-programmers and support transparent, AI-powered life science research.",Bioinformatics,http://arxiv.org/abs/2504.03976v2,arXiv,1
"Essential life processes take place across multiple space and time scales in living organisms but understanding their mechanistic interactions remains an ongoing challenge. Advanced multiscale modeling techniques are providing new opportunities and insights into these complex processes. In cells, meters of chromatin are folded into a nucleus with a diameter on the order of microns. The three-dimensional chromatin structure coupled with biochemical processes that turn genes on or off, specify a given cell type through a complicated set of interactions collectively referred to as epigenetics. Important epigenetic processes include the differential accessibility of genomic loci to transcription factors and chemical modifications to DNA and DNA-binding molecules such as histones. The dynamics of these epigenetic processes span timescales from milliseconds to years. How do chemical modifications consisting of a handful of atoms cooperate to modulate genome folding at the scale of the nucleus and impact organism outcomes? In this review, we highlight the inherently multiscale nature of chromatin organization, with a focus on computational modeling to bridge the gaps in our understanding of biochemical processes across scales. We review relevant chromatin biology, including major types of epigenetic modifications as well as the higher order chromatin structures to present a multiscale view of chromatin. We also review relevant computational methods to simulate chromatin structure, function, and dynamics, as well as experimental techniques that inform and validate said models. Finally, we argue that multiscale modeling provides a path forward towards understanding emergent behavior in this inherently multiscale system.",Bioinformatics,http://arxiv.org/abs/2504.03876v1,arXiv,1
"Viral mutations pose significant threats to public health by increasing infectivity, strengthening vaccine resistance, and altering disease severity. To track these evolving patterns, agencies like the CDC annually evaluate thousands of virus strains, underscoring the urgent need to understand viral mutagenesis and evolution in depth. In this study, we integrate genomic analysis, clustering, and three leading dimensionality reduction approaches, namely, principal component analysis (PCA), t-distributed stochastic neighbor embedding (t-SNE), and uniform manifold approximation and projection (UMAP)-to investigate the effects of COVID-19 on influenza virus propagation. By applying these methods to extensive pre- and post-pandemic influenza datasets, we reveal how selective pressures during the pandemic have influenced the diversity of influenza genetics. Our findings indicate that combining robust dimension reduction with clustering yields critical insights into the complex dynamics of viral mutation, informing both future research directions and strategies for public health intervention.",Bioinformatics,http://arxiv.org/abs/2504.03550v1,arXiv,1
"In primates, loci associated with adaptive trait variation often fall in non-coding regions. Understanding the mechanisms linking these regulatory variants to fitness-relevant phenotypes remains challenging, but can be addressed using functional genomic data. However, such data are rarely generated at scale in non-human primates. When they are, only select tissues, cell types, developmental stages, and cellular environments are typically considered, despite appreciation that adaptive variants often exhibit context-dependent effects. In this review, we 1) discuss why context-dependent regulatory loci might be especially evolutionarily relevant in primates, 2) explore challenges and emerging solutions for mapping such context-dependent variation, and 3) discuss the scientific questions these data could address. We argue that filling this gap will provide critical insights into evolutionary processes, human disease, and regulatory adaptation.",Bioinformatics,http://arxiv.org/abs/2504.02081v1,arXiv,1
"Background: Esophageal cancer poses a significant global health challenge, with the incidence of esophageal adenocarcinoma (EAC), a predominant subtype, increasing notably in Western countries. Cathepsins, a family of lysosomal proteolytic enzymes, have been implicated in the progression of various tumors. However, the causal relationship between the cathepsin family and EAC remains unresolved. Methods: To evaluate these potential causal associations, integrative analyses were conducted, integrating Mendelian randomization (MR), transcriptome-wide association study (TWAS), single-cell RNA sequencing (scRNA-seq), and single-cell expression quantitative trait locus (sc-eQTL) analyses. Results: Univariable and multivariable MR analyses demonstrated that elevated levels of cathepsin B (CTSB) were associated with a reduced risk of EAC. The TWAS analysis identified a negative association between CTSB expression in esophageal tissue and EAC, consistent with experimental validation using immunohistochemistry. The scRNA-seq data analysis indicated that CTSB expression was predominantly localized in macrophages infiltrating EAC. Colocalization analysis incorporating sc-eQTL data specific to macrophages confirmed a shared causal variant between CTSB and macrophages. Additionally, MR analysis of CTSB and macrophage scavenger receptor (MSR) types I and II established their interrelationship, suggesting that CTSB may influence the proinflammatory phenotype of macrophages, ultimately affecting EAC risk. Conclusions: This integrative analysis, utilizing MR, TWAS, scRNA-seq, and sc-eQTL data, identified a significant causal association between CTSB and EAC, potentially mediated through macrophage MSR regulation. These findings suggest that targeting cathepsin B could represent a novel strategy for the diagnosis and treatment of EAC.",Bioinformatics,http://arxiv.org/abs/2504.01270v1,arXiv,1
"Epigenetics encompasses mechanisms that can alter the expression of genes without changing the underlying genetic sequence. The epigenetic regulation of gene expression is initiated and sustained by several mechanisms such as DNA methylation, histone modifications, chromatin conformation, and non-coding RNA. The changes in gene regulation and expression can manifest in the form of various diseases and disorders such as cancer and congenital deformities. Over the last few decades, high throughput experimental approaches have been used to identify and understand epigenetic changes, but these laboratory experimental approaches and biochemical processes are time-consuming and expensive. To overcome these challenges, machine learning and artificial intelligence (AI) approaches have been extensively used for mapping epigenetic modifications to their phenotypic manifestations. In this paper we provide a narrative review of published research on AI models trained on epigenomic data to address a variety of problems such as prediction of disease markers, gene expression, enhancer promoter interaction, and chromatin states. The purpose of this review is twofold as it is addressed to both AI experts and epigeneticists. For AI researchers, we provided a taxonomy of epigenetics research problems that can benefit from an AI-based approach. For epigeneticists, given each of the above problems we provide a list of candidate AI solutions in the literature. We have also identified several gaps in the literature, research challenges, and recommendations to address these challenges.",Bioinformatics,http://arxiv.org/abs/2504.03733v1,arXiv,1
"In mammalian and vertebrate genomes, the promoter regions of the gene and their distal enhancers may be located millions of base-pairs from each other, while a promoter may not interact with the closest enhancer. Since base-pair proximity is not a good indicator of these interactions, there is considerable work toward developing methods for predicting Enhancer-Promoter Interactions (EPI). Several machine learning methods have reported increasingly higher accuracies for predicting EPI. Typically, these approaches randomly split the dataset of Enhancer-Promoter (EP) pairs into training and testing subsets followed by model training. However, the aforementioned random splitting causes information leakage by assigning EP pairs from the same genomic region to both testing and training sets, leading to performance overestimation. In this paper we propose to use a more thorough training and testing paradigm i.e., Leave-one-chromosome-out (LOCO) cross-validation for EPI-prediction. We demonstrate that a deep learning algorithm, which gives higher accuracies when trained and tested on random-splitting setting, drops drastically in performance under LOCO setting, confirming overestimation of performance. We further propose a novel hybrid deep neural network for EPI-prediction that fuses k-mer features of the nucleotide sequence. We show that the hybrid architecture performs significantly better in the LOCO setting, demonstrating it can learn more generalizable aspects of EP interactions. With this paper we are also releasing the LOCO splitting-based EPI dataset. Research data is available in this public repository: https://github.com/malikmtahir/EPI",Bioinformatics,http://arxiv.org/abs/2504.00306v1,arXiv,1
"Genome sequence analysis, which analyzes the DNA sequences of organisms, drives advances in many critical medical and biotechnological fields. Given its importance and the exponentially growing volumes of genomic sequence data, there are extensive efforts to accelerate genome sequence analysis. In this work, we demonstrate a major bottleneck that greatly limits and diminishes the benefits of state-of-the-art genome sequence analysis accelerators: the data preparation bottleneck, where genomic sequence data is stored in compressed form and needs to be decompressed and formatted first before an accelerator can operate on it. To mitigate this bottleneck, we propose SAGe, an algorithm-architecture co-design for highly-compressed storage and high-performance access of large-scale genomic sequence data. The key challenge is to improve data preparation performance while maintaining high compression ratios (comparable to genomic-specific compression algorithms) at low hardware cost. We address this challenge by leveraging key properties of genomic datasets to co-design (i) a new (de)compression algorithm, (ii) hardware that decompresses data with lightweight operations and efficient streaming accesses, (iii) storage data layout, and (iv) interface commands to access data. SAGe is highly versatile as it supports datasets from different sequencing technologies and species. Thanks to its lightweight design, SAGe can be seamlessly integrated with a broad range of genome sequence analysis hardware accelerators to mitigate their data preparation bottlenecks. Our results demonstrate that SAGe improves the average end-to-end performance and energy efficiency of two state-of-the-art genome sequence analysis accelerators by 3.0x-32.1x and 13.0x-34.0x, respectively, compared to when the accelerators rely on state-of-the-art decompression tools.",Bioinformatics,http://arxiv.org/abs/2504.03732v3,arXiv,1
"Genome annotation is essential for understanding the functional elements within genomes. While automated methods are indispensable for processing large-scale genomic data, they often face challenges in accurately predicting gene structures and functions. Consequently, manual curation by domain experts remains crucial for validating and refining these predictions. These combined outcomes from automated tools and manual curation highlight the importance of integrating human expertise with AI capabilities to improve both the accuracy and efficiency of genome annotation. However, the manual curation process is inherently labor-intensive and time-consuming, making it difficult to scale for large datasets. To address these challenges, we propose a conceptual framework, Human-AI Collaborative Genome Annotation (HAICoGA), which leverages the synergistic partnership between humans and artificial intelligence to enhance human capabilities and accelerate the genome annotation process. Additionally, we explore the potential of integrating Large Language Models (LLMs) into this framework to support and augment specific tasks. Finally, we discuss emerging challenges and outline open research questions to guide further exploration in this area.",Bioinformatics,http://arxiv.org/abs/2503.23691v1,arXiv,1
"Recent breakthroughs in single-cell technology have ushered in unparalleled opportunities to decode the molecular intricacy of intricate biological systems, especially those linked to diseases unique to humans. However, these progressions have also ushered in novel obstacles-specifically, the efficient annotation of extensive, long-tailed single-cell data pertaining to disease conditions. To effectively surmount this challenge, we introduce Celler, a state-of-the-art generative pre-training model crafted specifically for the annotation of single-cell data. Celler incorporates two groundbreaking elements: First, we introduced the Gaussian Inflation (GInf) Loss function. By dynamically adjusting sample weights, GInf Loss significantly enhances the model's ability to learn from rare categories while reducing the risk of overfitting for common categories. Secondly, we introduce an innovative Hard Data Mining (HDM) strategy into the training process, specifically targeting the challenging-to-learn minority data samples, which significantly improved the model's predictive accuracy. Additionally, to further advance research in this field, we have constructed a large-scale single-cell dataset: Celler-75, which encompasses 40 million cells distributed across 80 human tissues and 75 specific diseases. This dataset provides critical support for comprehensively exploring the potential of single-cell technology in disease research. Our code is available at https://github.com/AI4science-ym/HiCeller.",Bioinformatics,http://arxiv.org/abs/2504.00020v2,arXiv,1
"Motivation: Bulk RNA-Seq is a widely used method for studying gene expression across a variety of contexts. The significance of RNA-Seq studies has grown with the advent of high-throughput sequencing technologies. Computational methods have been developed for each stage of the identification of differentially expressed genes. Nevertheless, there are few studies exploring the association between different types of methods. In this study, we evaluated the impact of the association of methodologies in the results of differential expression analysis. By adopting two data sets with qPCR data (to gold-standard reference), seven methods were implemented and assessed in R packages (EBSeq, edgeR, DESeq2, limma, SAMseq, NOISeq, and Knowseq), which was performed and assessed separately and in association. The results were evaluated considering the adopted qPCR data. Results: Here, we introduce consexpressionR, an R package that automates differential expression analysis using consensus of at least seven methodologies, producing more assertive results with a significant reduction in false positives. Availability: consexpressionR is an R package available via source code and support are available at GitHub (https://github.com/costasilvati/consexpressionR).",Bioinformatics,http://arxiv.org/abs/2503.21546v1,arXiv,1
"The AGP format is a tab-separated table format describing how components of a genome assembly fit together. A standard submission format for genome assemblies is a fasta file giving the sequence of contigs along with an AGP file showing how these components are assembled into larger pieces like scaffolds or chromosomes. For this reason, many scaffolding software pipelines output assemblies in this format. However, although many programs for assembling and scaffolding genomes read and write this format, there is currently no published software for making edits to AGP files when performing assembly curation. We present agptools, a suite of command-line programs that can perform common operations on AGP files, such as breaking and joining sequences, inverting pieces of assembly components, assembling contigs into larger sequences based on an AGP file, and transforming between coordinate systems of different assembly layouts. Additionally, agptools includes an API that writers of other software packages can use to read, write, and manipulate AGP files within their own programs. Agptools gives bioinformaticians a simple, robust, and reproducible way to edit genome assemblies that avoids the shortfalls of other methods for editing AGP files.",Bioinformatics,http://arxiv.org/abs/2503.20451v1,arXiv,1
"In mass spectrometry-based proteomics, experts usually project data onto a single set of reference sequences, overlooking the influence of common haplotypes (combinations of genetic variants inherited together from a parent). We recently introduced ProHap, a tool for generating customized protein haplotype databases. Here, we present ProHap Explorer, a visualization interface designed to investigate the influence of common haplotypes on the human proteome. It enables users to explore haplotypes, their effects on protein sequences, and the identification of non-canonical peptides in public mass spectrometry datasets. The design builds on well-established representations in biological sequence analysis, ensuring familiarity for domain experts while integrating novel interactive elements tailored to proteogenomic data exploration. User interviews with proteomics experts confirmed the tool's utility, highlighting its ability to reveal whether haplotypes affect proteins of interest. By facilitating the intuitive exploration of proteogenomic variation, ProHap Explorer supports research in personalized medicine and the development of targeted therapies.",Bioinformatics,http://arxiv.org/abs/2504.06282v1,arXiv,1
"With the surge in the number of variants of uncertain significance (VUS) reported in ClinVar in recent years, there is an imperative to resolve VUS at scale. Multiplexed assays of variant effect (MAVEs), which allow the functional consequence of 100s to 1000s of genetic variants to be measured in a single experiment, are emerging as a powerful source of evidence which can be used in clinical gene variant classification. Increasingly, multiple published MAVEs are available for the same gene, sometimes measuring different aspects of variant impact. When multiple functional roles of a gene need to be considered, combining data from multiple MAVEs may provide a more comprehensive measure of the consequence of a genetic variant, which could impact variant classifications. Here, we provide guidance for combining such multiplexed functional data, incorporating a stepwise process from data curation and collection to model generation and validation. We demonstrate the potential and pitfalls of this approach by showing the integration of multiplexed functional data from five MAVEs for the gene TP53, two MAVEs for the gene LDLR and two MAVEs for PTEN. We also present a web applet that allows users to test various methods for combining score sets from multiple assays, calculate integrated functional scores for all variants, and assess whether combining data enables the application of stronger evidence for pathogenicity or benignity. By following these steps with appropriate guardrails, researchers can maximize the value of MAVEs, strengthen the functional evidence for clinical variant classification, and potentially uncover novel mechanisms of pathogenicity for clinically relevant genes.",Bioinformatics,http://arxiv.org/abs/2503.18810v2,arXiv,1
"Accurate variant descriptions are of paramount importance in the field of genomics. The domain is confronted with increasingly complex variants, e.g., combinations of multiple indels, making it challenging to generate proper variant descriptions directly from chromosomal sequences. We present a graph based on all minimal alignments that is a complete representation of a variant, which gives insight into the nature of a variant compared to a single variant description. We provide three complementary extraction methods to derive variant descriptions from this graph, including one that yields domain-specific constructs from the HGVS nomenclature. Our experiments show that our methods in comparison with dbSNP, the authoritative variant database from the NCBI, result in identical HGVS descriptions for simple variants and more meaningful descriptions for complex variants, in particular for repeat expansions and contractions.",Bioinformatics,http://arxiv.org/abs/2503.18472v2,arXiv,1
"As a system of integrated homeostasis, life is susceptible to disruptions by visceral inflammation, which can disturb internal environment equilibrium. The role of body-spread subcutaneous fascia (scFascia) in this process is poorly understood. In the rat model of Salmonella-induced dysentery, scRNA-seq of scFascia and deep-learning analysis revealed Warburg-like metabolic reprogramming in macrophages (MPs) with reduced citrate cycle activity. Cd34+/Pdgfra+ telocytes (CPTCs) regulated MPs differentiation and proliferation via Wnt/Fgf signal, suggesting a pathological crosstalk pattern in the scFascia, herein termed the fascia-visceral inflammatory crosstalk pattern (FVICP). PySCENIC analysis indicated increased activity transcription factors Fosl1, Nfkb2, and Atf4, modulated by CPTCs signaling to MPs, downregulating aerobic respiration and upregulating cell cycle, DNA replication, and transcription. This study highlights scFascia's role in immunomodulation and metabolic reprogramming during visceral inflammation, underscoring its function in systemic homeostasis.",Bioinformatics,http://arxiv.org/abs/2503.17418v1,arXiv,1
"Deep learning architectures such as convolutional neural networks and Transformers have revolutionized biological sequence modeling, with recent advances driven by scaling up foundation and task-specific models. The computational resources and large datasets required, however, limit their applicability in biological contexts. We introduce Lyra, a subquadratic architecture for sequence modeling, grounded in the biological framework of epistasis for understanding sequence-to-function relationships. Mathematically, we demonstrate that state space models efficiently capture global epistatic interactions and combine them with projected gated convolutions for modeling local relationships. We demonstrate that Lyra is performant across over 100 wide-ranging biological tasks, achieving state-of-the-art (SOTA) performance in many key areas, including protein fitness landscape prediction, biophysical property prediction (e.g. disordered protein region functions) peptide engineering applications (e.g. antibody binding, cell-penetrating peptide prediction), RNA structure analysis, RNA function prediction, and CRISPR guide design. It achieves this with orders-of-magnitude improvements in inference speed and reduction in parameters (up to 120,000-fold in our tests) compared to recent biology foundation models. Using Lyra, we were able to train and run every task in this study on two or fewer GPUs in under two hours, democratizing access to biological sequence modeling at SOTA performance, with potential applications to many fields.",Bioinformatics,http://arxiv.org/abs/2503.16351v1,arXiv,1
"This study explores the application of machine learning-based genetic linguistics for identifying heavy metal response genes in rice (Oryza sativa). By integrating convolutional neural networks and random forest algorithms, we developed a hybrid model capable of extracting and learning meaningful features from gene sequences, such as k-mer frequencies and physicochemical properties. The model was trained and tested on datasets of genes, achieving high predictive performance (precision: 0.89, F1-score: 0.82). RNA-seq and qRT-PCR experiments conducted on rice leaves which exposed to Hg0, revealed differential expression of genes associated with heavy metal responses, which validated the model's predictions. Co-expression network analysis identified 103 related genes, and a literature review indicated that these genes are highly likely to be involved in heavy metal-related biological processes. By integrating and comparing the analysis results with those of differentially expressed genes (DEGs), the validity of the new machine learning method was further demonstrated. This study highlights the efficacy of combining machine learning with genetic linguistics for large-scale gene prediction. It demonstrates a cost-effective and efficient approach for uncovering molecular mechanisms underlying heavy metal responses, with potential applications in developing stress-tolerant crop varieties.",Bioinformatics,http://arxiv.org/abs/2503.16582v1,arXiv,1
"We introduce Gene42, a novel family of Genomic Foundation Models (GFMs) designed to manage context lengths of up to 192,000 base pairs (bp) at a single-nucleotide resolution. Gene42 models utilize a decoder-only (LLaMA-style) architecture with a dense self-attention mechanism. Initially trained on fixed-length sequences of 4,096 bp, our models underwent continuous pretraining to extend the context length to 192,000 bp. This iterative extension allowed for the comprehensive processing of large-scale genomic data and the capture of intricate patterns and dependencies within the human genome. Gene42 is the first dense attention model capable of handling such extensive long context lengths in genomics, challenging state-space models that often rely on convolutional operators among other mechanisms. Our pretrained models exhibit notably low perplexity values and high reconstruction accuracy, highlighting their strong ability to model genomic data. Extensive experiments on various genomic benchmarks have demonstrated state-of-the-art performance across multiple tasks, including biotype classification, regulatory region identification, chromatin profiling prediction, variant pathogenicity prediction, and species classification. The models are publicly available at huggingface.co/inceptionai.",Bioinformatics,http://arxiv.org/abs/2503.16565v1,arXiv,1
"How a single fertilized cell gives rise to a complex array of specialized cell types in development is a central question in biology. The cells grow, divide, and acquire differentiated characteristics through poorly understood molecular processes. A key approach to studying developmental processes is to infer the tree graph of cell lineage division and differentiation histories, providing an analytical framework for dissecting individual cells' molecular decisions during replication and differentiation. Although genetically engineered lineage-tracing methods have advanced the field, they are either infeasible or ethically constrained in many organisms. In contrast, modern single-cell technologies can measure high-content molecular profiles (e.g., transcriptomes) in a wide range of biological systems.   Here, we introduce CellTreeQM, a novel deep learning method based on transformer architectures that learns an embedding space with geometric properties optimized for tree-graph inference. By formulating lineage reconstruction as a tree-metric learning problem, we have systematically explored supervised, weakly supervised, and unsupervised training settings and present a Lineage Reconstruction Benchmark to facilitate comprehensive evaluation of our learning method. We benchmarked the method on (1) synthetic data modeled via Brownian motion with independent noise and spurious signals and (2) lineage-resolved single-cell RNA sequencing datasets. Experimental results show that CellTreeQM recovers lineage structures with minimal supervision and limited data, offering a scalable framework for uncovering cell lineage relationships in challenging animal models. To our knowledge, this is the first method to cast cell lineage inference explicitly as a metric learning task, paving the way for future computational models aimed at uncovering the molecular dynamics of cell lineage.",Bioinformatics,http://arxiv.org/abs/2503.13925v1,arXiv,1
"Cancer progression involves the sequential accumulation of genetic alterations that cumulatively shape the tumour phenotype. In prostate cancer, tumours can follow divergent evolutionary trajectories that lead to distinct subtypes, but the causes of this divergence remain unclear. While causal inference could elucidate the factors involved, conventional methods are unsuitable due to the possibility of unobserved confounders and ambiguity in the direction of causality. Here, we propose a method that circumvents these issues and apply it to genomic data from 829 prostate cancer patients. We identify several genetic alterations that drive divergence as well as others that prevent this transition, locking tumours into one trajectory. Further analysis reveals that these genetic alterations may cause each other, implying a positive-feedback loop that accelerates divergence. Our findings provide insights into how cancer subtypes emerge and offer a foundation for genomic surveillance strategies aimed at monitoring the progression of prostate cancer.",Bioinformatics,http://arxiv.org/abs/2503.13189v1,arXiv,1
"An important goal in cancer research is the survival prognosis of a patient based on a minimal panel of genomic and molecular markers such as genes or proteins. Purely data-driven models without any biological knowledge can produce non-interpretable results. We propose a penalized semiparametric Bayesian Cox model with graph-structured selection priors for sparse identification of multi-omics features by making use of a biologically meaningful graph via a Markov random field (MRF) prior to capturing known relationships between multi-omics features. Since the fixed graph in the MRF prior is for the prior probability distribution, it is not a hard constraint to determine variable selection, so the proposed model can verify known information and has the potential to identify new and novel biomarkers for drawing new biological knowledge. Our simulation results show that the proposed Bayesian Cox model with graph-based prior knowledge results in more trustable and stable variable selection and non-inferior survival prediction, compared to methods modeling the covariates independently without any prior knowledge. The results also indicate that the performance of the proposed model is robust to a partially correct graph in the MRF prior, meaning that in a real setting where not all the true network information between covariates is known, the graph can still be useful. The proposed model is applied to the primary invasive breast cancer patients data in The Cancer Genome Atlas project.",Bioinformatics,http://arxiv.org/abs/2503.13078v1,arXiv,1
"Identifying transcription factor binding sites (TFBS) is crucial for understanding gene regulation, as these sites enable transcription factors (TFs) to bind to DNA and modulate gene expression. Despite advances in high-throughput sequencing, accurately identifying TFBS remains challenging due to the vast genomic data and complex binding patterns. GCBLANE, a graph-enhanced convolutional bidirectional Long Short-Term Memory (LSTM) attention network, is introduced to address this issue. It integrates convolutional, multi-head attention, and recurrent layers with a graph neural network to detect key features for TFBS prediction. On 690 ENCODE ChIP-Seq datasets, GCBLANE achieved an average AUC of 0.943, and on 165 ENCODE datasets, it reached an AUC of 0.9495, outperforming advanced models that utilize multimodal approaches, including DNA shape information. This result underscores GCBLANE's effectiveness compared to other methods. By combining graph-based learning with sequence analysis, GCBLANE significantly advances TFBS prediction.",Bioinformatics,http://arxiv.org/abs/2503.12377v1,arXiv,1
"Metabolism plays a crucial role in sleep regulation, yet its effects are challenging to track in real time. This study introduces a machine learning-based framework to analyze sleep patterns and identify how metabolic changes influence sleep at specific time points. We first established that sleep periods in Drosophila melanogaster function independently, with no causal relationship between different sleep episodes. Using gradient boosting models and explainable artificial intelligence techniques, we quantified the influence of time-dependent sleep features. Causal inference and autocorrelation analyses further confirmed that sleep states at different times are statistically independent, providing a robust foundation for exploring metabolic effects on sleep. Applying this framework to flies with altered monocarboxylate transporter 2 expression, we found that changes in ketone transport modified sleep stability and disrupted transitions between day and night sleep. In an Alzheimers disease model, metabolic interventions such as beta hydroxybutyrate supplementation and intermittent fasting selectively influenced the timing of day to night transitions rather than uniformly altering sleep duration. Autoencoder based similarity scoring and wavelet analysis reinforced that metabolic effects on sleep were highly time dependent. This study presents a novel approach to studying sleep-metabolism interactions, revealing that metabolic states exert their strongest influence at distinct time points, shaping sleep stability and circadian transitions.",Bioinformatics,http://arxiv.org/abs/2503.12330v1,arXiv,1
"Background: Several studies show that large language models (LLMs) struggle with phenotype-driven gene prioritization for rare diseases. These studies typically use Human Phenotype Ontology (HPO) terms to prompt foundation models like GPT and LLaMA to predict candidate genes. However, in real-world settings, foundation models are not optimized for domain-specific tasks like clinical diagnosis, yet inputs are unstructured clinical notes rather than standardized terms. How LLMs can be instructed to predict candidate genes or disease diagnosis from unstructured clinical notes remains a major challenge. Methods: We introduce RAG-driven CoT and CoT-driven RAG, two methods that combine Chain-of-Thought (CoT) and Retrieval Augmented Generation (RAG) to analyze clinical notes. A five-question CoT protocol mimics expert reasoning, while RAG retrieves data from sources like HPO and OMIM (Online Mendelian Inheritance in Man). We evaluated these approaches on rare disease datasets, including 5,980 Phenopacket-derived notes, 255 literature-based narratives, and 220 in-house clinical notes from Childrens Hospital of Philadelphia. Results: We found that recent foundations models, including Llama 3.3-70B-Instruct and DeepSeek-R1-Distill-Llama-70B, outperformed earlier versions such as Llama 2 and GPT-3.5. We also showed that RAG-driven CoT and CoT-driven RAG both outperform foundation models in candidate gene prioritization from clinical notes; in particular, both methods with DeepSeek backbone resulted in a top-10 gene accuracy of over 40% on Phenopacket-derived clinical notes. RAG-driven CoT works better for high-quality notes, where early retrieval can anchor the subsequent reasoning steps in domain-specific evidence, while CoT-driven RAG has advantage when processing lengthy and noisy notes.",Bioinformatics,http://arxiv.org/abs/2503.12286v1,arXiv,1
"Pangenomes serve as a framework for joint analysis of genomes of related organisms. Several pangenome models were proposed, offering different functionalities, applications provided by available tools, their efficiency etc. Among them, two graph-based models are particularly widely used: variation graphs and de Bruijn graphs. In the current paper we propose an axiomatization of the desirable properties of a graph representation of a collection of strings. We show the relationship between variation graphs satisfying these criteria and de Bruijn graphs. This relationship can be used to efficiently build a variation graph representing a given set of genomes, transfer annotations between both models, compare the results of analyzes based on each model etc.",Bioinformatics,http://arxiv.org/abs/2503.14520v1,arXiv,1
"Genotype-to-Phenotype (G2P) prediction plays a pivotal role in crop breeding, enabling the identification of superior genotypes based on genomic data. Rice (Oryza sativa), one of the most important staple crops, faces challenges in improving yield and resilience due to the complex genetic architecture of agronomic traits and the limited sample size in breeding datasets. Current G2P prediction methods, such as GWAS and linear models, often fail to capture complex non-linear relationships between genotypes and phenotypes, leading to suboptimal prediction accuracy. Additionally, population stratification and overfitting are significant obstacles when models are applied to small datasets with diverse genetic backgrounds. This study introduces the Learnable Group Transform (LGT) method, which aims to overcome these challenges by combining the advantages of traditional linear models with advanced machine learning techniques. LGT utilizes a group-based transformation of genotype data to capture spatial relationships and genetic structures across diverse rice populations, offering flexibility to generalize even with limited data. Through extensive experiments on the Rice529 dataset, a panel of 529 rice accessions, LGT demonstrated substantial improvements in prediction accuracy for multiple agronomic traits, including yield and plant height, compared to state-of-the-art baselines such as linear models and recent deep learning approaches. Notably, LGT achieved an R^2 improvement of up to 15\% for yield prediction, significantly reducing error and demonstrating its ability to extract meaningful signals from high-dimensional, noisy genomic data. These results highlight the potential of LGT as a powerful tool for genomic prediction in rice breeding, offering a promising solution for accelerating the identification of high-yielding and resilient rice varieties.",Bioinformatics,http://arxiv.org/abs/2503.11180v1,arXiv,1
"Repetitive DNA sequences underpin genome architecture and evolutionary processes, yet they remain challenging to classify accurately. Terrier is a deep learning model designed to overcome these challenges by classifying repetitive DNA sequences using a publicly available, curated repeat sequence library trained under the RepeatMasker schema. Poor representation of taxa within repeat databases often limits the classification accuracy and reproducibility of current repeat annotation methods, limiting our understanding of repeat evolution and function. Terrier overcomes these challenges by leveraging deep learning for improved accuracy. Trained on Repbase, which includes over 100,000 repeat families -- four times more than Dfam -- Terrier maps 97.1% of Repbase sequences to RepeatMasker categories, offering the most comprehensive classification system available. When benchmarked against DeepTE, TERL, and TEclass2 in model organisms (rice, fruit flies, humans, and mice), Terrier achieved superior accuracy while classifying a broader range of sequences. Further validation in non-model amphibian, flatworm and Northern krill genomes highlights its effectiveness in improving classification in non-model species, facilitating research on repeat-driven evolution, genomic instability, and phenotypic variation.",Bioinformatics,http://arxiv.org/abs/2503.09312v2,arXiv,1
"Cis-regulatory elements (CREs), such as promoters and enhancers, are relatively short DNA sequences that directly regulate gene expression. The fitness of CREs, measured by their ability to modulate gene expression, highly depends on the nucleotide sequences, especially specific motifs known as transcription factor binding sites (TFBSs). Designing high-fitness CREs is crucial for therapeutic and bioengineering applications. Current CRE design methods are limited by two major drawbacks: (1) they typically rely on iterative optimization strategies that modify existing sequences and are prone to local optima, and (2) they lack the guidance of biological prior knowledge in sequence optimization. In this paper, we address these limitations by proposing a generative approach that leverages reinforcement learning (RL) to fine-tune a pre-trained autoregressive (AR) model. Our method incorporates data-driven biological priors by deriving computational inference-based rewards that simulate the addition of activator TFBSs and removal of repressor TFBSs, which are then integrated into the RL process. We evaluate our method on promoter design tasks in two yeast media conditions and enhancer design tasks for three human cell types, demonstrating its ability to generate high-fitness CREs while maintaining sequence diversity. The code is available at https://github.com/yangzhao1230/TACO.",Bioinformatics,http://arxiv.org/abs/2503.07981v1,arXiv,1
"Biomedical research increasingly relies on heterogeneous, high-dimensional datasets, yet effective visualization remains hindered by fragmented code resources, steep programming barriers, and limited domain-specific guidance. Bizard is an open-source visualization code repository engineered to streamline data analysis in biomedical research. It aggregates a diverse array of executable visualization scripts, empowering researchers to select and tailor optimal graphical methods for their specific investigative demands. The platform features an intuitive interface equipped with sophisticated browsing and filtering capabilities, exhaustive tutorials, and interactive discussion forums that foster knowledge dissemination. Through its community-driven paradigm, Bizard promotes continual refinement and functional expansion, establishing itself as an essential resource for elevating biomedical data visualization and analytical standards. By harnessing Bizard's infrastructure, researchers can augment their visualization proficiency, propel methodological progress, and enhance interpretive rigor, ultimately accelerating precision medicine and personalized therapeutics. Bizard is freely accessible at https://openbiox.github.io/Bizard/.",Bioinformatics,http://arxiv.org/abs/2503.06845v2,arXiv,1
"Large Language Models (LLMs) are revolutionizing bioinformatics, enabling advanced analysis of DNA, RNA, proteins, and single-cell data. This survey provides a systematic review of recent advancements, focusing on genomic sequence modeling, RNA structure prediction, protein function inference, and single-cell transcriptomics. Meanwhile, we also discuss several key challenges, including data scarcity, computational complexity, and cross-omics integration, and explore future directions such as multimodal learning, hybrid AI models, and clinical applications. By offering a comprehensive perspective, this paper underscores the transformative potential of LLMs in driving innovations in bioinformatics and precision medicine.",Bioinformatics,http://arxiv.org/abs/2503.04490v2,arXiv,1
"Genes, proteins and other biological entities influence one another via causal molecular networks. Causal relationships in such networks are mediated by complex and diverse mechanisms, through latent variables, and are often specific to cellular context. It remains challenging to characterise such networks in practice. Here, we present a novel framework to evaluate large language models (LLMs) for zero-shot inference of causal relationships in biology. In particular, we systematically evaluate causal claims obtained from an LLM using real-world interventional data. This is done over one hundred variables and thousands of causal hypotheses. Furthermore, we consider several prompting and retrieval-augmentation strategies, including large, and potentially conflicting, collections of scientific articles. Our results show that with tailored augmentation and prompting, even relatively small LLMs can capture meaningful aspects of causal structure in biological systems. This supports the notion that LLMs could act as orchestration tools in biological discovery, by helping to distil current knowledge in ways amenable to downstream analysis. Our approach to assessing LLMs with respect to experimental data is relevant for a broad range of problems at the intersection of causal learning, LLMs and scientific discovery.",Bioinformatics,http://arxiv.org/abs/2503.04347v1,arXiv,1
"The advent of high-throughput sequencing technologies has revolutionized genome analysis by enabling the rapid and cost-effective sequencing of large genomes. Despite these advancements, the increasing complexity and volume of genomic data present significant challenges related to accuracy, scalability, and computational efficiency. These challenges are mainly due to various forms of unwanted and unhandled variations in sequencing data, collectively referred to as noise. In this dissertation, we address these challenges by providing a deep understanding of different types of noise in genomic data and developing techniques to mitigate the impact of noise on genome analysis.   First, we introduce BLEND, a noise-tolerant hashing mechanism that quickly identifies both exactly matching and highly similar sequences with arbitrary differences using a single lookup of their hash values. Second, to enable scalable and accurate analysis of noisy raw nanopore signals, we propose RawHash, a novel mechanism that effectively reduces noise in raw nanopore signals and enables accurate, real-time analysis by proposing the first hash-based similarity search technique for raw nanopore signals. Third, we extend the capabilities of RawHash with RawHash2, an improved mechanism that 1) provides a better understanding of noise in raw nanopore signals to reduce it more effectively and 2) improves the robustness of mapping decisions. Fourth, we explore the broader implications and new applications of raw nanopore signal analysis by introducing Rawsamble, the first mechanism for all-vs-all overlapping of raw signals using hash-based search. Rawsamble enables the construction of de novo assemblies directly from raw signals without basecalling, which opens up new directions and uses for raw nanopore signal analysis.",Bioinformatics,http://arxiv.org/abs/2503.02997v1,arXiv,1
"Genomic language models (gLMs) have shown mostly modest success in identifying evolutionarily constrained elements in mammalian genomes. To address this issue, we introduce a novel framework for training gLMs that explicitly models nucleotide evolution on phylogenetic trees using multispecies whole-genome alignments. Our approach integrates an alignment into the loss function during training but does not require it for making predictions, thereby enhancing the model's applicability. We applied this framework to train PhyloGPN, a model that excels at predicting functionally disruptive variants from a single sequence alone and demonstrates strong transfer learning capabilities.",Bioinformatics,http://arxiv.org/abs/2503.03773v1,arXiv,1
"Comparing specific types of organisms as they are found across environmental conditions has helped inform how genes and gene products of these organisms relate to phenotypes and adaptation. In this study, we examine metatranscriptomic data as found for oceanic fungi across different oceanic sampling sites. A specific set of three genes was chosen for evaluation based on conserved orthology, known association with core physiological processes in fungi, and level of abundance within oceanic metatranscriptomic data. We report upon a potential association of genetic variance with environmental conditions of iron, salt and phosphate in oceanic waters based on heatmap visualization and PERMANOVA analysis.",Bioinformatics,http://arxiv.org/abs/2503.01994v1,arXiv,1
"Motivation: PCR is more economical and quicker than Next Generation Sequencing for detecting target organisms, with primer design being a critical step. In epidemiology with rapidly mutating viruses, designing effective primers is challenging. Traditional methods require substantial manual intervention and struggle to ensure effective primer design across different strains. For organisms with large, similar genomes like Escherichia coli and Shigella flexneri, differentiating between species is also difficult but crucial.   Results: We developed Primer C-VAE, a model based on a Variational Auto-Encoder framework with Convolutional Neural Networks to identify variants and generate specific primers. Using SARS-CoV-2, our model classified variants (alpha, beta, gamma, delta, omicron) with 98% accuracy and generated variant-specific primers. These primers appeared with >95% frequency in target variants and <5% in others, showing good performance in in-silico PCR tests. For Alpha, Delta, and Omicron, our primer pairs produced fragments <200 bp, suitable for qPCR detection. The model also generated effective primers for organisms with longer gene sequences like E. coli and S. flexneri.   Conclusion: Primer C-VAE is an interpretable deep learning approach for developing specific primer pairs for target organisms. This flexible, semi-automated and reliable tool works regardless of sequence completeness and length, allowing for qPCR applications and can be applied to organisms with large and highly similar genomes.",Bioinformatics,http://arxiv.org/abs/2503.01459v2,arXiv,1
"Genome length varies widely among organisms, from compact genomes of prokaryotes to vast and complex genomes of eukaryotes. In this study, we theoretically identify the evolutionary pressures that may have driven this divergence in genome length. We use a parameter-free model to study genome length evolution under selection pressure to minimize replication time and maximize information storage capacity. We show that prokaryotes tend to reduce genome length, constrained by a single replication origin, while eukaryotes expand their genomes by incorporating multiple replication origins. We propose a connection between genome length and cellular energetics, suggesting that endosymbiotic organelles, mitochondria and chloroplasts, evolutionarily regulate the number of replication origins, thereby influencing genome length in eukaryotes. We show that the above two selection pressures also lead to strict equalization of the number of purines and their corresponding base-pairing pyrimidines within a single DNA strand, known as Chagraff's second parity rule, a hitherto unexplained observation in genomes of nearly all known species. This arises from the symmetrization of replichore length, another observation that has been shown to hold across species, which our model reproduces. The model also reproduces other experimentally observed phenomena, such as a general preference for deletions over insertions, and elongation and high variance of genome lengths under reduced selection pressure for replication rate, termed the C-value paradox. We highlight the possibility of regulation of the firing of latent replication origins in response to cues from the extracellular environment leading to the regulation of cell cycle rates in multicellular eukaryotes.",Bioinformatics,http://arxiv.org/abs/2502.21125v1,arXiv,1
"Genotype-to-Phenotype prediction can promote advances in modern genomic research and crop improvement, guiding precision breeding and genomic selection. However, high-dimensional nonlinear features often hinder the accuracy of genotype-to-phenotype prediction by increasing computational complexity. The challenge also limits the predictive accuracy of traditional approaches. Therefore, effective solutions are needed to improve the accuracy of genotype-to-phenotype prediction. In our paper, we propose MLFformer. MLFformer is a Transformer-based architecture that incorporates the Fast Attention mechanism and a multilayer perceptron module to handle high-dimensional nonlinear features. In MLFformer, the Fast Attention mechanism is utilized to handle computational complexity and enhance processing efficiency. In addition, the MLP structure further captures high-dimensional nonlinear features. Through experiments, the results show that MLFformer reduces the average MAPE by 7.73% compared to the vanilla Transformer. In univariate and multivariate prediction scenarios, MLFformer achieves the best predictive performance among all compared models.",Bioinformatics,http://arxiv.org/abs/2502.18758v1,arXiv,1
"Nonsense-mediated mRNA decay (NMD) is a critical post-transcriptional surveillance mechanism that degrades transcripts with premature termination codons, safeguarding transcriptome integrity and shaping disease phenotypes. However, accurately predicting NMD efficiency remains challenging, as existing models often rely on simplistic rule-based heuristics or limited feature sets, constraining their accuracy and generalizability. Using paired DNA and RNA data from The Cancer Genome Atlas, we benchmark embedding-only models and demonstrate that they underperform compared to a simple rule-based approach. To address this, we develop NMDEP (NMD Efficiency Predictor), an integrative framework that combines optimized rule-based methods, sequence embeddings, and curated biological features, achieving state-of-the-art predictive performance. Through explainable AI, we identify key NMD determinants, reaffirming established factors such as variant position while uncovering novel contributors like ribosome loading. Applied to over 2.9 million simulated stop-gain variants, NMDEP facilitates large-scale mRNA degradation assessments, advancing variant interpretation and disease research.",Bioinformatics,http://arxiv.org/abs/2502.14547v1,arXiv,1
"mRNA-based vaccines have become a major focus in the pharmaceutical industry. The coding sequence as well as the Untranslated Regions (UTRs) of an mRNA can strongly influence translation efficiency, stability, degradation, and other factors that collectively determine a vaccine's effectiveness. However, optimizing mRNA sequences for those properties remains a complex challenge. Existing deep learning models often focus solely on coding region optimization, overlooking the UTRs. We present Helix-mRNA, a structured state-space-based and attention hybrid model to address these challenges. In addition to a first pre-training, a second pre-training stage allows us to specialise the model with high-quality data. We employ single nucleotide tokenization of mRNA sequences with codon separation, ensuring prior biological and structural information from the original mRNA sequence is not lost. Our model, Helix-mRNA, outperforms existing methods in analysing both UTRs and coding region properties. It can process sequences 6x longer than current approaches while using only 10% of the parameters of existing foundation models. Its predictive capabilities extend to all mRNA regions. We open-source the model (https://github.com/helicalAI/helical) and model weights (https://huggingface.co/helical-ai/helix-mRNA).",Bioinformatics,http://arxiv.org/abs/2502.13785v2,arXiv,1
"We consider the problem of predicting gene expressions from DNA sequences. A key challenge of this task is to find the regulatory elements that control gene expressions. Here, we introduce Seq2Exp, a Sequence to Expression network explicitly designed to discover and extract regulatory elements that drive target gene expression, enhancing the accuracy of the gene expression prediction. Our approach captures the causal relationship between epigenomic signals, DNA sequences and their associated regulatory elements. Specifically, we propose to decompose the epigenomic signals and the DNA sequence conditioned on the causal active regulatory elements, and apply an information bottleneck with the Beta distribution to combine their effects while filtering out non-causal components. Our experiments demonstrate that Seq2Exp outperforms existing baselines in gene expression prediction tasks and discovers influential regions compared to commonly used statistical methods for peak detection such as MACS3. The source code is released as part of the AIRS library (https://github.com/divelab/AIRS/).",Bioinformatics,http://arxiv.org/abs/2502.13991v1,arXiv,1
"Ba$_2$IrO$_4$ has been refined in the tetragonal $I4/mmm$ phase without octahedral rotations, and its physical properties have been interpreted in this high-symmetry structure. However, the dynamical stability of this undistorted phase has not previously been questioned. It is important to establish whether other lower-symmetry structures are energetically more favorable because octahedral rotations control electronic bandwidths and constrain which magnetic interactions are allowed by symmetry. Here I compute first-principles phonon dispersions of $I4/mmm$ Ba$_2$IrO$_4$ including spin-orbit interaction. I find a nearly-flat nondegenerate unstable branch along the Brillouin-zone boundary segment $XP$ associated with inplane rotations of the IrO$_6$ octahedra. Using group-theoretical analysis, I enumerate the symmetry-allowed distortions associated with the $X_2^+$ and $P_4$ instabilities and fully relax the resulting structures. Only five of the twelve possible distortions can be stabilized, and the energy gain scales with the number of layers that exhibit octahedral rotations: phases with rotations in every IrO$_6$ layer are lower by $-5.8$ meV/atom and are nearly degenerate with respect to the stacking phase. Electronic structure calculations show that these rotated phases host a narrow and well-separated half-filled $J_{\textrm{eff}} = 1/2$ manifold, whereas structures with rotations only in alternate layers have broader and more entangled bands. This motivates a reinvestigation of the crystal structure of Ba$_2$IrO$_4$ and indicates that octahedral rotations should be considered in modeling its correlated electronic and magnetic properties.",Materials Science,http://arxiv.org/abs/2512.23690v1,arXiv,2
"Recent years have seen a proliferation in investigations on Altermagnetism due to its exciting prospects both from an applications perspective and theoretical standpoint. Traditionally, altermagnets are distinguished from collinear antiferromagnets using the central concept of halving subgroups within the spin space group formalism. In this work, we propose the Fundamental Lemma of Altermagnetism (FLAM) deriving the exact conditions required for the existence of altermagnetic phase in a magnetic material on the basis of site-symmetry groups and halving subgroups for a given crystallographic space group. The spin group formalism further clubs ferrimagnetism with ferromagnetism since the same-spin and opposite-spin sublattices lose their meaning in the presence of multiple magnetic species. As a consequence of FLAM, we further propose a class of fully compensated ferrimagnets, termed as Alterferrimagnets (AFiMs), which can show alternating momentum-dependent spin-polarized non-relativistic electronic bands within the first Brillouin zone. We show that alterferrimagnetism is a generalization of traditional collinear altermagnetism where multiple magnetic species are allowed to coexist forming fully compensated magnetic-sublattices, each with individual up-spin and down-spin sublattices.",Materials Science,http://arxiv.org/abs/2512.23589v1,arXiv,2
"Nanoconfined liquid crystals (LCs) and their nanocomposites are driving the next generation of photonic applications. Consequently, deepening our understanding of mesophase stability, defect topology, and the dynamic response of LCs at the nanoscale requires the development of novel characterization approaches. This motivates us to perform in situ observations on model 4'-octyl-4-cyanobiphenyl (8CB) LC using liquid-phase scanning transmission electron microscopy (LP-STEM). We find that the electron beam induced consecutive phase changes from smectic A to nematic (SmA-N) and from nematic to isotropic (N-I). The kinetic dependence of the phase transition on dose rate shows that the time between SmA-N and N-I shortens with increasing rate, revealing the hypothesis that a higher electron dose rate increases the energy dissipation rate, leading to substantial heat generation in the sample. We report on the spontaneous formation of disclinations, ordering effects, and complete process reversibility. Radiolytic effects of the electron beam are discussed in detail, and additional experiments with external heating indicate that the observed phenomena are mainly thermal in nature. The results are supported by calculations of heat diffusion, suggesting the nanoconfined 8CB differs significantly in thermal properties compared to the bulk one. This is the first detailed study of LC phase transitions using LP-STEM, which paves the way for further studies of nanoconfined LCs and for the development of the technique for advanced LC materials research.",Materials Science,http://arxiv.org/abs/2512.23588v1,arXiv,2
"Phosphorus-based lubricant additives are used for protecting metallic contacts under boundary lubrication by forming surface films that reduce wear and friction. Despite their importance, the molecular mechanisms driving their friction-reducing effects remain unclear, especially for phosphate esters, whose molecular structure critically impact tribological behavior. In this study, we use machine learning-based molecular dynamics simulations to investigate the tribological performance of three representative phosphorus-based additives, Dibutyl Hydrogen Phosphite (DBHP), Octyl Acid Phosphate (OAP), and Methyl Polyethylene Glycol Phosphate (mPEG-P), on iron surfaces. The mPEG-P family is further analyzed by varying esterification degree and chain length. DBHP exhibits the lowest friction and largest interfacial separation, resulting from steric hindrance and tribochemical reactivity, as indicated by P-O bond cleavage and enhanced O-Fe interactions. In contrast, OAP and mPEG-P monoesters produce higher friction due to limited steric protection and reduced resistance to shear, leading to partial loss of surface coverage under extreme conditions. Within the mPEG-P family, multi-ester and longer-chain molecules significantly lower friction by maintaining larger separations, demonstrating that steric effects can outweigh surface reactivity under severe confinement. Overall, these results provide atomistic insights into how molecular architecture controls additive performance and support the design of phosphorus-based lubricants combining reactive anchoring with optimized steric structures for durable, low-friction interfaces.",Materials Science,http://arxiv.org/abs/2512.23583v1,arXiv,2
"Photonic topological insulators (PTIs) offer robust platforms for light manipulation, but reconfigurable control of their topological properties without degrading performance remains a major challenge. While phase-change materials (PCMs) provide large refractive index modulation, widely used materials such as Ge2Sb2Te5 (GST) have been successfully deployed in commercial applications including optical data storage. However, they exhibit significant optical absorption in their crystalline state, which poses a challenge for transmissive photonic devices such as PTIs where high transparency is essential. Here, we overcome this fundamental limitation by integrating the ultra-low-loss PCM antimony triselenide (Sb2Se3) onto a silicon-based 2D PTI. We achieve submicron-scale selective patterning of Sb2Se3 on a photonic crystal for the first time, and demonstrate a topological phase transition induced by the material phase change. Owing to the transparency of Sb2Se3 in both its amorphous and crystalline states, a high Q-factor on the order of 10^3 is preserved-representing nearly an order-of-magnitude improvement over previous GST-based devices. This work resolves the absorption-loss bottleneck in reconfigurable PTIs and paves the way for practical, low-loss, tunable topological photonic devices.",Materials Science,http://arxiv.org/abs/2512.23559v1,arXiv,2
"Using quasi-simultaneous synchrotron X-ray diffraction and tomography techniques, we have studied in-situ and in real-time the nucleation and co-growth dynamics of the peritectic structures in an Al-Mn alloy during solidification. We collected ~30 TB 4D datasets which allow us to elucidate the phases' co-growth dynamics and their spatial, crystallographic and compositional relationship. The primary Al4Mn hexagonal prisms nucleate and grow with high kinetic anisotropy -70 times faster in the axial direction than the radial direction. In all cases, a ~5 um Mn-rich diffusion layer forms at the liquid-solid interface, creating a sharp local solute gradient that governs subsequent phase transformation. The peritectic Al6Mn phases nucleate epitaxially within this diffusion zone, initially forming a thin shell surrounding the Al4Mn with an orientation relationship of {10-10}HCP // {110}O, [0001]HCP // [001]O. Such ~5 um Mn-rich diffusion layers also cause solute depletion at the liquid side of the liquid-solid interface, limiting further epitaxial phase growth, but prompting phase re-nucleation and branching at crystal edges, resulting tetragonal prism structures that no longer follow the initial orientation relationship. The anisotropic diffusion also led to the formation of core defects at the centre of both phases. Furthermore, increasing cooling rate from 0.17 to 20 Â°C/s can disrupt the stability of the solute diffusion zone, effectively suppressing the formation of the core defects and forcing a transition from faceted to non-faceted morphologies. Our work establishes a new theoretical framework for how to tailor and control the peritectic structures in metallic alloys through solidification processes.",Materials Science,http://arxiv.org/abs/2512.23501v1,arXiv,2
"Theoretical simulation is helpful for accurate interpretation of experimental X-ray absorption near-edge structure (XANES) spectra that contain rich atomic and electronic structure information of materials. However, current simulation methods are usually too complex to give the needed accuracy and timeliness when a large amount of data need to be analyzed, such as for in-situ characterization of battery materials. To address these problems, artificial intelligence (AI) models have been developed for XANES prediction. However, instead of using experimental XANES data, the existing models are trained using simulated data, resulting in significant discrepancies between the predicted and experimental spectra. Also, the universality across different elements has not been well studied for such models. In this work, we firstly establish a crystal graph neural network, pre-trained on simulated XANES data covering 48 elements, to achieve universal XANES prediction with a low average relative square error of 0.020223; and then utilize transfer learning to calibrate the model using a small experimental XANES dataset. After calibration, the edge energy misalignment error of the predicted S, Ti and Fe K edge XANES is significantly reduced by about 55%. The method demonstrated in this work opens up a new way to achieve fast, universal, and experiment-calibrated XANES prediction.",Materials Science,http://arxiv.org/abs/2512.23449v1,arXiv,2
"Order versus disorder in the structure of materials plays a key role in the theoretical prediction of their properties. However, this structural description appears to be ineffective for new families of materials such as high entropy alloys (HEAs), which combine crystallographic order with chemical disorder. Here, we demonstrate for five-element HEAs as pure solid solutions that the chemical disorder of the elements decorating their cubic structure underlies the generation of second optical harmonics, overcoming the theoretical limit imposed on centrosymmetric crystals. Moreover, we discover that this disorder, inherent to HEAs, sets a threshold for non-linear light emission from the 4th to the 26th order. As a consequence of the 0.5 eV broadening of the energy levels of the five elements of the HEA, the emission spectrum covers broad visible (400-650 nm) and infrared (800-1600 nm) ranges. In addition to the challenge of theoretically predicting non-linear effects in unconventional materials, the duality of structural order and chemical disorder in HEAs offers the opportunity to design sustainable alternatives to urgently needed optical materials.",Materials Science,http://arxiv.org/abs/2512.23370v1,arXiv,2
"Altermagnetism simultaneously possesses nonrelativistic spin responses and zero net magnetization, thus combining advantages of ferromagnetism and antiferromagnetism. This superiority originates from its unique dual feature, i.e., opposite-magnetic sublattices in real space and alternating spin polarization in momentum space enforced by the same crystal symmetry. Therefore, the determination of an altermagnetic order and its unique spin response inherently necessitates atomic-scale spin-resolved measurements in real and momentum spaces, an experimental milestone yet to be achieved. Here, via utilizing the helical edge (hinge) modes of a higher order topological insulator as the spin sensor, we realize spin-resolved scanning tunneling microscopy which enables us to pin down the dual-space feature of a layered $d$-wave altermagnet, KV$_2$Se$_2$O. In real space, atomic-registered mapping demonstrates the checkerboard antiferromagnetic order together with density-wave lattice modulation, and in momentum space, spin-resolved spectroscopic imaging provides a direct visualization of d-wave spin splitting of the band structure. Critically, using this new topology-guaranteed spin filter we directly reveal the unidirectional, spin-polarized quasiparticle excitations originating from the crystal symmetry-paired X and Y valleys around opposite magnetic sublattices simultaneously --the unique spin response for $d$-wave altermagnetism. Our experiments establish a solid basis for the exploration and utilization of altermagnetism in layered materials and further facilitate access to atomic-scale spin sensing and manipulating of 2D quantum materials.",Materials Science,http://arxiv.org/abs/2512.23290v1,arXiv,2
"Topologically protected edge channels show prospects for quantum devices. They have been found experimentally in two-dimensional (2D) quantum spin Hall insulators (QSHIs), weak topological insulators and higher-order topological insulators (HOTIs), but the number of materials realizing these topologies is still quite limited. Here, we provide evidence for topological edge states within a novel topology named three-dimensional (3D) QSHIs. Its topology originates solely from a nonzero $S_z$ spin Chern number for each $k_z$ plane of the crystal and is realized in bulk $Î±$-Bi$_4$I$_4$ with trivial symmetry indicators, as we show by density functional theory calculations. We experimentally observe the related edge states at each type of monolayer and bilayer step of this material by scanning tunneling microscopy. Consistently, the edge states are neither interrupted, nor backscattered by defects at the step edges corroborating their helical character as expected from the nontrivial topology. Furthermore, two individual edge channels are directly observed at bilayer steps without visible interaction gap opening, demonstrating the robustness of these edge modes against vertical stacking. Our results establish $Î±$-Bi$_4$I$_4$ as the first material realization of a 3D QSHI whose definition goes beyond the scope of topological symmetry indicators, and provide a pathway for realizing nearly-quantized spin Hall conductivity per unit cell in a bulk crystal.",Materials Science,http://arxiv.org/abs/2512.23277v1,arXiv,2
"The recent synthesis of Goldene, a 2D sheet of gold exfoliated from $Ti_3AuC_2$, offers high specific surface area (~260 $m^2g^{-1}$), roughly twice that of fine nanodots (~100 $m^2g^{-1}$), and unique electronic properties due to its dense d-orbital. In this work, we investigate the adsorption of single atom catalyst (SAC) of hydrogen atom on pristine goldene (pG), monovacant goldene (vG), and sulfur-functionalized variants (thiol-pG and thiol-vG) using ab initio calculations. The adsorption energy of a single H atom and $ÎG_H$, determines the efficiency of the Volmer step of the hydrogen evolution reaction (HER) and is a key descriptor for HER activity. We explore various potential sites for H adsorption and its impact on descriptors such as Bader charges, d-band shift and the exchange current density.",Materials Science,http://arxiv.org/abs/2512.23270v1,arXiv,2
"Two-dimensional (2D) materials provide unique opportunities to realize emergent phenomena by reducing dimensionality. Using scanning tunneling microscopy combined with first-principles calculations, we determine an intriguing case of a metal-insulator transition (MIT) in a bulk compound, (TBA)$_{0.3}$VSe$_2$. Atomic-scale imaging reveals that the initial $4a_0 \times 4a_0$ charge density wave (CDW) order in 1T-VSe$_2$ transforms to $\sqrt{7}a_0 \times \sqrt{3}a_0$ ordering upon intercalation, which is associated with an insulating gap with a magnitude of up to approximately 115 meV. Our calculations reveal that this energy gap is highly tunable through electron doping introduced by the intercalant. Moreover, the robustness of the $\sqrt{7}a_0 \times \sqrt{3}a_0$ CDW order against the Lifshitz transition points to the key role of electron-phonon interactions in stabilizing the CDW state. Our work clarifies a rare example of a CDW-driven MIT in quasi-2D materials and establishes cation intercalation as an effective pathway for tuning both the dimensionality and the carrier concentration without inducing strain or disorder.",Materials Science,http://arxiv.org/abs/2512.23264v1,arXiv,2
"Graphene, a two-dimensional monolayer of sp2-bonded carbon atoms in a honeycomb lattice, possesses exceptional electronic, mechanical, and quantum properties, making it highly attractive for energy storage, spintronics, and microelectronics. Functionalizing graphene with platinum (Pt) adatoms can further enhance its properties, particularly for hydrogen storage applications. In this study, we experimentally investigate hydrogen adsorption on Pt-decorated graphene using Elastic Recoil Detection Analysis (ERDA). By irradiating the Pt/graphene film with a 4.1 MeV C2+ ion beam and detecting recoiled hydrogen atoms at a 30 degree scattering angle, we obtain the hydrogen depth profile, providing critical insights into its storage behavior.",Materials Science,http://arxiv.org/abs/2512.23261v1,arXiv,2
"The interplay between laser parameters and liquid environments dictates the outcome of femtosecond laser-induced nanoparticle modification. We present a study of gold and iron oxide nanoparticles in water and a water-acetone mixture, irradiated with femtosecond lasers at 808 nm and 404 nm. While aggregation was observed in pure water at both wavelengths, the results revealed a strong stability and a rather unexpected wavelength-dependency in the acetone-water mixture. In this case, 808 nm irradiation produced some decrease in nanoparticle sizes, while 404 nm led to some nanoparticle growth. As a result, the acetone effect is found to be twofold: (i) on one hand, it helps to prevent aggregation; (ii) on the other hand, it acts as a reactive medium allowing to tune the nanoparticle size and composition simply by changing laser wavelength. So, this work emphasizes that solvent physical properties as well as laser-induced chemical processes in the solvent are not merely secondary effects but can dominate the final morphological outcome, providing a predictive framework for nanoparticle synthesis.",Materials Science,http://arxiv.org/abs/2512.23256v1,arXiv,2
"The Pomeranchuk effect is a counterintuitive phenomenon where liquid helium-3 (3He) solidifies under specific pressures, not when cooled, but when heated. This behaviour originates from the magnetic entropy of nuclear spins, suggesting a magnetic field should influence it. However, its detailed response to magnetic fields remains elusive due to the small nuclear magneton of 3He and lack of analogous fermion systems. Here, we show that an electron system also exhibit the Pomeranchuk effect, where the Fermi liquid state solidifies in a high magnetic field, unlike conventional electron systems where a field melts an electron solid into a metal. Remarkably, the electron system displays a reentrant liquid state in ultrahigh fields. These responses are explained by changes in magnetic entropy and magnetisation, extending the underlying physics to 3He. Our findings clarify magnetic-field impact on the Pomeranchuk effect and open avenues for magnetic control of chemical interactions.",Materials Science,http://arxiv.org/abs/2512.23254v1,arXiv,2
"Altermagnets demonstrate significant potential in spintronics due to their unique non-relativistic spin-splitting properties, yet altermagnetic devices still face challenges in efficiently switching logic states. Here, we report electrostatically controllable spin-momentum locking in bilayer Cr$_{2}$SeO and design a dual-gate altermagnetic tunnel junction (AMTJ), which can switch between high and low resistance states without switching the NÃ©el vector. First-principles calculations demonstrate that vertical electric field can induce significant spin splitting in bilayer Cr$_{2}$SeO. Reversing the electric field direction can alter the spin-momentum locking in bilayer Cr$_{2}$SeO. Leveraging this electric-field-tunable spin splitting, the dual-gate AMTJ exhibits an ultrahigh tunneling magnetoresistance (TMR) ratio of $10^{7}$. This work provides theoretical support for the design of fully electrically controlled AMTJs and demonstrates their great potential for applications in spintronic devices.",Materials Science,http://arxiv.org/abs/2512.23253v1,arXiv,2
"H2O is a unique substance with exceptional thermal properties arising from the subtle interplay between its electronic, phononic, and structural degrees of freedom. Of particular interest in H2O are the negative thermal expansion (NTE) phenomena, observed in its solid phase (ice) at low temperature, and in its liquid phase (water) near the freezing temperature. Furthermore, ice and water exhibit the abnormal volume isotope effect (VIE), where volume expansions occur when replacing H with its heavier isotope, deuterium (D). In order to capture more conceptual and intuitive understanding of intriguing NTE and VIE phenomena in ice and water, we have explored isotope effects in their NTE and melting properties by employing a type of Born-Oppenheimer-approximation approach and the Lindemann criterion. Our findings demonstrate that unusual isotope effects in these phenomena stem from competition between zero-point-energy phonons, thermal phonons, and the hydrogen bonding in H2O. All these components originate from nuclear quantum mechanical (QM) processes, revealing that QM physics plays a crucial role in the seemingly classical ice/water systems.",Materials Science,http://arxiv.org/abs/2512.23247v1,arXiv,2
"We demonstrate anisotropic photostriction in two-dimensional orthorhombic semiconductors using time-dependent density functional theory. By tracing the dynamics of photoexcited carriers, we establish a quantitative link between carrier density and lattice deformation in layered black phosphorus and germanium selenides. The structural response exhibits significant anisotropy, featuring lattice expansion along the armchair direction and contraction along the zigzag direction, which is attributed to the interplay between charge redistribution and intrinsic lattice anisotropy. Both the magnitude and orientation of the photostrictive strains can be tuned by photodoping densities, enabling precise control over the photoinduced response. Notably, the photoinduced strains significantly increase carrier recombination lifetimes by suppressing nonradiative recombination, primarily due to the enlarged bandgap and weakened nonadiabatic coupling. These results provide microscopic insight into the origin of anisotropic photostriction in low-dimensional systems and lay the groundwork for light-controllable, directionally sensitive optomechanical devices at the atomic scale.",Materials Science,http://arxiv.org/abs/2512.23187v1,arXiv,2
"Cerium-based lanthanide high-entropy oxides (LN-HEOs) are promising candidates for solid-state electrolyte (mass transport) applications due to their ability to accommodate high concentrations of oxygen vacancies while retaining a fluorite-derived structure. However, synthesis often yields undesired ordered oxygen-deficient phases, such as bixbyite, depending on composition and processing conditions. We utilize first-principles density functional theory (DFT) calculations to systematically investigate phase stability in the model system Ce$_x$(YLaPrSm)$_{1-x}$O$_{2-Î´}$, with the aim of elucidating the thermodynamic factors governing fluorite-bixbyite competition and identifying structure-property relationships to oxygen transport. By independently varying cerium concentration and oxygen vacancy content, we predict that the transition from disordered fluorite to ordered bixbyite is driven primarily by compositional and vacancy-ordering effects, rather than through changes in cation valence. Free-energy analysis reveals that at high vacancy concentrations, bixbyite is enthalpically favored due to ordered oxygen vacancies, while fluorite is stabilized at lower vacancy concentrations and higher cerium content through configurational entropy of the anion sublattice. These DFT results clarify the competing energetic contributions that control phase stability and structure-valence relationships in LN-HEOs and establishes a mechanistic framework for designing vacancy-tolerant oxide electrolytes with tunable phase behavior.",Materials Science,http://arxiv.org/abs/2512.23120v1,arXiv,2
"Titanium MXenes are two-dimensional inorganic structures composed of titanium and carbon or nitrogen elements, with distinctive electronic, thermal and mechanical properties. Despite the extensive experimental investigation, there is a paucity of computational studies at the level of classical molecular dynamics (MD). As demonstrated in a preceding study, known MD potentials are not capable of fully reproducing the structure and elastic properties of every titanium MXene. In this study, we present a simply trained, but yet efficient, non-density functional theory-based machine learning interatomic potential (MLIP) capable of simulating the structure and elastic properties of titanium MXenes and bulk titanium carbide and nitride with precision comparable to DFT calculations. The training process for the MLIP is delineated herein, in conjunction with a series of dynamical tests. Limitations of the MLIP and steps towards improving its efficacy to simulate titanium MXenes are discussed.",Materials Science,http://arxiv.org/abs/2512.23063v1,arXiv,2
"Density functional theory (DFT) and machine learning potentials (MLPs) are essential for predicting and understanding materials properties, yet preparing, executing, and analyzing these simulations typically requires extensive scripting, multi-step procedures, and significant high-performance computing (HPC) expertise. These challenges hinder reproducibility and slow down discovery. Here, we introduce Masgent, an AI-assisted materials simulation agent that unifies structure manipulation, automated VASP input generation, DFT workflow construction and analysis, fast MLP-based simulations, and lightweight machine learning (ML) utilities within a single platform. Powered by large language models (LLMs), Masgent enables researchers to perform complex simulation tasks through natural-language interaction, eliminating most manual scripting and reducing setup time from hours to seconds. By standardizing protocols and integrating advanced simulation and data-driven tools, Masgent democratizes access to state-of-the-art computational methodologies, accelerating hypothesis testing, pre-screening, and exploratory research for both new and experienced practitioners.",Materials Science,http://arxiv.org/abs/2512.23010v1,arXiv,2
"Topological magnetic textures are particle-like spin configurations stabilized by competing interactions. Their formation is commonly attributed to fluctuation-driven, first-order nucleation processes requiring activation over a topological energy barrier. Here, we demonstrate an alternative barrier- and fluctuation-free pathway for nucleating topological magnetic textures, triggered in our experiments by an excitation-induced spin reorientation transition. By combining x-ray imaging, scattering and micromagnetic simulations, we show that the system follows a deterministic cascade of symmetry-breaking phase transitions after excitation. First, the system undergoes a second-order phase transition from a homogeneous state to weak stripe domains, then a first-order transition to topologically trivial bubbles, and finally a topological switching event into skyrmionic textures. Through simulations, we generalize our findings and demonstrate that this pathway is active in a vast range of low-anisotropy materials. This previously unrecognized, spontaneous transition pathway suggests strategies for rapid, low-energy generation of topological spin textures and points to a general role of intrinsic modulational instabilities in phase transitions beyond magnetism.",Materials Science,http://arxiv.org/abs/2512.22947v1,arXiv,2
"Understanding the mechanisms of hydrogen embrittlement (HE) is essential for advancing next-generation high-strength steels, thereby motivating the development of highly accurate machine-learning interatomic potentials (MLIPs) for the Fe-H binary system. However, the substantial computational expense associated with existing MLIPs has limited their applicability in practical, large-scale simulations. In this study, we construct a new MLIP within the Performant Implementation of the Atomic Cluster Expansion (PACE) framework, trained on a comprehensive HE-related dataset generated through a concurrent-learning strategy. The resulting potential achieves density functional theory-level accuracy in reproducing a wide range of lattice defects in alpha-Fe and their interactions with hydrogen, including both screw and edge dislocations. More importantly, it accurately captures the deformation and fracture behavior of nanopolycrystals containing hydrogen-segregated general grain boundaries-phenomena not explicitly represented in the training data. Despite its high fidelity, the developed potential requires computational resources only several tens of times greater than empirical potentials and is more than an order of magnitude faster than previously reported MLIPs. By delivering both a high-precision and computationally efficient potential, as well as a generalizable methodology for constructing such models, this study significantly advances the atomic-scale understanding of HE across a broad range of metallic materials.",Materials Science,http://arxiv.org/abs/2512.22934v1,arXiv,2
"We report the influence of Ce$^{3+}$ substitution on the magnetic structures and phonon dynamics in SmCrO$_3$ perovskites. Magnetic landscapes are spanned by long-range canted anti-ferromagnetism, AFM with Neel temperatures $\sim$196 K accompanied by spin-reorientation transitions, T$_{SRPT}$ at 42 K. In Sm$_{0.9}$Ce$_{0.1}$CrO$_3$ (SCCO), Ce$^{3+}$ substitution at Sm$^{3+}$ sites transform the weak ferromagnetic (FM) $Î_4$ state into robust AFM $Î_1$ configuration through a gradual crossover. Such coexistence of magnetic spin configurations $Î_1$(AFM) to $Î_4$ (WFM) results in the enhanced high coercive field and a pronounced exchange bias-field, $H_{\mathrm{EB}}$ $\sim$ 2 kOe. Spin-only driven magneto-crystalline anisotropy of Cr$^{3+}$ and spin-orbit driven magnetic moment in Sm$^{3+}$, and Ce$^{3+}$ exhibits spin-phonon coupling through $A_{1g}(6)$ mode in SCCO are consistent with the temperature dependent spectral features of the isostructural magnetic systems and quite significant in SCCO which is in accordance with the higher structural distortion in SCCO. These results demonstrate that site-specific R$^{3+}$ substitution modulates lattice distortions, spin-phonon coupling, and spin-orbit interactions, offering pathways to optimize perovskites for diverse spintronic applications.",Materials Science,http://arxiv.org/abs/2512.22891v1,arXiv,2
"We investigate the effect of Ce$^{3+}$ substitution on the magnetic ordering and phonon dynamics of the GdCrO$_3$ orthorhombic perovskite. The Ce doped compound exhibits long-range canted antiferromagnetism with Neel transitions, T$_N$ at $\sim$ 173 K, accompanied by spin-flip, T$_{SF}$ at $\sim$ 10 K. Ce$^{3+}$ incorporation drives a modification of the spin-flip transition from the $Î{(G_x,A_y,F_z)}$ configuration to $Î$ inducing a reorientation of the spin axis between the (001) and (001) crystallographic planes. This spin reorientation is governed by Zeeman energy and produces pronounced field-induced irreversibility between FCC and FCW magnetization processes. The substituted compound Gd$_{0.9}$Ce$_{0.1}$CrO$_3$ (GCCO) exhibits a remarkably large magnetic entropy change, $Î$ S $\sim$ 45-40 J/kg-K for $Î$ H = 90-70 kOe at 3 K among the highest reported for rare-earth orthochromites. The interplay of spin-only magnetocrystalline anisotropy from Cr$^{3+}$ and spin-orbit-driven magnetic moments of Gd$^{3+}$ and Ce$^{3+}$ results in pronounced spin-phonon coupling, manifested through the A$_{1g}$(6) vibrational mode. The observed temperature-dependent spectral evolution is consistent with behaviour reported in isostructural magnetic perovskites.",Materials Science,http://arxiv.org/abs/2512.22884v1,arXiv,2
"$SrVO_3$ (SVO), a model correlated metal and a promising transparent conducting oxide, develops a several-nanometer-thick near-surface region (NSR), rich in $V^{5+}$ species under ambient conditions. This oxidized layer obscures the intrinsic correlated-metallic $V^{4+}$ character and limits both fundamental studies of the physics and the material's integration into electronic devices. Here, we demonstrate a direct and controllable approach for recovering the metallic SVO surface by thermally reducing the NSR under ultra-high vacuum. Real-time in-situ X-ray photoelectron spectroscopy (XPS) reveals a sharp transformation from a $V^{5+}$-dominated surface to mixed valence states, dominated by $V^{4+}$, and a recovery of its metallic character. Ex-situ X-ray diffraction (XRD), atomic force microscopy (AFM), and high-resolution scanning electron microscopy (HR-SEM) suggest that this transformation is accompanied by mass redistribution and partial oxygen loss, leading to nanoscale surface reorganization and modest lattice expansion. While thermodynamic considerations motivate evaluation of a $V_2O_5$ volatilization pathway, the combined experimental evidence instead points toward a predominantly structural surface reorganization. These findings establish a practical method for obtaining predominantly $V^{4+}$ SVO surfaces without protective capping layers, a capability that expands the utility of SVO for advanced spectroscopies, interface engineering, and oxide-electronics device integration.",Materials Science,http://arxiv.org/abs/2512.22843v1,arXiv,2
"Deformable and flexible electronics have garnered significant attention due to their distinctive properties; however, their current applications are primarily limited to the thermoelectric domain. Expanding the range of these electronics and their application scope represents a pivotal trend in their development. In this work, a plastic inorganic semiconductor material, Sn2BiS2I3, with a band gap of 1.2 eV was synthesized and fabricated into a three-electrode flexible and portable electronic tongue capable of detecting heavy metal elements. The electronic tongue device exhibits exceptional linearity and demonstrates resistance against interference from impurity ions. The linear regression equation is expressed as Y=0.24+19.06X, yielding a linear coefficient of approximately 0.96, and the detectable limit stands at around 1.1 ppb, surpassing the 2.0 ppb limit of the ICP-AES instrument. Furthermore, mechanical testing reveals the favorable plasticity of Sn2BiS2I3, as evidenced by the absence of cracks during nanoindentation. The indentation hardness of Sn2BiS2I3 is approximately 1.67 GPa, slightly exceeding the hardness of Cu, which is 1.25 GPa. This study expands the repertoire of deformable and flexible electronics, offering a new and exceptional choice for biomimetic tongue sensor materials.",Materials Science,http://arxiv.org/abs/2512.22811v1,arXiv,2
"Emerging spin-orbit-torque devices based on spin valves require an ultrathin (e.g., $\lesssim$2 nm) magnetic free layer to maximize the torque per moment. However, reducing the free-layer thickness deteriorates the giant magnetoresistance (GMR) signal for electrical readout. Here, we demonstrate that the addition of a 1-nm Cu seed layer enables high GMR ratios of 5-7% at free-layer thicknesses of $\lesssim$2 nm by promoting high-quality, textured growth of spin valves. Our work offers a pathway for engineering high-signal GMR readout in spin-orbit-torque digital memories and neuromorphic computers.",Materials Science,http://arxiv.org/abs/2512.22726v1,arXiv,2
"Sensing biological forces with microscopic lasers is an emerging technique that offers significant advantages over conventional fluorescent probes and imaging-based techniques. However, the limited availability of suitable deformable or elastic microlaser materials is restricting the scale of forces that can be detected which strongly narrows their overall applicability. Here, we describe the synthesis of spherical whispering gallery mode microbead lasers from a commercially available elastomer material in a microfluidic system with high viscosity. Upon doping with organic dye molecules, the microbeads show excellent lasing characteristics with low lasing thresholds. Measurements of the mechanical properties reveal that the lasing characteristics are directly proportional to the applied external force. The measured Young's modulus confirms that the newly synthesized microlasers are very soft and can detect higher forces than previously applied deformable microlasers made from oil droplets. Furthermore, we show that elastomer microlasers are non-toxic and stable in aqueous environments, making them ideally suited for sensing forces inside tissues and small animals.",Materials Science,http://arxiv.org/abs/2512.22707v1,arXiv,2
"The nominal divide between $p$- and $d$-electron systems often obscures a deep underlying unity in condensed matter physics. This review elucidates the orbital homology between the $p$ and $t_{2g}$ orbital manifolds, establishing the correspondence that extends from minimal model Hamiltonians to the complex behaviors of real quantum materials. We demonstrate that despite their distinct atomic origins, these orbitals host nearly identical hopping physics and spin-orbit coupling, formalized through an effective ${l=1}$ angular momentum algebra for the $t_{2g}$ case. This equivalence allows one to transpose physical intuition and theoretical models developed for $p$-orbital systems directly onto the more complex $t_{2g}$ materials, and vice versa. We showcase how this paradigm provides a unified understanding of emergent phenomena, including non-trivial band topology, itinerant ferromagnetism, and unconventional superconductivity, across a wide range of platforms, from transition metal compounds, two-dimensional oxide heterostructures, and iron-based superconductors, to $p$-orbital ultracold gases. Ultimately, this $p$-$t_{2g}$ homology serves not only as a tool for interpretation but also as a robust design principle for engineering novel quantum states.",Materials Science,http://arxiv.org/abs/2512.22658v1,arXiv,2
"In this work, we study the anisotropic magnetoresistance (AMR) behavior of [001] epitaxial Fe65Co35 thin films along different crystallographic directions as a function of temperature. The AMR ratio is found to strongly depend on the current orientation relative to the crystal axes, reaching 0.16 % and 0.10 % at room temperature when the current is applied along the magnetic hard and easy axes, respectively. Moreover, the AMR ratio decreases at different rates as the temperature is reduced to 80 K. The longitudinal and transverse magnetoresistance curves were fitted using the Stoner-Wohlfarth formalism to describe the magnetization reversal path and to extract the magnetic anisotropy constants. The fitted cubic and uniaxial anisotropy constants are Kc = -2.36 kJ/m3 and Ku = 2.18 kJ/m3, verifying the change in the cubic anisotropy compared to Fe-richer Fe100-xCox compositions. These results demonstrate that by tailoring the crystalline orientation and temperature dependence of AMR, epitaxial Fe65Co35 thin films can enable the design of magnetic sensors with tunable sensitivity.",Materials Science,http://arxiv.org/abs/2512.22590v1,arXiv,2
"We report a molecular dynamics investigation of 2,4-dichlorophenoxyacetic acid (2,4-D) adsorption at the aqueous-biochar interface using experimentally constrained woody biochar models representative of softwood-derived biochars produced at 400, 600 and 800 $Â°$C. The models reproduce experimental descriptors (H/C, O/C, aromaticity, true density, and surface functionality) of their experimental counterparts, and simulations enable calculation of adsorption isotherms that align with available experimental measurements. Our results reveal that 2,4-D$^{-}$ uptake is governed by a synergy of three interaction classes: (i) $Ï$-$Ï$ and $Ï$-Cl contacts with graphitic domains with either parallel or perpendicular alignments, (ii) polar interactions including H-bonding to surface -OH and other oxygen-containing groups, and (iii) Na$^{+}$-mediated cation bridging that links 2,4-D$^{-}$ anion to surface oxygens, that would have an increasing relevance for biochars near or above the pH at point of zero charge. Notably, we found that low-temperature produced biochars, which retain higher densities of surface O functionalities, exhibit higher adsorption per unit surface area due to cooperative polar interactions alongside $Ï$-$Ï$ binding, whereas medium-to-high temperature biochars rely more on $Ï$-$Ï$ and cation-bridging mechanisms. The distinct adsorption distances measured emphasize surface heterogeneity and porosity. Taken together, these atomistic insights corroborate experimental observations and yield actionable guidance for the rational design of biochars for remediation of anionic herbicides, highlighting how surface functionality and solution chemistry can be tuned to optimize sorption. Our approach provides a general framework to interrogate pollutant-biochar interactions and to inform remediation strategies.",Materials Science,http://arxiv.org/abs/2512.22573v1,arXiv,2
"Aluminum nanoparticles (ANPs) are among the most energy-dense solid fuels, yet the atomic mechanisms governing their transition from passivated particles to explosive reactants remain elusive. This stems from a fundamental computational bottleneck: ab initio methods offer quantum accuracy but are restricted to small spatiotemporal scales (< 500 atoms, picoseconds), while empirical force fields lack the reactive fidelity required for complex combustion environments. Herein, we bridge this gap by employing a ""human-in-the-loop"" closed-loop framework where self-auditing AI Agents validate the evolution of a machine learning potential (MLP). By acting as scientific sentinels that visualize hidden model artifacts for human decision-making, this collaborative cycle ensures quantum mechanical accuracy while exhibiting near-linear scalability to million-atom systems and accessing nanosecond timescales (energy RMSE: 1.2 meV/atom, force RMSE: 0.126 eV/Angstrom). Strikingly, our simulations reveal a temperature-regulated dual-mode oxidation mechanism: at moderate temperatures, the oxide shell acts as a dynamic ""gatekeeper,"" regulating oxidation through a ""breathing mode"" of transient nanochannels; above a critical threshold, a ""rupture mode"" unleashes catastrophic shell failure and explosive combustion. Importantly, we resolve a decades-old controversy by demonstrating that aluminum cation outward diffusion, rather than oxygen transport, dominates mass transfer across all temperature regimes, with diffusion coefficients consistently exceeding those of oxygen by 2-3 orders of magnitude. These discoveries establish a unified atomic-scale framework for energetic nanomaterial design, enabling the precision engineering of ignition sensitivity and energy release rates through intelligent computational design.",Materials Science,http://arxiv.org/abs/2512.22529v1,arXiv,2
"Frustrated Kondo lattices are ideal platforms for studying how both the Kondo effect and quantum fluctuations compete with the magnetic exchange interactions that drive magnetic ordering. Here, we investigate the effect of tuning the heavy-fermion compound CePdIn, which crystallizes in the geometrically frustrated ZrNiAl-type structure, using applied magnetic fields and hydrostatic pressure. At ambient pressure, CePdIn exhibits two magnetic transitions, one at $T_{\rm{N}} \approx 1.65$ K and another at $T_{\rm{M}} \approx 1.15$ K, which are both suppressed by applied $c$-axis fields. Upon applying pressure in zero magnetic field, there is a non-monotonic evolution of $T_{\rm{N}}$, which decreases to 0.8 K at 2.3 GPa, before abruptly increasing to 1.5 K at 2.6 GPa. At higher pressures, $T_{\rm{N}}$ has a weak pressure dependence, and vanishes near 5 GPa. Together with the high-pressure phase being more robust to applied fields, these results suggest two distinct antiferromagnetic phases in CePdIn, which are separated near 2.6 GPa, and this change may be driven by the evolution of the underlying electronic structure due to enhanced Kondo hybridization under pressure.",Materials Science,http://arxiv.org/abs/2512.22528v1,arXiv,2
"The nanoscale charge environment critically influences semiconductor physics and device performance. While conventional bulk characterization techniques provide volume-averaged defect properties, they lack the spatial resolution to resolve nanoscale charge heterogeneity and identify microscopic noise sources. Here, we utilize single PL5 centers in 4H-SiC as room-temperature broadband quantum sensors to fill in the gap. We report the first real-time, nanoscale observation of singlecharge tunneling dynamics in a commercial semiconductor at room temperature, by monitoring the random telegraph noise using optically detected magnetic resonance (ODMR). This capability enables an electrical noise imaging technique, showing distinct noise variations across different wafer substrates. By employing dynamical decoupling, we extend noise spectroscopy from near-DC to MHz frequencies, uncovering significant noise spectral density correlations across frequency bands. Finally, we probe MHz-GHz noise and identify its origin via T1 relaxation spectroscopy, obtaining the first nanoscale electron paramagnetic resonance (EPR) spectroscopic fingerprint of charge defects in SiC. These techniques open avenues for characterizing noise environments in semiconductor devices, providing critical insights for optimizing SiC fabrication processes, defect control, and advancing quantum technologies.",Materials Science,http://arxiv.org/abs/2512.22521v1,arXiv,2
"Understanding and controlling the chemical states both in the bulk and at the interfaces of complex oxide thin films is essential for engineering a wide range of electronic, optical, and magnetic functionalities, which arise through emergent phenomena such as two-dimensional electron gases, interfacial magnetism, and associated phase transitions. Here, we demonstrate the use of in situ Auger Electron Spectroscopy (AES) as a powerful tool for probing oxidation states and dynamic chemical processes during the growth of complex oxide heterostructures. By leveraging the chemical sensitivity of AES to subtle changes in valence electron populations, we show that this technique can distinguish distinct oxidation states in multivalent perovskite manganate and vanadate systems with high fidelity during deposition. Furthermore, we show evidence for dynamic chemical phenomena, specifically charge transfer processes at the polar-nonpolar LaMnO3/SrTiO3 interface. Our results establish in situ AES as a powerful diagnostic tool for monitoring and controlling interfacial chemistry during thin film growth, offering a pathway toward the atomic-scale engineering of chemical states in functional oxide heterostructures.",Materials Science,http://arxiv.org/abs/2512.22507v1,arXiv,2
"Kitaev materials are of great interest due to their potential in realizing quantum spin liquid (QSL) states and applications in topological quantum computing. In the pursuit of realizing Kitaev QSL, a Mott insulator with strong bond-dependent frustration and weak geometric frustration is highly desirable. Here we explore Kitaev physics in the van der Waals triangular antiferromagnet (AF) CoI$_2$, through the spin-orbital states and Wannier function analyses, exact diagonalization and density matrix renormalization group study of the electronic structure and magnetic properties. We find that the high-spin Co$^{2+}$ ion is in the $J_\mathrm{eff}=1/2$ state because of strong spin-orbit coupling, and the weak trigonal elongation and crystal field contribute to the observed weak in-plane magnetic anisotropy. The strong $t_{2g}$-$e_g$ hopping via the strong Co 3$d$-I 5$p$ hybridization gives rise to a strong Kitaev interaction ($K_1$) at the first nearest neighbors (1NN), and the long Co-Co distance and the weak $t_{2g}$-$t_{2g}$ hoppings determine a weak Heisenberg interaction $J_1$. The resultant $|K_1/J_1|$ = 6.63 confirms a strong bond-dependent frustration, while the geometric frustration due to the 3NN Heisenberg interaction $J_3$ gets involved, and they all together result in the experimental helical AF order in CoI$_2$. We then propose to suppress the $J_3$ using a partial Mg substitution for Co, and indeed we find that Co$_{2/3}$Mg$_{1/3}$I$_2$ has the much reduced geometric frustration but hosts the robust bond-dependent frustration, and thus it would be a promising Kitaev material being so far closest to the QSL state.",Materials Science,http://arxiv.org/abs/2512.22453v1,arXiv,2
"Materials featuring kagome lattices have attracted significant research interest due to their unique geometric frustration, which gives rise to rich physical phenomena such as non-trivial topology, spin fluctuations, and superconductivity. In this work, using CaPd5 as the prototype structure, we discover and systematically investigate a new class of kagome superconductors, CaMxPd5-x (M = Nb and V) alloys. First-principles calculations confirm that these compounds are non-magnetic metals, among which four are dynamically stable: CaNb5, CaV5, CaNb2Pd3, and CaV2Pd3. CaNb5 is identified as a strong electron-phonon coupling (EPC) superconductor with the highest superconducting transition temperature (Tc) of 10.1 K, which can be further increased to 12.8 K under external pressure. In contrast, CaV5, CaNb2Pd3, and CaV2Pd3 exhibit weaker EPC and correspondingly lower Tc values. Furthermore, by applying the method of symmetry indicators, we systematically classify the topological and nodal characteristics of CaNb5, providing valuable insights for determining its superconducting pairing symmetry. Our findings demonstrate that Nb and V substitution in kagome CaPd5 provides an effective route for designing a new type of kagome superconductor with relatively high Tc. This study also offers new perspectives on topological superconductivity in kagome systems and establishes a useful guideline for discovering other superconducting materials with unique properties.",Materials Science,http://arxiv.org/abs/2512.22430v1,arXiv,2
"Superionic conduction in solid-state materials is governed not only by static factors, such as structure and composition, but also by dynamic interactions between the mobile ion and the crystal lattice. Specifically, the dynamics of lattice vibrations, or phonons, have attracted interest because of their hypothesized ability to facilitate superionic conduction. However, direct experimental measurement of the role of phonons in ionic conduction is challenging due to the fast intrinsic timescales of ion hopping and the difficulty of driving relevant phonon modes, which often lie in the low-energy THz regime. To overcome these limitations, we use laser-driven ultrafast impedance spectroscopy (LUIS). LUIS resonantly excites phonons using a THz field and probes ion hopping with picosecond time resolution. We apply LUIS to understand the dynamical role of phonons in $Li_7La_3Zr_2O_{12}$ (LLZO). When in its cubic phase (c-LLZO), this garnet-type solid electrolyte has an ionic conductivity two orders of magnitude greater than its tetragonal phase (t-LLZO). T-LLZO is characterized by an ordered and filled $Li^+$ sublattice necessitating synchronous ion hopping. In contrast, c-LLZO is characterized by a disordered and vacancy-rich $Li^+$ sublattice, and has a conduction mechanism dominated by single hops. We find that, upon excitation of phonons in the 0.5-7.5 THz range, the impedance of t-LLZO experiences a longer ion hopping decay signal in comparison to c-LLZO. The results suggest that phonon-mediated ionic conduction by THz modes may lead to larger ion displacements in ordered and fully occupied mobile ion sublattices as opposed to those that are disordered and vacancy-rich. Overall, this work highlights the interplay between static and dynamic factors that enables improved ionic conductivity in otherwise poorly conducting inorganic solids.",Materials Science,http://arxiv.org/abs/2512.22427v1,arXiv,2
"Silver iodide (AgI) thin films offer a compelling platform for studying nonlinear optical phenomena due to their intrinsic noncentrosymmetric lattice and direct band gap. Here, we investigate the nonlinear optical properties of AgI thin films grown by physical vapor deposition that selectively produce zincblende (\zbAgI) and wurtzite (\wzAgI) phases. Using a combination of polarization-resolved second harmonic generation (SHG) and two-photon photoluminescence (2PPL) spectroscopy, we identify clear phase- and morphology-dependent anisotropic nonlinear responses. Triangular \zbAgI $(111)$ flakes exhibit sixfold SHG symmetry and isotropic 2PPL emission, while rod-shaped \wzAgI $(101)$ samples display twofold-symmetric patterns in both SHG and 2PPL, which are explained by polarization analysis using second- and third- order nonlinear susceptibilities. These findings establish AgI as a promising halide semiconductor platform for phase-selective nonlinear optics and quantum photonic applications.",Materials Science,http://arxiv.org/abs/2512.22409v1,arXiv,2
"Artificial Intelligence (AI), particularly Large Language Models (LLMs), is transforming scientific discovery, enabling rapid knowledge generation and hypothesis formulation. However, a critical challenge is hallucination, where LLMs generate factually incorrect or misleading information, compromising research integrity. To address this, we introduce HalluMatData, a benchmark dataset for evaluating hallucination detection methods, factual consistency, and response robustness in AI-generated materials science content. Alongside this, we propose HalluMatDetector, a multi-stage hallucination detection framework that integrates intrinsic verification, multi-source retrieval, contradiction graph analysis, and metric-based assessment to detect and mitigate LLM hallucinations. Our findings reveal that hallucination levels vary significantly across materials science subdomains, with high-entropy queries exhibiting greater factual inconsistencies. By utilizing HalluMatDetector verification pipeline, we reduce hallucination rates by 30% compared to standard LLM outputs. Furthermore, we introduce the Paraphrased Hallucination Consistency Score (PHCS) to quantify inconsistencies in LLM responses across semantically equivalent queries, offering deeper insights into model reliability.",Materials Science,http://arxiv.org/abs/2512.22396v1,arXiv,2
"Anisotropic intensity distributions on nanostructured surfaces and polarization-sensitive spectra have been observed in a number of nanoscale infrared spectroscopic imaging methods, including nano-FTIR [Bakir et al., Molecules, 2020, 25, 4295], photothermal induced resonance (PTIR) [Waeytens et al., Analyst, 2021, 146], tapping AFM-IR [Hondl et al., ACS Meas. Sci. Au, 2025, 5, 469; Luo et al., APL, 2022, 121, 23330], infrared photoinduced force microscopy (PiF-IR) [Anindo et al., JPCC, 2025, 129, 4517; Shcherbakov et al., Rev Methods Primers, 2025, 5, 1; Ali et al., Anal. Chem., 2025, 97, 23914] and peak force infrared microscopy (PFIR) [Xie et al., JPCC, 2022, 126, 8393; Anindo, JPCC, 2025]. A recent work combining modeling and experiment demonstrated that the hybrid field coupling of the IR illumination E0 with a polymer nanosphere and a metallic AFM probe is nearly as strong as the plasmonic coupling in case of a gold nanosphere [Anindo, JPCC, 2025]. For p-polarized illumination, this results in enhanced IR absorption on the surface perpendicular to the propagation of E0 which can explain the observed anisotropic intensity distribution. An additional anisotropy may be introduced by aligned surface molecules with oriented vibrational transition moments [Bakir et al., Molecules, 2020, 25, 4295; Luo, APL, 2022]. PiF-IR is strongly surface sensitive combining an unprecedented spatial resolution < 5 nm with high spectral resolution [Shcherbakov, Rev Methods Primers, 2025; Ali, Anal. Chem., 2025], which allows, for example, to visualize nanoscale chemical variation on the surface of bacteria cells affected by antimicrobial interaction [Ali, Anal. Chem., 2025]. We compare PiF-IR hyperspectra of aligned perylene Langmuir Blodgett monolayers on nanostructured and planar gold substrates and use quantum chemical calculations of the oriented vibrational oscillators to interpret the observations.",Materials Science,http://arxiv.org/abs/2512.22373v1,arXiv,2
"This work explores how phonon perturbations can induce the breaking of electronic degeneracies near the Fermi level and how this response can be interpreted from a chemical perspective through the SSAdNDP method. We apply this approach to a family of structurally similar yet electronically distinct hexagonal materials-MgB2, graphene, and hBN-to analyze how a single phonon mode simultaneously modifies the electronic structure (band dispersion) and the nature of chemical bonding (natural occupations and nodal patterns) in real space. Our results show that band splitting becomes physically relevant only when it is accompanied by an electronic redistribution, reflected in changes of the occupation numbers or bonding topology. Thus, SSAdNDP provides a direct bridge between reciprocal- and real-space representations, translating phenomena such as electron-phonon coupling into chemically intuitive reorganizations of multicenter bonds, and offering a unified framework to interpret vibrationally driven electronic effects in solids.",Materials Science,http://arxiv.org/abs/2512.22372v1,arXiv,2
"The equilibrium vacancy concentration in multi-principal element alloys remains a controversial and nontrivial subject, primarily because of chemical complexity and chemical short-range order (CSRO). Here we derive an exact expression that is amenable to atomistic calculations, using multiple perspectives. We applied this expression to equiatomic CrCoNi alloys in the face-centered cubic structure. The derived equilibrium vacancy concentration is used in our recent work, which predicts the chemical short-range order formation timescale consistent with experimental observation. The results demonstrate the practical utility of the approach for predicting equilibrium vacancy concentrations in compositionally complex alloys.",Materials Science,http://arxiv.org/abs/2512.22370v1,arXiv,2
"The symmetric Heisenberg exchange interaction and antisymmetric Dzyaloshinskii-Moriya interaction are parts of the tensor potential describing effective spin-spin interaction caused by the superexchange interaction of magnetic ions via nonmagnetic ion. There is the Keffer form of the vector constant of the Dzyaloshinskii-Moriya interaction, which includes the shift of the nonmagnetic ion (ligand) from the line connecting two magnetic ions. It is suggested, in this paper, that the ligand shift can give contribution in the constant of the symmetric Heisenberg interaction in antiferromagnetic or ferrimagnetic materials. Hence, the constant of the Heisenberg interaction is composed minimum of two terms. One does not depend on the ligand shift an gives standard contribution in the energy density like term with no derivatives of the spin densities or term containing two spatial derivatives of the spin densities. It is demonstrated that additional term gives a term in the energy density containing one spatial derivative of the spin density. Corresponding contribution in the Landau--Lifshitz--Gilbert equation is found. Possibility of the noncollinear equilibrium order of spin under influence of new spin torque is discussed. Modification of the spin wave (normal modes) dispersion dependencies in the antiferromagnetic materials is found for the collinear order and for the cycloidal order of spins. Effective spin current is derived and applied for the spin-current model of the polarization origin in multiferroics.",Materials Science,http://arxiv.org/abs/2512.22108v1,arXiv,2
"Copper selenide is an exceptional quasi-layered monolithic material that exhibits both semiconducting and metallic properties in adjacent visible and near-infrared (NIR) spectral ranges. Here we introduce a thiol-free colloidal synthesis for generating quasi-2D klockmannite copper selenide nanocrystals via hot injection method, achieving shape control by tuning the injection temperature and precursor concentrations without any additional ligands. This approach produces large klockmannite nanosheets with lateral sizes from 200 nm to several micrometres, as well as uniform triangular nanoplatelets with sizes of 12-25 nm that are monocrystalline and display strong NIR plasmonic absorption. The spectral features of the anisotropic klockmannite phase in the NIR have been analysed using complex-scaled discrete dipole approximation (CSDDA) calculations, which reveal pronounced optical anisotropy and the emergence of hyperbolic regime. The combined effect of propagating and evanescent fields is regarded as the underlying reason of such modes in the hyperbolic domain. Finally, the ultrafast photophysical behaviour of the material in klockmannite phase is examined, including hot-hole cooling, trapping, and coherent phonons generation. Our findings emphasize the important role of the intrinsic crystal anisotropy in governing the physical properties of nanoscale klockmannite.",Materials Science,http://arxiv.org/abs/2512.22086v1,arXiv,2
"Understanding and controlling spin dynamics in two-dimensional (2D) van der Waals (vdW) ferromagnets is essential for their application in magnonics and hybrid quantum platforms. Here, we investigate the spin dynamics of the vdW ferromagnet 1T-CrTe_{2} and demonstrate their systematic tunability via niobium (Nb) substitution in Cr_{1-x}Nb_{x}Te_{2}(x=0-0.2). Ferromagnetic resonance (FMR) spectroscopy reveals that Nb doping enables wide-band tuning of the resonance frequency from 40 GHz down to the few-GHz regime, accompanied by a moderate increase in the Gilbert damping constant from ~0.066 to ~0.14, while preserving robust room-temperature ferromagnetism. Complementary magnetometry shows a concurrent reduction of the Curie temperature and saturation magnetization with increasing Nb content. Density functional theory calculations attribute the observed spin-dynamic trends to Nb-induced modifications of magnetic anisotropy and magnetic exchange interactions. Furthermore, CrTe_{2} flakes (~100nm thick) exhibit lower resonance frequencies than bulk crystals, consistent with thickness-dependent magnetic anisotropy. These results establish Nb-doped CrTe_{2} as a tunable vdW ferromagnet with controllable spin dynamics, extending its functionality from spintronics to broadband magnonics and quantum magnonics.",Materials Science,http://arxiv.org/abs/2512.22058v1,arXiv,2
"X-ray circular dichroism (XCD), defined as the difference in absorption or scattering intensity between X-rays of opposite polarizations, arises from the breaking of spatial inversion or time-reversal symmetry and is thus sensitive to chirality, magnetism, and their interplay. Non-reciprocal XCD, in which the dichroic response changes upon reversing the propagation direction of the probe, is generally forbidden in systems with both symmetries. Using resonant inelastic X-ray scattering, we identify circularly polarized phonons in ferro-rotational MnTiO$_3$, which we term ferro-rotational phonons. Their excitations provide a direct demonstration of non-reciprocal XCD in a system that globally preserves inversion and time-reversal symmetries. We propose that a condensate of these phonons, manifested as standing waves, underlies the ferro-rotational order in MnTiO$_3$. The interplay among photon helicity, phonon polarization, and the axial ferro-rotational order gives rise to the observed non-reciprocal circular dichroism.",Materials Science,http://arxiv.org/abs/2512.22050v1,arXiv,2
"Perovskite solar cells (PSCs) based on methylammonium lead iodide (MAPbI3) exhibit remarkable photovoltaic performance, where interface engineering with hole transport layers (HTLs) is crucial for optimizing charge transfer and device efficiency. In this work, we present a density functional theory (DFT) study of the MAPbI3/poly(3-hexylthiophene) (P3HT) hybrid interface, focusing on the role of perovskite surface termination in determining interfacial stability and electronic structure. We model MAI- and PbI-terminated MAPbI3 surfaces interfaced with P3HT and compare their interfacial electronic properties. Electronic structure calculations reveal distinct differences in orbital hybridization and band alignment: the MAI/m-P3HT interface exhibits weak coupling, whereas the PbI/m-P3HT interface shows stronger hybridization and enhanced charge transfer. Band alignment confirms type-II, hole-selective character in both cases, with more pronounced valence band maximum adjustment for PbI. Charge difference maps, Bader analysis, and local density of states consistently indicate higher charge transfer and stronger electronic coupling for PbI termination. Electrostatic potential offsets and transport parameters further highlight termination-dependent differences, with lighter effective masses at PbI/m-P3HT and higher hole velocity at MAI/m-P3HT. These findings provide theoretical insight into interfacial charge transport mechanisms and offer guidelines for optimizing perovskite-organic hybrid solar cells.",Materials Science,http://arxiv.org/abs/2512.22321v1,arXiv,2
"In this work, we investigate the relative importance of electronic and phononic energy dissipation during the molecular adsorption of CO on Cu(110). Initial sticking probabilities as a function of impact energy for CO impinging at normal incidence at a surface temperature of 90 K were computed using classical trajectory simulations. To this aim, we use a full-dimensional potential energy surface constructed using an atomistic neural network trained on density functional theory data obtained with the nonlocal vdW-DF2 exchange-correlation functional. Two models are compared: one allowing only energy transfer and dissipation from the molecule to lattice vibrations, and the other also incorporating the effect of molecular energy loss due to the excitation of electron-hole pairs, modeled within the local-density friction approximation. Our results reveal, firstly, that the molecule mainly transfers energy to lattice vibrations, and this channel determines the adsorption probabilities, with electronic friction playing a minor role. Secondly, once the molecule is trapped near the surface (where electronic density is higher), electron-hole pair excitations accelerate energy dissipation, significantly promoting CO thermalization. Still, the faster energy dissipation when electron-hole pair excitations are accounted for accelerates the accommodation of the adsorbed molecules in the chemisorption well but does not significantly alter their lateral displacements over the surface.",Materials Science,http://arxiv.org/abs/2512.21996v1,arXiv,2
"In this work we investigate the dissociation of CO$_2$ on Cu(110) by performing density functional theory calculations using the vdW-DF2 exchange-correlation functional, with a potential energy surface parameterized using artificial neural networks. We computed quasi-classical trajectory calculations of molecular and dissociative adsorption probabilities as a function of the initial impact energy of the molecules and surface temperature, by comparing our results with available supersonic molecular beam experimental data for normal incidence. Concerning the general dependence of the molecular and dissociative adsorption probabilities on the initial translational energy of the molecules, our theoretical results agree with experiments. Also in agreement with experiments, we have found that dissociative adsorption is not affected by surface temperature between 50 and 400 K, for impact energies for which the dissociation probability is larger than $\sim 10^{-3}$. We have investigated the influence of impact energy and surface temperature on the final state of the dissociation products by extending the time integration of the reactive trajectories up to 10 ps. We have found that above $\sim 2.5$ eV and close to or above room temperature, CO$_2$ dissociation induces strong surface distortions including final structures involving Cu adatoms. The creation of Cu vacancy-adatom pairs is stimulated by the presence of both CO$_{ads}$ and O$_{ads}$ which interact strongly with the Cu adatoms and even give rise to unexpected (O-Cu-CO)$_{ads}$ linear moieties anchored to the surface by the dissociated O atom and involving a Cu adatom almost detached from the surface. These surface distortions produced by dissociation products of high energy CO$_2$ molecules at and above room temperature might explain recent experiments that have found a greater saturation oxygen coverage for high energy molecules.",Materials Science,http://arxiv.org/abs/2512.21994v1,arXiv,2
"We present a comprehensive density functional theory (DFT) study of the electronic, magnetic, and topological properties of the layered pnictides EuMnXBi2 (X = Mn, Fe, Co, Zn), focusing in particular on the relatively unexplored Bi-based member of the EuMn2X2 family. Unlike the well-studied As-, Sb-, and P--based analogues, we show that EuMn2Bi2 stabilizes in a C-type antiferromagnetic ground state with a narrow-gap semiconducting character. Inclusion of spin-orbit coupling (SOC) drives a transition from this trivial antiferromagnetic semiconductor to a Weyl semimetal hosting four symmetry-related Weyl points and robust Fermi arc states. Systematic substitution of Mn with Fe, Co, and Zn further reveals a tunable sequence of magnetic ground states: Fe and Co induce ferrimagnetism with semimetallic behavior, while Zn stabilizes a ferromagnetic semimetal with a large net moment. These findings establish Bi-based EuMnXBi2 pnictides as a versatile platform where magnetic exchange interactions and band topology can be engineered through SOC and chemical substitution. The complex interplay of magnetic interactions and topological effects in the proposed bulk and doped pnictides opens a promising avenue to explore a wide range of electronic and magnetic phenomena. In particular, this study demonstrates that EuMn2Bi2 hosts tunable magnetic and topological phases driven by electron correlations, chemical substitution, and spin-orbit coupling.",Materials Science,http://arxiv.org/abs/2512.21978v1,arXiv,2
"The impact of projective lattice symmetry on electronic band structures has attracted significant attention in recent years, particularly in light of growing experimental studies of two-dimensional hexagonal materials in magnetic fields. Yet, most theoretical work to date has focused on the square lattice due to its relative simplicity. In this work, we investigate the role of projective lattice symmetry in a hexagonal lattice with rational magnetic flux, emphasizing the resulting topological constraints on the electronic band structure. We show that, at pi flux, the symmetry in the hexagonal lattice enforces novel Dirac band touchings at E not equal to zero, and for general rational flux it constrains the number of Dirac points at E = 0. We further analyze the symmetry-imposed constraints on the Chern numbers of both isolated gapped bands and band multiplets connected by Dirac-point touchings. Our results demonstrate that these constraints in the hexagonal lattice differ substantially from those in the square lattice.",Materials Science,http://arxiv.org/abs/2512.21966v1,arXiv,2
"Thin-film transistors based on amorphous oxide semiconductors (AOS) are promising candidates for enabling further DRAM scaling and 3D integration, which are critical for advanced computing. Despite extensive research, the charge transport mechanism in these disordered semiconductors remains poorly understood. In this work, we investigate charge transport in the archetypical AOS material, indium gallium zinc oxide (IGZO), across a range of compositions and temperatures using thin-film transistors and Hall bar structures. Our results show that the electrons involved in transport exhibit partially spatial coherence and non-degenerate conduction. Under these conditions, transport is dominated by electron transfer across insulating gaps between locally coherent regions, rather than by degenerate percolative transport above a mobility edge, or by localized-state hopping, both of which are widely assumed in the literature. While fluctuation-induced tunnelling has previously been invoked to describe low-temperature transport in oxide transistors, we show that such behavior originates from partially coherent electronic states and develop a field-effect-aware fluctuation-induced tunnelling (FEAFIT) framework that explicitly accounts for gate modulation of the tunneling landscape. The FEAFIT model accurately predicts experimental data across all compositions, temperatures, and gate voltages, enabling extraction of fundamental transport parameters. These tunnelling parameters are then correlated with electron coherence dimensions and the degree of energetic disorder obtained from first-principles calculations. Our findings advance the fundamental understanding of charge transport in AOS-based transistors and provide a foundation for further performance improvements",Materials Science,http://arxiv.org/abs/2512.21945v1,arXiv,2
"Increasing the valley splitting in Si-based heterostructures is critical for improving the performance of semiconductor qubits. This paper demonstrates that the two low-energy conduction band valleys are not independent parabolic bands. Instead, they originate from the X-point of the Brillouin zone, where they are interconnected by a degeneracy protected by the non-symmorphic symmetry of the diamond lattice. This semi-Dirac-node degeneracy gives rise to the $Î_1$ and $Î_{2'}$ bands, which constitute the valley degrees of freedom. By explicitly computing the two-component Bloch functions $X_1^\pm$, using the wave vector group at the X-point, we determine the transformation properties of the object $(X_1^+,X_1^-)$. We demonstrate that these properties are fundamentally different from those of a spinor. Consequently, we introduce the term ""valleyor"" to emphasize this fundamental distinction. The transformation properties of valleyors induce corresponding transformations of the Pauli matrices $Ï_1,Ï_2$ and $Ï_3$ in the valley space. Determining these transformations allows us to classify possible external perturbations that couple to each valley Pauli matrix, thereby identifying candidates for valley-magnetic fields, ${\mathsf B}$. These fields are defined by a Zeeman-like coupling ${\mathsf B}\cdot\vecÏ$ to the valley degree of freedom. In this way, we identify scenarios where an applied magnetic field $\vec B$ can leverage other background fields, such as strain, to generate a valley-magnetic field ${\mathsf B}$. This analysis suggests that beyond the well-known mechanism of potential scattering from Ge impurities, there exist additional channels (mediated by combinations of magnetic and strain-induced vector potentials) to control the valley degree of freedom",Materials Science,http://arxiv.org/abs/2512.21930v1,arXiv,2
"Ferroaxial order is characterized by the breaking of mirror symmetry parallel to the crystallographic principal axis, which often originates from spontaneous rotational distortions of the crystal lattice. Such rotational distortions are, by symmetry, allowed to couple to specific phonon modes. However, Raman-active phonons associated with these rotational distortions have not yet been clearly identified on a symmetry-consistent basis. Here, we perform polarization-resolved Raman spectroscopy on the ferroaxial phase of Na2BaNi(PO4)2 single crystals and combine the measurements with first-principles lattice-dynamics calculations. This symmetry-guided analysis enables a comprehensive assignment of Raman-active modes in the ferroaxial phase. Several low-frequency Ag modes exhibit finite linewidth broadening, suggesting that these phonons may be weakly affected by the underlying rotational distortion. These results establish a symmetry-based spectroscopic framework for analyzing phonons associated with rotational distortions in ferroaxial materials and provide a basis for future studies of ferroaxial order in complex oxides.",Materials Science,http://arxiv.org/abs/2512.21876v1,arXiv,2
"In metal-insulator transition materials, a small perturbation can shift the delicate balance between competing or coexisting electronic phases, leading to dramatic changes of the material's properties. Using La0.7Sr0.3MnO3, a prototypical metal-insulator transition manganite, we show that local low-dose focused ion beam irradiation increases resistivity by several orders of magnitude, converting the ferromagnetic-metal ground state into a paramagnetic-insulator. Surprisingly, we found that applying electric stimuli to the irradiated material induces a non-thermal insulator-to-metal transition, which results in low-power, repeatable volatile resistive switching. Magnetotransport measurements revealed that this voltage-induced metallic phase in the irradiated material is ferromagnetic, exhibiting clear anisotropic magnetoresistance. This work, thus, reports the discovery of a unique material in which the electrical triggering of the electronic phase transition results in the onset of magnetism, in stark contrast to the magnetic order suppression commonly observed in other metal-insulator transition switching materials. We demonstrate that local focused ion beam irradiation provides new and exciting opportunities to engineer electronic and magnetic functionalities that can find practical applications ranging from spintronics to neuromorphic hardware.",Materials Science,http://arxiv.org/abs/2512.21875v1,arXiv,2
"A variety of distinct anisotropic exchange interactions commonly exist in one magnetic material due to complex crystal, magnetic and orbital symmetries. Here we investigate the effects of multiple anisotropic exchange interactions on topological magnon in a honeycomb ferromagnet, and find a chirality-selective topological magnon phase transition induced by a complicated interplay of Dzyaloshinsky-Moriya interaction (DMI) and pseudo-dipolar interaction (PDI), accompanied by the bulk gap close and reopen with chiral inversion. Moreover, this novel topological phase transition involves band inversion at high symmetry points $K$ and $K'$, which can be regarded as a pseudo-orbital reversal, i.e. magnon valley degree of freedom, implying a new manipulation corresponding to a sign change of the magnon thermal Hall conductivity. Indeed, it can be realized in 4$d$ or 5$d$ correlated materials with both spin-orbit coupling and orbital localized states, such as iridates and ruthenates, etc. This novel regulation may have potential applications on magnon devices and topological magnonics.",Materials Science,http://arxiv.org/abs/2512.21850v1,arXiv,2
"High-quality metallocene single crystals with a low density of impurities and high homogeneity were prepared using the physical vapor transport method. These crystals were then characterized using various spectroscopic tools and X-ray diffraction. Laser-induced breakdown spectroscopy confirmed the presence of metal ions in each freshly grown sample despite all these crystals undergoing physical deformation with different lifetimes. X-ray diffraction analysis confirmed that all our metallocene single crystals retained a monoclinic structure at room temperature. The vibrational properties of our metallocene crystals were examined using Raman and Fourier-transform infrared spectroscopy. The inter- and intra-ring vibrational modes, along with additional modes associated with the crystalline form, were identified as inherent vibrational properties of our metallocene single crystals. Given the increasingly important role of metallocene in organic solar cells, organic light-emitting displays and molecular quantum systems, this research will enhance our understanding of the intrinsic physical properties of cleaner, more crystalline metallocene single crystals.",Materials Science,http://arxiv.org/abs/2512.21836v1,arXiv,2
"van der Waals (vdW) epitaxy is conventionally regarded as a rotation-free and strain-free growth mode driven by weak, isotropic interactions, yet many interfaces paradoxically exhibit strictly locked orientations that defy standard surface-energy models. We resolve this inconsistency by establishing a unified quantitative framework for 2D-3D systems, in which strong electrostatic and chemical interactions compete with entropic forces. We introduce a two-tier descriptor set-the predictive index (I_pre) and the thermodynamic locking criterion (I_lock)-to quantify the energetic sufficiency for locked epitaxy. Our theory accurately predicted the competitive interactions at the interface within the 2D-3D system, precisely characterized whether the epitaxial layer underwent free growth or was constrained in a locked growth mode, demonstrating robust consistency with diverse experimental observations. This framework unifies orientation selection in 3D-on-2D films and rotational locking in 2D-on-3D layers within a single-phase diagram. Our work provides a generalizable, predictive route to controlling epitaxial orientation across a broad spectrum of layered heterostructure",Materials Science,http://arxiv.org/abs/2512.21833v1,arXiv,2
"We tackle the challenge of predicting vibrational stability in inorganic semiconductors for high-throughput screening, an essential attribute for evaluating synthesizability alongside thermodynamic stability, frequently missing in prominent materials databases. We create a physics-informed neural network (PINN) that incorporates the Born stability requirements directly into its loss function. This integration is a key learning constraint since it only allows the model to make predictions that do not violate fundamental physics. The model shows consistent and improved performance, having been trained on a dataset of 2112 inorganic materials with validated phonon spectra, and getting an F1-score of 0.83 for both stable and unstable classes. The model shows an AUC-ROC of 0.82 on a benchmark dataset of 1296 materials. Our PINN surpasses the best models in comparative tests, especially when it comes to accurately identifying unstable materials, which is crucial for a stability filter. This work offers a comprehensive screening tool for identifying materials and a methodology for incorporating domain knowledge to enhance predictive accuracy in materials informatics.",Materials Science,http://arxiv.org/abs/2512.21830v1,arXiv,2
"We present a graph neural network (GNN) based surrogate framework for molecular dynamics simulations that directly predicts atomic displacements and learns the underlying evolution operator of an atomistic system. Unlike conventional molecular dynamics, which relies on repeated force evaluations and numerical time integration, the proposed surrogate model propagates atomic configurations forward in time without explicit force computation. The approach represents atomic environments as graphs and combines message-passing layers with attention mechanisms to capture local coordination and many-body interactions in metallic systems. Trained on classical molecular dynamics trajectories of bulk aluminum, the surrogate achieves sub angstrom level accuracy within the training horizon and exhibits stable behavior during short- to mid-horizon temporal extrapolation. Structural and dynamical fidelity are validated through agreement with reference radial distribution functions and mean squared displacement trends, demonstrating that the model preserves key physical signatures beyond pointwise coordinate accuracy. These results establish GNN-based surrogate integrators as a promising and computationally efficient complement to traditional molecular dynamics for accelerated atomistic simulations within a validated regime.",Materials Science,http://arxiv.org/abs/2512.21822v1,arXiv,2
"Understanding lattice dynamics and structural transitions in vacancy-ordered double perovskites is crucial for developing lead-free optoelectronic materials, yet the role of dopants in modulating these properties remains poorly understood. We investigate Sb-doped Cs$_2$TiCl$_6$ through temperature-dependent Raman spectroscopy (4 to 273 K), high-pressure studies (0 to 30 GPa), powder XRD, and photoluminescence measurements. Sb doping dramatically improves phase purity, eliminating all impurity-related Raman modes present in pristine and Bi-doped samples while retaining only the three fundamental [TiCl$_6$]$^{2-}$ octahedral vibrations. This enhanced purity reveals a previously unobserved structural phenomenon: Sb-doped samples (2\% doped and 3\%) incorporated) exhibit a sharp anomaly at 100 K marked by the emergence of a new Raman mode M$_1$ at 314--319 cm$^{-1}$ and abrupt changes in the temperature coefficient $Ï$ (factor of 2--8$\times$ change) and anharmonic constant $A$ across this threshold. No such transition occurs in pristine Cs$_2$TiCl$_6$, indicating Sb-dopant-induced order-disorder transformation. The enhanced phonon anharmonicity in Sb-doped samples directly manifests in photoluminescence: self-trapped exciton emission at 448 nm shows 19\% broader FWHM (164.73 nm) compared to Bi-doped samples (138.2 nm), confirming stronger electron-phonon coupling. High-pressure measurements reveal structural robustness to 30 GPa with no phase transitions. These findings establish that strategic Sb doping not only improves material quality but also enables a novel low-temperature structural transition, providing fundamental insights into dopant-mediated phase control in vacancy-ordered perovskites for next-generation optoelectronic devices.",Materials Science,http://arxiv.org/abs/2512.21810v1,arXiv,2
"After the synthesis of the carbon biphenylene network (C-BPN), research has increasingly focused on adapting elements from other groups of the periodic table to this lattice structure. In this study, the direction-dependent electronic, thermal, and thermoelectric (TE) properties of semiconducting group-III (group-III = B, Al, Ga, In) nitride biphenylene networks are investigated using the non-equilibrium Green's function formalism in combination with first-principles calculations. Phonon spectra and force field molecular dynamics (MD) simulations were used to asses the dynamically and thermally stable structures. At room temperature, the lowest phonon thermal conductance values are obtained for InN-BPN, with $Îº_{\mathrm{ph}}$ = 0.12 nW/K/nm and $Îº_{\mathrm{ph}}$ = 0.21 nW/K/nm along the armchair and zigzag directions, respectively. The nearly dispersionless valence-band region between the $Î$--$X$ symmetry points causes a sharp increase in the $p$-type electronic transmission, which significantly enhances the $p$-type thermoelectric figure of merit, $zT$. Among the investigated group-III nitride BPNs, InN-BPN exhibits the best performance, with a $p$-type $zT$ value of 2.33 in the zigzag direction at 800 K.",Materials Science,http://arxiv.org/abs/2512.21784v1,arXiv,2
"There has been unprecedented interest in developing agents that expand the boundary of scientific discovery, primarily by optimizing quantitative objective functions specified by scientists. However, for grand challenges in science , these objectives are only imperfect proxies. We argue that automating objective function design is a central, yet unmet requirement for scientific discovery agents. In this work, we introduce the Scientific Autonomous Goal-evolving Agent (SAGA) to amend this challenge. SAGA employs a bi-level architecture in which an outer loop of LLM agents analyzes optimization outcomes, proposes new objectives, and converts them into computable scoring functions, while an inner loop performs solution optimization under the current objectives. This bi-level design enables systematic exploration of the space of objectives and their trade-offs, rather than treating them as fixed inputs. We demonstrate the framework through a broad spectrum of applications, including antibiotic design, inorganic materials design, functional DNA sequence design, and chemical process design, showing that automating objective formulation can substantially improve the effectiveness of scientific discovery agents.",Materials Science,http://arxiv.org/abs/2512.21782v1,arXiv,2
"Complex behaviors often sit at a critical threshold between order and disorder. But not all disorder is created equal. Disorder can be trivial or constrained, and correlated disorder can even be topological. Crucially, constrained disorder can harbor memory, leading to non-trivial, sequence-dependent responses to external manipulations. And yet the fascinating subject of ""memory of disorder"" remains poorly explored, as memory is often associated to the retention of metastable order. In recent years artificial frustrated materials -- in particular arrays of frustrated nanomagnets known as artificial spin ices -- have been employed to study complex disorders and its wealth of exotic behaviors, yet their memory properties have received much less attention. Here, we investigate both analytically and numerically the sequence-dependent responses of two somehow opposite yet related artificial spin ices: the Landau-ordered square spin ice and the disordered but topologically-ordered Shakti spin ice. We find that Shakti exhibits a pronounced sequence-dependent response, whereas in the square lattice, such path dependence is absent. Within Shakti, even the minimal periodic supercell demonstrates both deterministic and stochastic forms of sequence memory, depending on the interaction strength. Extending our study to cyclic driving, we find that retracing the same input path leads to enhanced memory retention. These results open new perspectives on how topological constraints and correlated disorder generate robust memory effects in frustrated artificial materials, hitherto examined mainly in terms of their ground-state kinetics and thermodynamics.",Materials Science,http://arxiv.org/abs/2512.21767v1,arXiv,2
"Ni-W based medium heavy alloys offer a promising pathway to bridge the density-strength gap between tungsten heavy alloys and ultrahigh-strength steels. In this study, the effects of W concentration on short-range order (SRO), deformation behavior, and grain boundary chemistry of Ni-xW alloys in the range x = 0 to 38 wt% were systematically investigated using a suite of advanced characterization and modeling techniques, including synchrotron X-ray diffraction, transmission electron microscopy, atom probe tomography, and first-principles thermodynamic simulations. Our study reveals that strong SRO emerges when W content exceeds about 30 wt%, producing distinct diffuse scattering and significantly enhancing strain-hardening capacity. During deformation, the presence of SRO promotes planar slip and twin formation, leading to strong dislocation interactions and elevated flow stress. Hall-Petch analysis demonstrates an exceptionally high grain boundary strengthening coefficient (ky about 1100 MPa micrometer^(1/2)) in Ni-38W, underscoring the intrinsic strengthening effect associated with SRO. First-principles cluster expansion coupled with Monte Carlo simulations reveals that increasing W content enhances SRO tendency through the stabilization of Ni4W-type local configurations. These findings establish a mechanistic link between W concentration, SRO evolution, and mechanical response, providing new insights for designing high-density, high-strength Ni-W based alloys with optimized performance.",Materials Science,http://arxiv.org/abs/2512.21745v1,arXiv,2
"We present a combined experimental and theoretical study of the triangular-lattice quantum antiferromagnet CsFeCl$_3$ under high magnetic fields and high pressure. Pulsed-field magnetization for the magnetic field along the symmetric $c$ direction at ambient pressure reveals a magnetization process from a nonmagnetic singlet ground state with a nearly linear increase between 3.7 and 10.7 T, a plateau-like region, and then a sharp stepwise metamagnetic transition near 32 T. Wide frequency--field range electron spin resonance indicates that the low-field regime originates from the $J = 1$ manifold, while the high-field metamagnetic transition suggests a level crossing between the $J = 1$ and $J = 2$ lowest states. Pulsed-field magnetic susceptibilities measured with a proximity detector oscillator under high pressure show that the low-field nonmagnetic singlet phase is gradually suppressed, while the high-field metamagnetic transition evolves into an increasingly rich pattern of fractional steps. While the observations at low to intermediate fields can be understood within the established spin-1 description, the high-field regime requires a new perspective, which we provide through a projected spin-1/2 framework built from Zeeman-selected crystal-field states not related by time reversal. This construction naturally allows emergent three-body interactions on triangular plaquettes and explains the asymmetric evolution of the fractional steps in the magnetization. Our findings reveal that high-field effective spin models in quantum magnets with separated yet accessible crystal-field multiplets are not constrained to even-body couplings, but can naturally host odd-body terms, opening a broader avenue for realizing field-asymmetric magnetization processes and exotic phases beyond conventional even-body physics.",Materials Science,http://arxiv.org/abs/2512.21682v1,arXiv,2
"MnBi2Te4 is a versatile platform for exploring diverse topological quantum states, yet its potential is hampered by intrinsic antisite defects. While Sb substitution has been employed to tune the Fermi level towards the charge neutral point, it exacerbates the formation of Mn-Sb antisite defects. Here, we address this challenge by combining first-principles calculations with strategic synthesis to systematically investigate and control antisite defects in Mn(Bi1-xSbx)2Te4. Our calculations reveal that increasing antisite defect density progressively destroys the field-forced magnetic Weyl state, eventually driving the system into a trivial magnetic insulator. Motivated by these findings, we develop an optimized chemical vapor transport method, yielding high-quality Mn(Bi1-xSbx)2Te4 crystals with significantly reduced antisite defect density. The emergence of strong Shubnikov-de Haas oscillations in the forced ferromagnetic state and a pronounced anomalous Hall effect near charge neutrality, with opposite signs for n- and p-type samples, confirms the type-II Weyl semimetal nature. These findings underscore the critical role of antisite defects in determining the magnetic and topological properties of Mn(Bi1-xSbx)2Te4 and establish defect engineering via optimized synthesis as a crucial strategy for realizing its exotic magnetic topological states.",Materials Science,http://arxiv.org/abs/2512.21680v1,arXiv,2
"Given the scarcity of experimentally confirmed magnetic structures, the reliable prediction of magnetic ground states is crucial; however, it remains a long-sought challenge because of the complex magnetic potential energy landscape. Here, we propose a symmetry-guided framework that systematically generates realistic magnetic configurations without requiring any experimental input or prior assumptions such as propagation vectors. Within a hierarchical symmetry-breaking scenario, we integrate the recently developed spin space group formalism and conventional magnetic space group description, respectively capturing symmetry breaking induced by magnetic ordering and spin-orbit coupling. Furthermore, we perform both nonrelativistic and relativistic first-principles calculations to establish the energy ordering of selected magnetic configurations. Exemplified by three recently reported three-dimensional unconventional magnets MnTe, Mn3Sn, and CoNb3S6 and two two-dimensional magnets CrTe2 and NiI2, we demonstrate that only a few dozen first-principles calculations are sufficient to identify the ground-state magnetic configuration along with several low-energy metastable states, which may exhibit exotic physical properties such as p-wave magnetism. Our work provides a general and efficient strategy for large-scale prediction of three-dimensional and two-dimensional magnetic configurations and offers insight into the microscopic origins of magnetic interactions across diverse material systems.",Materials Science,http://arxiv.org/abs/2512.21672v1,arXiv,2
"We present a first-principles and quantum transport study of proximity-induced spin-orbit torque (SOT) in graphene on a trigonal CrSBr monolayer. Density functional theory combined with nonequilibrium Green's function calculations shows that the CrSBr substrate induces spin polarization and a sizable exchange splitting in the graphene Dirac states. The resulting current-driven spin density in graphene generates a self-SOT on the Dirac electrons. The proximity-induced exchange field breaks time-reversal symmetry and gives rise to a purely odd SOT component, while the even contribution vanishes. The torque magnitude exhibits a strong angular dependence with phase shifts arising from the noncollinearity between the CrSBr magnetization and the induced magnetic moments in graphene. Monte Carlo simulations based on the calculated exchange parameters predict a Curie temperature of approximately 304 K, confirming the robustness of ferromagnetism in the trigonal CrSBr monolayer. These results identify graphene/CrSBr heterostructures as a promising platform for room-temperature two-dimensional spintronics.",Materials Science,http://arxiv.org/abs/2512.21636v1,arXiv,2
"Organic Magnetoresistance is defined as the change of resistance in an organic material, such as a conducting polymer, as a function of an imposed magnetic field. We demonstrate this effect in a Polypyrrole/ Polydimethylsiloxane complex by using a novel magnetic pulse system. The frequency spectrum of the current flowing through the sample reveals equally spaced reversed peaks in the current. We show that these peaks happen at the Lamour frequency for the dominant charge carrier of the system, namely polarons. This posits the origin Organic magnetoresistance as simple Rabi Oscillations rather than mechanisms based of bipolaron formation and singlet-triplet conversions. We directly estimate the effective polaronic mass in this complex. A semi classical theoretical approach is suggested to explain this effect as direct spin flipping in a time transient magnetic field. This is the first time that such an experimental approach has been applied in this field and the first time the effective polaronic mass has been measured in a conducting polymer.",Materials Science,http://arxiv.org/abs/2512.21629v1,arXiv,2
"Nonvolatile control of the Mott transition is a central goal in correlated-electron physics, offering access to fascinating emergent states and great potential for technological applications. Compared to chemical or mechanical approaches, ultrafast optical excitation further promises a path to create and manipulate novel non-equilibrium phases with ultimate spatiotemporal precision. However, achieving a truly nonvolatile electronic phase transition in laser-excited Mott systems remains an elusive challenge. Here, we present a highly robust and reversible method for optical control of the Mott state in van der Waals systems. Specifically, using angle-resolved photoemission spectroscopy, we observe a nonvolatile Mott-to-metallic transition in the ultrafast laser-excited charge density wave (CDW) material 1T-TaSe2. Complementary theoretical calculations reveal that this transition originates from a rearrangement of the interlayer CDW stacking. This new stacking order, formed following the ultrafast quenching of the CDW, circumvents the need for large-scale atomic sliding. Intriguingly, it introduces a significant in-plane component to the electron hopping and effectively reduces the ratio of on-site Coulomb interaction to bandwidth, thereby suppressing the Mott state and stabilizing a metallic phase. Our results establish optical-control of interlayer stacking as a versatile strategy for inducing nonvolatile phase transitions, opening a new route to tailor correlated electronic phases and realize reconfigurable high-frequency devices.",Materials Science,http://arxiv.org/abs/2512.21628v1,arXiv,2
"Within the framework of the local electron density functional theory, an ab-initio method is proposed that takes into account the self-interaction energy correction (SIC) for the crystal potential. The principle of dividing the unit cell into regions remained the same as for the ground-state potential. The expression for the self-consistent muffin-tin-SIC potential satisfies the condition of cell electroneutrality. This scheme was applied to calculate the spectrum of the fundamental absorption edge of LiCl. The obtained interband transition energies were not required for fitting to experimental values. In the calculated spectrum, the shape and position of the peaks above ~2.5 eV from the edge agree fairly well with the measured ones, which allows their intensity in this region to be attributed primarily to interband transitions. Closer to the edge, electron-hole interaction significantly alters the ground state, which must be taken into account when calculating the spectrum shape.",Materials Science,http://arxiv.org/abs/2512.21581v1,arXiv,2
"Chiral magnetic states give rise to rich phenomena, from the anomalous Hall effect and the nonlinear electrical current to multiferroics and magnetochiral dichroism. Most of the studies on electrical transport so far have focused on the cases where the magnetic moments are well approximated by classical local moments. Here, we reveal that the coexistence of quantum fluctuations and chiral spin correlations gives rise to a $\log(T)$ temperature dependence in the electrical magnetochiral effect, a nonreciprocal response. Using the Green's function method and a scattering theory approach, we show that the $\log(T)$ temperature dependence occurs through a scattering process similar to that of the Kondo effect. The electrical magnetochiral effect is sensitive to the sign of vector spin chirality and the magnetic field. The results demonstrate that local spin correlations and quantum fluctuations cooperatively induce nontrivial properties in transport phenomena.",Materials Science,http://arxiv.org/abs/2512.21575v1,arXiv,2
"We propose a fractionally quantized polarization induced by interlayer sliding in bilayer altermagnets, unveiling a previously unrecognized multiferroic phase termed sliding fractional quantum multiferroicity (SFQM). This unconventional magnetic phase uniquely integrates sliding ferroelectricity with fractional quantum ferroelectricity, enabling highly efficient switching and nonvolatile electrical control of spin.~Unlike conventional multiferroics, SFQM simultaneously exhibits lattice-scale atomic displacements, ultralow switching barriers, and spin splitting, giving rise to a large fractionally quantized polarization and strong magnetoelectric coupling. Through symmetry analysis and first-principles calculations, we identify bilayer altermagnet Ca(CoN)$_2$ and its family materials as promising candidates hosting SFQM. In contrast to gate-controlled schemes, the spin-layer coupling in SFQM is intrinsically induced by spontaneous electrical and layer polarization, requiring no sustained gate field and exhibiting nonvolatile character. This mechanism enables nonvolatile electrical control of spin through biaxial sliding, where displacements along the \textit{x}- and \textit{y}-axes generate opposite polarization directions in the layer-dependent electrical polarization. Furthermore, SFQM exhibits a fully switchable anomalous Hall effect and a pronounced magneto-optical response, which can be utilized for its detection and distinction. These findings highlight the promising role of sliding-mediated couplings among unconventional magnetism, fractional quantum ferroelectricity, and stacking order in realizing electrically controllable two-dimensional multiferroics.",Materials Science,http://arxiv.org/abs/2512.21559v1,arXiv,2
"In this paper, a novel kinematic framework for fiber-reinforced composite materials is presented. For this purpose, we use the multiple natural configurations in conjunction with the multi-continuum theory of Bedford and Stern~(1972). Keeping the underlying physics of the proposed kinematics consistent. The proposed kinematics results in a three-term decomposition of the deformation gradient i.e. $\mathbf{F}=\mathbf{F}^e\mathbf{F}^r_Î±\mathbf{F}^d_Î±$, where $Î±$ represents either the matrix or the fiber. After discussing the kinematic framework in detail, we use this new kinematic framework to characterize the damage contents associated with four damage mechanisms. These damage mechanisms are matrix cracking, fiber breakage, interfacial slip or debonding, and delamination. While the first two are derived by measuring the incompatibility of the pertinent configuration occupied by individual constituents, the latter two involve a relative displacement between either the constituents or the laminÃ¦. The geometric interpretation corresponding to these damage mechanisms is also presented using tools from differential geometry. The derived damage contents can be used in developing an appropriate constitutive model for laminated composites undergoing damage.",Materials Science,http://arxiv.org/abs/2512.22285v1,arXiv,2
"This study presents a first principles investigation of the structural, thermodynamic, electronic, optical and thermoelectric properties of aluminum antimonide (AlSb) in its cubic (F-43m) and hexagonal (P63mc) phases. Both structures are dynamically and mechanically stable, as confirmed by phonon calculations and the Born Huang criteria. The lattice constants obtained using the SCAN and PBEsol functionals show good agreement with experimental data. The cubic phase exhibits a direct band gap of 1.66 to 1.78 eV, while the hexagonal phase shows a band gap of 1.48 to 1.59 eV, as confirmed by mBJ and HSE06 calculations. Under external pressure, the band gap decreases in the cubic phase and increases in the hexagonal phase due to different s p orbital hybridization mechanisms. The optical absorption coefficient reaches 1e6 cm-1, which is comparable to or higher than values reported for other III V semiconductors. The Seebeck coefficient exceeds 1500 microV per K under intrinsic conditions, and the thermoelectric performance improves above 600 K due to enhanced phonon scattering and lattice anharmonicity. The calculated formation energies (-1.316 eV for F-43m and -1.258 eV for P63mc) confirm that the cubic phase is thermodynamically more stable. The hexagonal phase exhibits higher anisotropy and lower lattice stiffness, which is favorable for thermoelectric applications. These results demonstrate the strong interplay between crystal symmetry, phonon behavior and charge transport, and provide useful guidance for the design of AlSb based materials for optoelectronic and energy conversion technologies.",Materials Science,http://arxiv.org/abs/2512.22277v1,arXiv,2
"Carbon exhibits both a layered ground state structure that produces two-dimensional (2D) nanosheets and a non-layered diamond structure created under high pressure conditions. Motivated by this metastability relationship, we revisit the ground state structure of metal dichalcogenides that are known to have non-layered pyrite-type structure. Ultrathin films of pyrite-type ZnSe$_2$ spontaneously transform into a layered phase. This phase is identified as a ground state, and the monolayer exhibits strong elastic anisotropy and a semiconducting bandgap larger than that of the pyrite phase by a factor of two. We demonstrate that a two-valued but directional potential energy surface exists along a Bain-like distortion path, hiding the layered ground state. This work implies that many 2D materials are hidden in non-layered materials and connects 2D materials science with surface and high-pressure science.",Materials Science,http://arxiv.org/abs/2512.21474v1,arXiv,2
"Reducing the formation temperature of single-phase multioxides is one of the classic challenges in ceramic processing, including wet-chemical synthesis routes. Toward pursuing this aim for diopside (MgCaSi2O6), the merit of different sol-gel and coprecipitation processes using the related chloride precursors followed by calcination was compared from the viewpoints of crystallinity and homogeneity. In accordance to the results, the use of the sol-gel techniques, directed with/without an alkaline catalyst, gave rise to the unfavorable creation of multiphase and low-crystallinity structures. Regarding the coprecipitation methods, the one-step addition of a precipitant agent is accompanied by an indirect low-temperature formation of nano-diopside, while a direct crystallization into this phase was explored in the dropwise condition, albeit with a lower crystallinity. Thus, by employing a suitable synthesis processing, it is feasible to take control of a wide range of nanoparticulate diopside-based structures achieved after a low-temperature calcination.",Materials Science,http://arxiv.org/abs/2512.22269v1,arXiv,2
"As we move beyond the era of transistor miniaturization, back-end-of-line-compatible transistors that can be stacked monolithically in the third dimension promise improved performance for low-power electronics. In advanced transistor architectures, such as gate-all-around nanosheets, the conventional channel-first process involves depositing dielectrics directly onto the channel. Atomic layer deposition of gate dielectrics on back-end-of-line compatible channel materials, such as amorphous oxide semiconductors, can induce defects or cause structural modifications that degrade electrical performance. While post-deposition annealing can partially repair this damage, it often degrades other device metrics. We report a novel channel-last concept that prevents such damage. Channel-last gate-all-around self-aligned transistors with amorphous oxide-semiconductor channels exhibit high on-state current ($>$ 1 mA/$Î¼$m) and low subthreshold swing (minimum of 63 mV/dec) without the need for post-deposition processing. This approach offers a general, scalable pathway for transistors with atomic layer deposited channel materials, enabling the future of low-power three-dimensional electronics.",Materials Science,http://arxiv.org/abs/2512.21330v1,arXiv,2
"We investigate the representations of the symmetry groups of infinite crystals. Crystal symmetries are usually described as the finite symmetry group of a finite crystal with periodic boundary conditions, for which the Brillouin zone is a finite set of points. However, to deal with the continuous crystal momentum $\mathbf{k}$ required to discuss the continuity, singularity or analyticity of band energies $Îµ_n(\mathbf{k})$ and Bloch states $Ï_{\mathbf{k}}$, we need to consider infinite crystals. The symmetry groups of infinite crystals belong to the category of infinite non-compact groups, for which many standard tools of group theory break down. For example, character theory is no longer available for these groups and we use harmonic analysis to build the group algebra, the regular representation, the induction of irreducible representations of the crystallographic group from projective representations of the point groups and the decomposition of a representation into its irreducible parts. We deal with magnetic and non-magnetic groups in arbitrary dimensions. In the last part of the paper, we discuss Mackey's restriction of an induced representation to a subgroup, the tensor product of induced representations and the symmetric and antisymmetric squares of induced representations.",Materials Science,http://arxiv.org/abs/2512.22265v1,arXiv,2
"Black phosphorus and its single-layer constituent, phosphorene, have emerged as promising two-dimensional materials with remarkable tribological properties. However, recent experimental investigations revealed that the their lubricating capabilities can change with the substrate. The present computational study employs density functional theory calculations to quantify the adhesion energy of both pristine and oxidized phosphorene monolayers on various metallic substrates (aluminum, copper, iron, and chromium) and their corresponding oxides ($\mathrm{Al_2O_3}$, $\mathrm{Cu_2O}$, $\mathrm{Fe_2O_3}$, and $\mathrm{Cr_2O_3}$), correlating these interfacial property with experimentally observed tribological performance. Results demonstrate that oxidized phosphorene presents higher adhesion to all substrates with respect to pristine phosphorene, attributed to favorable interactions between oxygen non-bonding states and substrate empty states. Adhesion is systematically more favorable on pristine metals than on their corresponding oxides, with chromium and iron showing particularly strong interactions due to partially filled 3d orbitals. This result is consistent with the coefficient of friction decrease observed in tribological experiments after scratching the iron substrate, thus removing the outermost oxide layer. Charge redistribution correlates with the adhesion and electronic structure analyses reveal system-dependent interfacial bonding characteristics, with some configurations inducing metallic character in phosphorene. These findings provide fundamental insights into substrate-dependent lubricating properties of black phosphorus, highlighting the key role of layer-substrate adhesion.",Materials Science,http://arxiv.org/abs/2512.21265v2,arXiv,2
"Large Language Models can develop reasoning capabilities through online fine-tuning with rule-based rewards. However, recent studies reveal a critical constraint: reinforcement learning succeeds only when the base model already assigns non-negligible probability to correct answers -- a property we term 'latent solvability'. This work investigates the emergence of chemical reasoning capabilities and what these prerequisites mean for chemistry. We identify two necessary conditions for RL-based chemical reasoning: 1) Symbolic competence, and 2) Latent chemical knowledge. We propose mid-stage scientific training (MiST): a set of mid-stage training techniques to satisfy these, including data-mixing with SMILES/CIF-aware pre-processing, continued pre-training on 2.9B tokens, and supervised fine-tuning on 1B tokens. These steps raise the latent-solvability score on 3B and 7B models by up to 1.8x, and enable RL to lift top-1 accuracy from 10.9 to 63.9% on organic reaction naming, and from 40.6 to 67.4% on inorganic material generation. Similar results are observed for other challenging chemical tasks, while producing interpretable reasoning traces. Our results define clear prerequisites for chemical reasoning training and highlight the broader role of mid-stage training in unlocking reasoning capabilities.",Materials Science,http://arxiv.org/abs/2512.21231v1,arXiv,2
"In this work, we introduce PhononBench, the first large-scale benchmark for dynamical stability in AI-generated crystals. Leveraging the recently developed MatterSim interatomic potential, which achieves DFT-level accuracy in phonon predictions across more than 10,000 materials, PhononBench enables efficient large-scale phonon calculations and dynamical-stability analysis for 108,843 crystal structures generated by six leading crystal generation models. PhononBench reveals a widespread limitation of current generative models in ensuring dynamical stability: the average dynamical-stability rate across all generated structures is only 25.83%, with the top-performing model, MatterGen, reaching just 41.0%. Further case studies show that in property-targeted generation-illustrated here by band-gap conditioning with MatterGen--the dynamical-stability rate remains as low as 23.5% even at the optimal band-gap condition of 0.5 eV. In space-group-controlled generation, higher-symmetry crystals exhibit better stability (e.g., cubic systems achieve rates up to 49.2%), yet the average stability across all controlled generations is still only 34.4%. An important additional outcome of this study is the identification of 28,119 crystal structures that are phonon-stable across the entire Brillouin zone, providing a substantial pool of reliable candidates for future materials exploration. By establishing the first large-scale dynamical-stability benchmark, this work systematically highlights the current limitations of crystal generation models and offers essential evaluation criteria and guidance for their future development toward the design and discovery of physically viable materials. All model-generated crystal structures, phonon calculation results, and the high-throughput evaluation workflows developed in PhononBench will be openly released at https://github.com/xqh19970407/PhononBench",Materials Science,http://arxiv.org/abs/2512.21227v2,arXiv,2
"The review treats Heusler alloys that display distinctive functional properties, including shape-memory behavior and magnetocaloric effects. Particular emphasis is placed on Heusler systems in which half-metallic ferromagnetism and spin-gapless semiconductor state are realized. Although these compounds are crystallographically rather ""ordinary"", peculiarities of their electronic structure and magnetic state lead to unconventional kinetic and magnetic properties. Their magnetic and transport characteristics are highly sensitive to external stimuli, and changes in alloy composition or external parameters can induce transitions between the states considered. This tunability provides further opportunities for controlling the electronic and magnetic properties of Heusler alloys and for exploiting them in applications such as spintronics and micro- and nanoelectronics.",Materials Science,http://arxiv.org/abs/2512.21172v1,arXiv,2
"Single-layer transition metal dihalides grown on conducting substrates were shown to host stable polarons. Here, we investigate polarons in insulating single-layer MnBr$_2$ grown by molecular beam epitaxy on three different substrates, namely graphene on Ir(110), graphene on Ir(111), and Au(111). The number densities and species of polarons observed vary strongly as a function of the substrate. For MnBr$_2$ grown on Ir(110) the largest number of polaron species is observed, namely four, of which three show clear similarities with the species observed for CoCl$_2$ on graphite. Polarons in single-layer MnBr$_2$ are observed up to 300K. They can be created, converted, and moved by the STM tip when a tunneling current flows at a proper bias voltage. For graphene on Ir(110) as a substrate, mobile polarons in MnBr$_2$ are guided through the periodic potential imposed from the super-moirÃ© resulting from the interaction of MnBr$_2$ with graphene and Ir(110). Our findings indicate that modeling of polarons in such single-layer insulators in contact with a conducting substrate requires to take the substrate explicitly into account.",Materials Science,http://arxiv.org/abs/2512.21163v1,arXiv,2
"Neutral-atom quantum hardware has emerged as a promising platform for programmable many-body physics. In this work, we develop and validate a practical framework for extracting thermodynamic properties of materials using such hardware. As a test case, we consider nitrogen-doped graphene. Starting from Density Functional Theory (DFT) formation energies, we map the material energetics onto a Rydberg-atom Hamiltonian suitable for quantum annealing by fitting an on-site term and distance-dependent pair interactions. The Hamiltonian derived from DFT cannot be implemented directly on current QuEra devices, as the largest energy scale accessible on the hardware is two orders of magnitude smaller than the target two-body interaction in the material. To overcome this limitation, we introduce a rescaling strategy based on a single parameter, $Î±_v$, which ensures that the Boltzmann weights sampled by the hardware correspond exactly to those of the material at an effective temperature $T' = Î±_vT$, where $T$ is the device sampling temperature. This rescaling also establishes a direct correspondence between the global laser detuning $Î_g$ and the grand-canonical chemical potential $ÎÎ¼$. We validate the method on a 28-site graphene nanoflake using exhaustive enumeration, and on a larger 78-site system where Monte Carlo sampling confirms preferential sampling of low-energy configurations.",Materials Science,http://arxiv.org/abs/2512.21142v1,arXiv,2
"MoirÃ© superlattices in transition-metal dichalcogenide semiconductor heterobilayers enable the quantum confinement of interlayer excitons with large out-of-plane permanent electric dipoles and spin-valley control. Here, we report a novel phonon-assisted excitation mechanism of individual moirÃ©-trapped interlayer excitons in 2H-stacked MoSe$_2$/WSe$_2$ heterobilayers via chiral $E^{\prime\prime}$ in-plane optical phonons at the Î-point. This excitation pathway preserves valley-selective optical selection rules and enables deterministic generation of individual interlayer excitons with defined helicity, emitting within a spectrally narrow energy spread. Through photoluminescence excitation spectroscopy in both the ensemble and quantum emitter regimes, we identify a fixed phonon energy of $\sim$23 meV mediating the process. First-principles calculations corroborate the symmetry and energy of the relevant phonon mode and its coupling to interlayer excitons, providing microscopic support for the observed valley-selective phonon-assisted excitation mechanism. Our results highlight the utility of chiral phonons as a tool for controlled excitation of quantum emitters in TMD moirÃ© systems, opening new opportunities for valleytronic and quantum photonic applications.",Materials Science,http://arxiv.org/abs/2512.21125v1,arXiv,2
"Spin-orbit torque efficiency is conventionally fixed by bulk materials. $D$-wave altermagnets introduce an additional nonrelativistic spin-charge conversion channel beyond inverse spin-Hall effect. Using prototypical candidate RuO$_2$ as an example, we show that the adjacent ferromagnet alone can dictate both the magnitude and sign of spin-charge conversion. Spin-pumping measurements on RuO$_2$/Y$_3$Fe$_5$O$_{12}$ (YIG) and RuO$_2$/Ni$_{80}$Fe$_{20}$ (Py) bilayers yield opposite effective spin-Hall angles that persist across crystalline and polycrystalline RuO$_2$. Inserting an ultrathin Au spacer at the RuO$_2$/YIG interface reverses the signal, envidencing a dominant interfacial inverse Rashba-Edelstein effect, whereas RuO$_2$/Py is governed by bulk inverse spin-Hall effect. First-principles calculations trace this dichotomy to interface-selective band hybridization: Rashba surface states survive at the insulating YIG contact yet are quenched by metallic Py. Our findings establish ferromagnetic interfacing as a deterministic knob for tailoring spin-charge conversion in altermagnetic oxides, paving the way to field-free, low-dissipation spintronic memory devices.",Materials Science,http://arxiv.org/abs/2512.21100v1,arXiv,2
"Two-dimensional (2D) materials are promising candidates for next-generation spintronic devices due to their tunable properties and potential for efficient spin-charge interconversion. However, discovering materials with intrinsically high spin Hall conductivity (SHC) is hindered by the vast chemical space and expensive nature of conventional experimental and first-principles methods. In this work, we employ an active learning framework to accelerate the discovery of high-SHC 2D materials. Machine learning (ML) models were trained on SHC values computed from density functional theory calculations, incorporating the Kubo formalism via tight-binding Hamiltonians constructed from maximally localized Wannier functions, with explicit treatment of spin-orbit coupling. Starting from random but chemically diverse 24 2D systems, the dataset was expanded to 41 cases (from an overall pool of around 2000 materials) over three active learning loops using an expected improvement acquisition strategy. The ML technique successfully identified several high SHC candidates with the best candidate exhibiting a SHC of 271.52 (hbar/e) Ohm^-1, nearly 23 times higher than the top performer in the initial round. Beyond candidate discovery, several features such as orbital symmetry near the Fermi energy, types of atomic species, material composition, covalent radii, and electronegativity of constituent atoms were found to play critical role in shaping the spin Hall response in 2D systems. The data generated is made publicly available to facilitate further advances in 2D spintronics.",Materials Science,http://arxiv.org/abs/2512.21077v1,arXiv,2
"The evolution of cluster structure with size and the critical size for the transition from cluster to nanocrystal have long been fundamental problems in nanoscience. Due to limitations of experimental technology and computational methods, the exploration of the continuous evolution of clusters towards nanocrystal is still a big challenge. Here, we proposed a machine learning force field (MLFF) that can generalize well to various copper systems ranging from small clusters to large clusters and bulk. The continuous evolution of copper clusters CuN towards nanocrystal was revealed by investigating clusters in a wide size range (7 <= N <= 17885) based on MLFF simulated annealing. For small CuN (N < 40), electron counting rule plays a major role in stability. For large CuN (N > 80), geometric magic number rule plays a dominant role and the evolution of clusters is based on the formation of more and more icosahedral shells. For medium size CuN (40 <= N <= 80), both rules contribute. The critical size from cluster to nanocrystal was calculated to be around 8000 atoms (about 6 nm in diameter). Our work terminates the long-term challenge in nanoscience, and lay the methodological foundation for subsequent research on other cluster systems.",Materials Science,http://arxiv.org/abs/2512.21067v1,arXiv,2
"We present MOCVD-grown, nitrogen-doped \b{eta}-Ga2O3 films as an insulating buffer layer on Fe-doped (010) \b{eta}-Ga2O3 substrates in lieu of 49% HF treatment to remove unintentional silicon at the substrate-epitaxial layer growth interface. N-doped layer thickness and NH3 flow were systematically varied to experimentally determine the lowest nitrogen concentration and thickness of the buffer layer needed to fully compensate the interfacial silicon peak. The NH3 molar flow rate was varied from 200 sccm to 1800 sccm. Results showed fully insulating N-doped layers for samples with NH3 flow rates greater than or equal to 1200 sccm and a thickness of 50 nm. This study demonstrates the efficacy of in-situ, controllably doped nitrogen buffer layers as a mitigation method for unintentional interfacial silicon at the substrate-epitaxial layer growth interface.",Materials Science,http://arxiv.org/abs/2512.20989v1,arXiv,2
"An ab initio approach is presented for studying the collective excitations in excitonic insulators, charge/spin density waves and superconductors. We derive the Bethe-Salpeter-Equation for the particle-hole excitations in the quasiparticle representation, from which the collective excited states are solved and the corresponding order parameter fluctuations are computed. This method is demonstrated numerically for the excitonic insulating phases of the biased WSe2-MoSe2 bilayer. It reveals the gapless phase-mode, the subgap Bardasis-Schrieffer modes and the above-gap scattering states. Our work paves the way for quantitative predictions of excited state phenomena from first-principles calculations in electronic systems with spontaneous symmetry breaking.",Materials Science,http://arxiv.org/abs/2512.20969v1,arXiv,2
"The layered van der Waals (vdW) ferroelectric CuInP2S6 (CIPS) exhibits unique cation hopping-driven phenomena that bring about unconventional properties with intriguing mechanisms and hold promises for advanced applications in nanoelectronics. However, an explicit analysis of its lattice dynamics and vibrational symmetries, pivotal for understanding the material's peculiar ferroelectric and ferroionic behaviors, remains incomplete. Here, we employ angle-resolved polarized Raman spectroscopy in concert with first-principles calculations to systematically unravel the anisotropic lattice vibrations of CIPS single crystals. By analyzing the polarization-dependent Raman intensities, we determine the symmetry assignments and Raman tensors of all major vibrational modes, revealing good agreement with theoretical predictions. Furthermore, we demonstrate the utility of Raman spectroscopy as a sensitive and non-invasive probe for structural and ferroelectric order evolution, by examining temperature-driven phase transitions and thickness-dependent polarization suppression in CIPS. Our findings establish a foundational framework for correlating lattice dynamics with functional properties in CIPS and provide a methodological blueprint for studying other vdW ferroelectrics.",Materials Science,http://arxiv.org/abs/2512.20925v1,arXiv,2
"In this research, a novel group of Ca-Mg oxyfluorosilicates containing different levels of fluoride substituting for oxide was synthesized by an inorganic salt coprecipitation process followed by calcination/sintering. The effects of the incorporation of fluoride on the resultant structural characteristics, apatite-forming ability and biodegradability were evaluated by X-ray diffraction, transmission electron microscopy, scanning electron microscopy/energy-dispersive X-ray spectroscopy, Fourier transform infrared spectroscopy, inductively coupled plasma spectroscopy and pH measurements. According to the results, the samples containing up to 2 mol% F present a single-phase structure of diopside (MgCaSi2O6) doped with F. It was also found that to meet the most biomineralization characteristic, the optimal value of fluoride in the homogeneous samples is 1 mol%. In this regard, on the one hand, the partial incorporation of fluoride into apatite (via forming fluorohydroxyapatite) and, on the other hand, the absence of fluorite (CaF2) as a consumer of Ca in the deposits are responsible for achieving the most apatite-forming ability circumstance controlled by an ion-exchange reaction mechanism. In conclusion, this study reflects the merit of the optimization of fluoride-doping into Ca-Mg silicates for development in biomedicine.",Materials Science,http://arxiv.org/abs/2512.21369v1,arXiv,2
"Emergent behavior, which arises from local interactions between simple elements, is pervasive in nature. It underlies the energy-efficient computing in our brains. However, realizing such dynamics in artificial materials, particularly under low-energy stimuli, remains a fundamental challenge. While dipole-dipole interactions are typically suppressed in magnetic storage, here we harness and amplify them to construct a strongly dipolar-coupled network of SmCo5 macrospins at wafer scale, which can exhibit intrinsic interaction-driven collective dynamics in response to voltage pulses. The network combines three essential ingredients: strong dipolar coupling by large single-domain macrospin, giant voltage control of coercivity over nearly 1000-fold, and disordered network topology with frustrated Ising-like energy landscape. When stimulated by 1 V pulses, the network enters a regime where interaction-driven magnetic behaviors emerge, including spontaneous demagnetization, greatly enhanced magnetization modulation, reversible freeze and resume evolution and stochastic convergence toward low-energy magnetic configurations. All these behaviors are completely absent at the single-nanomagnet level. Furthermore, by constructing micromagnetic models of the strongly dipolar-coupled macrospin networks, we show that the resulting nonlinear, high-dimensional collective dynamics, intrinsic to strongly-interacting systems, can enable accurate chaotic Mackey-Glass prediction and multiclass drone-signal classification. Our work establishes the voltage-responsive strongly-coupled SmCo5 network as a mesoscopic platform for probing emergent magnetic dynamics previously inaccessible under ambient conditions. It also suggests a fundamental distinct route towards scalable, low-voltage computing, one rooted in native physical interaction-driven collective dynamics at the network level.",Materials Science,http://arxiv.org/abs/2512.20906v2,arXiv,2
"Electronic scattering is a powerful tool to identify underlying changes in electronic behavior and incipient electronic and magnetic orders. The nematic and magnetic phases are strongly intertwined under applied pressure in FeSe, however, the additional isoelectronic substitution of sulphur offers an elegant way to separate them. Here we report the detailed evolution of the electronic and superconducting behaviour of FeSe$_{0.96}$S$_{0.04}$ under applied pressure via longitudinal magnetoresistance studies up to 15T. At intermediate pressures, inside the nematic phase, the resistivity displays an upturn in zero magnetic field, which is significantly enhanced in the magnetic field, suggesting the stabilization of a spin-density wave phase, which competes with superconductivity. At higher pressures, beyond the nematic phase boundaries, the resistivity no longer displays any clear anomalies in the zero magnetic field, but an external magnetic field induces significant upturns in resistivity reflecting a field-induced order, where superconductivity and magnetic anomalies are enhanced in tandem. This study highlights the essential role of high magnetic fields in stabilizing different electronic phases and revealing a complex interplay between magnetism and superconductivity tuned by applied pressure in FeSe$_{1-x}$S$_{x}$.",Materials Science,http://arxiv.org/abs/2512.20862v1,arXiv,2
"Ultrathin polymer-graphene heterostructures are promising materials for next generation optoelectronic and photovoltaic technologies, while the influence of the polymer's structural variation on interfacial charge transfer remains unclear. Here, using ab initio quantum mechanical calculations we show how different forms of Poly(3-hexylthiophene) (P3HT), a widely used organic semiconductor, interact with graphene. We analyze the effects of molecular chain length, end-group termination, periodicity, and the distinction between ordered and random P3HT arrangements. For isolated P3HT, the band gap decreases with increasing chain length and layer thickness, while structural disorder leads to slightly larger gaps due to reduced electronic coupling. When P3HT is deposited on graphene, all configurations exhibit spontaneous charge transfer, with electrons accumulating on graphene and holes remaining in the polymer. This effect is significantly enhanced in ordered and fully periodic structures and is noticeably weaker in disordered ones. Charge density analyses further show that thicker and more ordered P3HT layers improve electron hole separation across the interface. Our results reveal how molecular structure governs charge transfer in P3HT-graphene heterojunctions and provide practical guidelines for designing high efficiency polymer-graphene photovoltaic devices.",Materials Science,http://arxiv.org/abs/2512.20849v1,arXiv,2
"Unintentionally formed interfacial layers are ubiquitous in semiconductor devices that operate at extreme conditions. However, these layers' structure and properties often remain unknown due to the thinness of these naturally formed interphases. Here, we report on the intentional epitaxial growth and semiconductor properties of NiGa2O4 spinel layers that form at Ga2O3/NiO interfaces used in high-power and high-temperature electronic devices. Cubic spinel NiGa2O4 films of 10-50 nm thicknesses and low surface roughness (~ 2 nm) were grown using pulsed laser deposition at a substrate temperatures in the 300-900 Â°C range on Î±-Al2O3 and Î²-Ga2O3 substrates of different orientation. The optical absorption onset (3.6-3.9 eV) and thermal conductivity (4-9 W m-1 K-1) vary systematically with substrate temperature, consistent with theoretical predictions of varying Ni and Ga cation ordering on the spinel lattice. The valence band offset between NiGa2O4 and Î²-Ga2O3 is determined to be 1.8 eV. The NiGa2O4-based p-n heterojunction devices on Ga2O3 (001) substrates exhibit a rectification ratio of 10^8 (for +/-2V) and a turn-on voltage of 1.4 V, maintaining diode behavior up to 600 Â°C. These results highlight the potential of NiGa2O4 as a p-type interlayer in Ga2O3-based devices and shows a new approach to investigate such interfacial layers.",Materials Science,http://arxiv.org/abs/2512.20841v1,arXiv,2
"Simulations of SiC crystal growth using molecular dynamics (MD) have become popular in recent years. They, however, simulate very fast deposition rates, to reduce computational costs. Therefore, they are more akin to surface sputtering, leading to abnormal growth effects, including thick amorphous layers and large defect densities. A recently developed method, called the minimum energy atomic deposition (MEAD), tries to overcome this problem by depositing the atoms directly at the minimum energy positions, increasing the time scale.   We apply the MEAD method to simulate SiC crystal growth on stepped C-terminated 4H substrates with 4Â° and 8Â° off-cut angle. We explore relevant calculations settings, such as amount of equilibration steps between depositions and influence of simulation cell sizes and bench mark different interatomic potentials. The carefully calibrated methodology is able to replicate the stable step-flow growth, which was so far not possible using conventional MD simulations. Furthermore, the simulated crystals are evaluated in terms of their dislocations, surface roughness and atom mobility. Our methodology paves the way for future high fidelity investigations of surface phenomena in crystal growth.",Materials Science,http://arxiv.org/abs/2512.20804v1,arXiv,2
"Machine learning models have become firmly established across all scientific fields. Extracting features from data and making inferences based on them with neural network models often yields high accuracy; however, this approach has several drawbacks. Symbolic regression is a powerful technique for discovering analytical equations that describe data, providing interpretable and generalizable models capable of predicting unseen data. Symbolic regression methods have gained new momentum with the advancement of neural network technologies and offer several advantages, the main one being the interpretability of results. In this work, we examined the application of the deep symbolic regression algorithm SEGVAE to determine the properties of two-dimensional materials with defects. Comparing the results with state-of-the-art graph neural network-based methods shows comparable or, in some cases, even identical outcomes. We also discuss the applicability of this class of methods in natural sciences.",Materials Science,http://arxiv.org/abs/2512.20785v1,arXiv,2
"Since the first evidence of luminescence of organic polymers in STM junctions, efforts have been invested in elucidating how to leverage the voltage, anchoring chemistry, and molecular structure to optimize emission power and efficiency. Understanding the fundamentals underlying current-driven molecular emission is important not only for OLED engineering, but also to control luminescence at the atomic scale toward the mastering of single or localized photon sources. However, the difficulty in isolating the separate roles of the variables at play in molecular junction experiments, has precluded a general comprehension of their distinctive effects on the emitted power and the quantum yield. In the present report, we use time-dependent electronic structure simulations based on quantum electrodynamics to disentangle the incidence of bias, electronic coupling and molecular length on device performance, with polyphenylene-vinylene as a case study. A careful validation demonstrates that our approach can achieve quantitative agreement with available experimental data. Through its application we identify the applied bias as the main factor determining emission power. The quantum efficiency, however, is influenced only minimally by bias and electronic coupling, and is instead dominated by polymer length, on which it depends exponentially. Thus, using longer polymer chains emerges as the primary strategy for achieving higher efficiencies. Our results thereby provide key prescriptions for designing single-molecule electroluminescent platforms.",Materials Science,http://arxiv.org/abs/2512.20782v1,arXiv,2
"Organic mixed conductors (OMCs) represent a promising class of materials for applications in bioelectronics, physical computing, and thermoelectrics. Rather unparalleled, OMCs feature dynamics spanning multiple length and time scales, involving an intricate coupling between electronic, ionic, and mass transport. These characteristics set them notably apart from traditional semiconductors and hinder the description by conventional semiconductor theory. In this work, we approach the charge carrier modulation of OMCs using statistical mechanics. We discuss OMCs from a first-principles perspective and contrast them with established semiconductor materials, highlighting key differences in their collective charge carrier dynamics. This motivates our toy model describing OMCs as a lattice gas, which we analyze within the grand canonical ensemble. The model exhibits a first-order phase transition analogous to a classical vapor$\unicode{x2013}$liquid transition, governed by temperature and chemical potential. In doing so, it explains the formation of distinct low- and high-density carrier phases $\unicode{x2013}$ a mesoscale phenomenon recently observed experimentally. It also demonstrates how metastability near the phase boundary can give rise to history-dependent characteristics in device operation, a similarly well-reported effect in OMC transistors. This work is intended as a simple motivation for studying OMCs through the lens of statistical mechanics, offering a more natural description than traditional semiconductor models developed for materials of fundamentally distinct character.",Materials Science,http://arxiv.org/abs/2512.20727v1,arXiv,2
"Orbital-Free Density Functional Theory (OFDFT) has re-emerged as a viable alternative to Kohn-Sham DFT, driven by recent advances in kinetic energy density functionals (KEDFs). Nonlocal (NL) KEDFs have significantly extended OFDFT's applicability, particularly for bulk solids, but their high computational cost and dependence of system-specific parameters limit their universality. In this work, we propose a semilocal KEDF at the Generalized Gradient Approximation (GGA) level that achieves accuracy comparable to state-of-the-art NL and meta-GGA functionals, while remaining entirely parameter-free. Our construction revives the Thomas-Fermi-von Weizsacker (TFvW) framework by modulating the relative contributions of TF and vW terms through physically motivated constraints and preserving the exact second-order gradient expansion. Despite its simple form, the proposed functional (KGE2) performs remarkably well across both extended systems (metals and semiconductors) and finite systems (clusters), without any need for parameter tuning. These results mark a step toward a transferable, computationally efficient, and general-purpose KEDF suitable for large-scale OFDFT simulations.",Materials Science,http://arxiv.org/abs/2512.20564v1,arXiv,2
"The effects of inter-defect interaction on the defect thermodynamics, local structure, and oxidation of acceptor-doped wide-gap ABO3 perovskites are explored using the developed statistical theory and Monte Carlo simulations. The results demonstrate that under realistic energy parameters the interaction between oxygen vacancies and impurities generally has a greater impact on the studied properties than inter-vacancy correlations. The influence of inter-vacancy interaction significantly depends on dopant content x: inter-site vacancy repulsion becomes noticeable at sufficiently high x, whereas on-site Fermi-type correlations can be pronounced within a narrow doping range at moderate x values. It is found that a non-uniform impurity allocation, which can result from a sample preparation procedure, considerably affects oxygen-vacancy distribution, and has a weaker effect on short-range order and oxidation. It is also shown that inter-defect interaction reduces the hole concentration, increases the oxidation enthalpy, and can result in their non-trivial dependence on x. The findings of this study contribute to understanding the fundamental properties of acceptor-doped oxides, facilitating the development of new materials for clean energy applications.",Materials Science,http://arxiv.org/abs/2512.20693v1,arXiv,2
"Murunskite K$_2$Cu$_3$FeS$_4$ is a representative sulfosalt, isostructural to the pnictides, but with electronic properties more similar to the insulating parent compounds of the cuprates. We use it as a bridge to compare the chemical and physical roles of metal and ligand orbitals in cuprates and pnictides.   In cuprates, ionicity, covalency, and metallicity are tightly interwoven to give rise to high-temperature superconductivity (SC). Their most remarkable property is the interaction of an ionically localized hole on the copper (Cu) with a Fermi liquid (FL) on the oxygens (O), which is critically important for understanding all key properties of these materials. The localization is due to strong correlations on the Cu $3d$ orbital. We describe a scenario in which the localized hole gives rise both to SC by Cooper scattering of O holes, and to Fermi arcs, as observed in cuprate spectroscopy, the latter by a purely kinematic projection of the static local disorder, without invoking any residual interactions between the mobile O FL carriers.   In the pnictides, the orbitals responsible for binding and metallic conduction appear to be separate. The Fe $3d$ $e_{g}$ orbitals hybridized with the ligands set the lattice spacing. The $3d$ $t_{2g}$ orbitals overlap directly between the Fe atoms, resulting in several electronic bands appearing at the Fermi level. The ensuing Fermi liquid exhibits both charge and magnetic correlations. We argue that a similar SC scenario as in the cuprates is plausible in the pnictides, except that a light FL scatters on a slow nearly-antiferromagnetic (AF) one, rather than on localized holes as in the cuprates.",Materials Science,http://arxiv.org/abs/2512.20530v1,arXiv,2
"MoirÃ© superlattices in van der Waals materials have revolutionized the study of electronic and excitonic systems by creating periodic electrostatic potentials. Extending this concept to magnetic materials promises new pathways in merging spintronics with photonics. While moirÃ© magnetism has been revealed with near-field probes and nonlinear optical techniques, the coupling of these magnetic textures to optical excitations - magneto-moirÃ© excitons - remains unexplored. Here, we report the observation of magneto-moirÃ© excitons in twisted bilayer CrSBr, correlated with moirÃ© spin textures that emerge below a critical twist angle of ~2Â°. The nanoscale moirÃ© spin texture imprints distinct signatures onto the optical spectrum, shifting the exciton energy via a periodic magnetic exchange field. First-principles calculations corroborate that these signatures arise from one-dimensional spin textures governed by the balance of exchange interactions and domain wall energy. Our results demonstrate that moirÃ© magnetism can be used to engineer nanoscale excitonic energy landscapes, providing a new platform for magneto-optical sensing, quantum transduction, and control of non-collinear magnetism and topology through light.",Materials Science,http://arxiv.org/abs/2512.20507v1,arXiv,2
"Gelatin is often used as an analog for studying soft and biological materials in order to understand the mechanics of behavior of biological tissue in events like traumatic brain injuries. The material properties of gelatin change with the ratio of water to gelatin powder used to make a given sample. Characterizing the relationship between this ratio and the material properties of gelatin is crucial to enable its use in mechanics experiments. In this work, compression tests were performed on a texture analyzer on samples which ranged from a 2:1 to 20:1 ratio of water to gelatin powder. In this range, instantaneous stiffnesses were well fit via power law in this ratio and decreased from 277 +/- 30 kPa to 4.34 +/- 0.64 kPa. The dominant (longest) timescales of the samples were well fit via a sigmoid function in this ratio and increased from 29.8 +/- 1.0 s to 621 +/- 92 s. The resulting ratio-property relationships offer a functional way to design gelatin samples for use in mechanics experiments.",Materials Science,http://arxiv.org/abs/2512.20502v1,arXiv,2
"Flat bands in graphitic materials emerged as a platform for realizing tunable correlated physics. As a nodal-line semimetal, rhombohedral graphite features flat drumhead surface states in the vicinity of the Dirac points, which carry a nontrivial topological charge. We present a comprehensive study on rhombohedral graphite with twist stacking faults. Using both the continuum models and the realistic tight-binding models, we show that the twist angle between the graphene layers can tune the interface states at such stacking faults. The evolution of interface states originates from the interplay between the moirÃ© periodicity and Zak phase topology, predicting the occurrence of nearly flat bands throughout the moirÃ© Brillouin zone. We further investigate the disorder-induced layer polarization and tunable Chern number for flat band, and characterize the relationship between the disorder strength and Chern number in twisted rhombohedral graphite.",Materials Science,http://arxiv.org/abs/2512.20493v1,arXiv,2
"First-principles based crystal structure prediction (CSP) methods have revealed an essential tool for the discovery of new materials. However, in solids close to displacive phase transitions, which are common in ferroelectrics, thermoelectrics, charge-density wave systems, or superconducting hydrides, the ionic contribution to the free energy and lattice anharmonicity become essential, limiting the capacity of CSP techniques to determine the thermodynamical stability of competing phases. While variational methods like the stochastic self-consistent harmonic approximation (SSCHA) accurately account for anharmonic lattice dynamics \emph{ab initio}, their high computational cost makes them impractical for CSP. Machine-learning interatomic potentials offer accelerated sampling of the energy landscape compared to purely first-principles approaches, but their reliance on extensive training data and limited generalization restricts practical applications. Here, we propose an iterative learning framework combining evolutionary algorithms, atomic foundation models, and SSCHA to enable CSP with anharmonic lattice dynamics. Foundation models enable robust relaxations of random structures, drastically reducing required training data. Applied to the highly anharmonic H$_3$S system, our framework achieves good agreement with the benchmarks based on density functional theory, accurately predicting phase stability and vibrational properties from 50 to 200 GPa. Importantly, we find that the statistical averaging in the SSCHA reduces the error in the free energy evaluation, avoiding the need for extremely high accuracy of machine-learning potentials. This approach bridges the gap between data efficiency and predictive power, establishing a practical pathway for CSP with anharmonic lattice dynamics.",Materials Science,http://arxiv.org/abs/2512.20424v1,arXiv,2
"Hyperbolic metamaterials enable extreme light confinement and control of photonic states, but their realization has been restricted to inorganic architectures. Here, a fully organic route to fabricate artificial hyperbolic metamaterials based on multilayered thin films of J-aggregate carbocyanine dyes alternated with polyelectrolytes is introduced. These structures exhibit strong optical anisotropy and experimentally support hyperbolic surface exciton polaritons and, for selected dyes, additional surface waves in near-zero permittivity regimes. Spectroscopic ellipsometry confirms a uniaxial dielectric tensor with negative in-plane and positive out-of-plane components, close to the absorption peaks of the constituent J-aggregates. This anisotropy is preserved across individual layers, demonstrating the robustness of the layer-by-layer approach and enabling the coupling of surface exciton polaritons and near-zero permittivity modes even in films only a few nanometres thick. Transfer-matrix simulations based on the obtained dielectric tensor reproduce the coupling conditions for all thicknesses, validating the optical model. Structural characterization reveals the link between optical anisotropy and supramolecular order, with preferential in-plane molecular orientation and the evolution from discrete nanostructures to continuous films as deposition progresses. These organic hyperbolic metamaterial architectures, associated with narrow excitonic resonances from J-aggregates, offer a unique platform for tailoring emission, energy transport, and exploring polariton dynamics at the nanoscale.",Materials Science,http://arxiv.org/abs/2512.20411v1,arXiv,2
"We present a comprehensive investigation of the structural and electronic properties of Sn intercalated buffer layers on SiC(0001) using low-temperature scanning tunneling microscopy and spectroscopy (LT-STM/STS), spot-profile analysis low-energy electron diffraction (SPA-LEED), and density functional theory (DFT) calculations. Sn intercalation effectively decouples the buffer layer, yielding quasi-free-standing monolayer graphene (QFMLG) while introducing local lattice distortions. Bias-dependent STM imaging revealed the coexistence of conventional and Kekule-ordered graphene domains, governed by the underlying Sn(1x1) reconstruction at the SiC interface. The measured STS spectra exhibit good agreement with DFT results. However, achieving homogeneous Sn(1x1) domains remains challenging, apparently, due to strain within the Sn monolayer, which drives the emergence of Kekule distortions and the associated electronic band-gap opening omogeneously in graphene. These findings highlight the crucial role of intercalant homogeneity and strain in tuning graphene`s structural and electronic properties.",Materials Science,http://arxiv.org/abs/2512.20366v1,arXiv,2
"Using ac composite magnetoelectric technique, we map the phase diagrams of EuFe(As1-xPx)2 to resolve the interplay between superconductivity and ferromagnetism. For samples with Tc<TFM, the transition to a ferromagnetic multi-domain state suppresses Hc2 through the breakdown of Jaccarino-Peter compensation and enhanced magnetic scattering from inter-domain disorder, while Hirr is reduced due to vortex-antivortex pair nucleation at domain walls disrupting the vortex lattice. Conversely, for samples with Tc>TFM, strong short-range spin correlations and phase boundaries within a multiphase coexistence regime near the triple point act as potent pair-breaking centers, leading to pronounced Hc2 suppression.",Materials Science,http://arxiv.org/abs/2512.20364v2,arXiv,2
"Magnetic and electronic transport properties of Co$_2$MnZ (Z = Al, Ga, Ge, Si, Sn) Heusler alloys were experimentally investigated. Electrical resistivity, in the temperature range from 4.2 to 300 K, as well as field dependences of the Hall effect and magnetization at T = 4.2 K in magnetic fields up to 100 kOe and 70 kOe, respectively, were measured. Experimental data are in good agreement with the results of the theoretical DFT calculations of the electronic structure and magnetic moments. In the band structure of Co$_2$MnSi, half-metallicity is formed with the full spin polarization and the half-metallic gap of about 0.6 eV. In Co$_2$MnZ (Z = Al, Ge, Sn), it is shifted from the Fermi energy by the hole pockets at the point $Î$, preventing thereby the formation of the half-metallic state. In a peculiar case of Co$_2$MnGa, the antisite defects are expected to determine structural and electronic properties. For the Co$_2$MnAl and Co$_2$MnGa topological semimetals, Weyl topological points are found at the Fermi energy; however, for Z = Si, Ge, Si, these features are located deeper within to the valence band. The results show that Co$_2$MnGe and Co$_2$MnSn are usual ferromagnets, Co$_2$MnAl and Co$_2$MnGa alloys are topological semimetals that can find application in microelectronics, while Co$_2$MnSi is a half-metallic ferromagnet that is in high demand in spintronics.",Materials Science,http://arxiv.org/abs/2512.20358v2,arXiv,2
"Kagome magnets naturally hosting Dirac points and flat bands exhibit novel topological phases, enabling rich interplays between interactions and topologies. The discovery of two-dimensional (2D) magnets generally coexisting with different types of magnetic interactions poses a challenge for topological magnonic manipulation. Here we investigate the topological magnon phases of 2D Kagome ferromagnet with multiple magnetic anisotropic interactions, i.e. Dzyaloshinskii-Moriya interaction (DMI) and pseudo-dipolar interaction (PDI). It is found that the different sole magnetic anisotropic interactions introduce completely distinct topological phase diagrams and topological states. The multiple topological magnon phases with high Chern number emerge due to the distinct anisotropic interactions. Moreover, the interplay of the multiple anisotropic DMI and PDI interactions involved with Dirac and flat bands controls a variety of topological phase transitions, implying greater manipulation potential. In addition, the sign reversal of thermal Hall and Nernst conductivities induced by temperature is found in particular topological phase regions, namely topological origin, relating to the energy gap and Berry curvature (Chern number) in the vicinity of magnetic phase transition from the thermal fluctuations, providing a possible explanation for the experimental puzzles. All these results demonstrate that the novel topological magnonic properties in Kagome magnet with multiple magnetic anisotropic interactions can realize a potential platform for magnonic devices and quantum computing.",Materials Science,http://arxiv.org/abs/2512.20297v1,arXiv,2
"Lead-free tin-based halide perovskites are attractive for flexible and environmentally benign optoelectronics, but their application is limited by the rapid oxidation of Sn2+ to Sn4+ and poor operational stability. Here, we report a flexible CsSnI3 nanowire photodetector that achieves both high near-infrared photoresponse and long-term stability through synergistic aluminium-substrate contact engineering and dipolar interface modification. A 0.2 mm anodized aluminium foil serves as the flexible substrate, where localized laser ablation exposes metallic aluminium regions that act as reductive sites, effectively suppressing Sn2+ oxidation during nanowire growth. Simultaneously, a polar interlayer of 3-fluoro-2-nitroanisole is introduced to improve energy-level alignment, suppress interfacial deprotonation, and enhance charge extraction. The resulting device exhibits a responsivity of 0.39 A W-1, a specific detectivity of 1.38 * 10^13 Jones, and a wide linear dynamic range of 156 dB under 850 nm illumination. Moreover, the device retains over 85% of its initial photocurrent after 60 days in ambient air and maintains 94% of its initial photocurrent after 1000 bending cycles. This work establishes an effective strategy for stabilizing Sn-based perovskites toward high-performance flexible optoelectronic devices.",Materials Science,http://arxiv.org/abs/2512.20267v1,arXiv,2
"The discovery of ordered magnetism in two-dimensional van der Waals materials at the monolayer limit challenges the Mermin-Wagner theorem, which forbids spontaneous breaking of continuous symmetries in two dimensions at finite temperatures. The persistence of static magnetism in low-dimensions is fundamentally influenced by magnetic anisotropy and the local single-ion crystalline electric field. Crucially, spin-orbit coupling connects the structural properties with spin degrees of freedom. We investigate the magnetic single-ion properties in the van der Waals magnet VI$_3$. Utilizing neutron and x-ray diffraction, we map out the symmetry breaking phase transitions and argue for a single structural transition at T$_S \sim$ 80 K, driven by an orbital degeneracy, followed by a ferromagnetic transition at a lower temperature, T$_C \sim$ 50 K. Through a comparative analysis of samples prepared under varying conditions, we suggest that lower temperature transitions reported near $\sim$ 30 K are not intrinsic to VI$_{3}$. A group theoretical analysis suggests a structural transition from rhombohedral $R\overline{3}$ to triclinic $P\overline{1}$ or $P1$. This transition is significant as it suggests the formation of two distinct crystallographyically inequivalent V$^{3+}$ sites, each with distinct spin-orbital properties. Neutron spectroscopy provides evidence for dominant magnetic exchange coupling only between symmetry-equivalent sites in the triclinc unit cell. We suggest this breaks up the low-temperature honeycomb VI$_3$ lattice into two interpenetrating approximately hexagonal planes resulting in a fragmentated honeycomb. Our findings highlight the critical role of magnetoelastic coupling in determining the magnetic and structural phases in two-dimensional van der Waals magnets.",Materials Science,http://arxiv.org/abs/2512.20240v1,arXiv,2
"The rapid emergence of universal Machine Learning Interatomic Potentials (uMLIPs) has transformed materials modeling. However, a comprehensive understanding of their generalization behavior across configurational space remains an open challenge. In this work, we introduce a benchmarking framework to evaluate both the equilibrium and far-from-equilibrium performance of state-of-the-art uMLIPs, including three MACE-based models, MatterSim, and PET-MAD. Our assessment utilizes Equation-of-State (EOS) tests to evaluate near-equilibrium properties, such as bulk moduli and equilibrium volumes, alongside extensive Minima Hopping (MH) structural searches to probe the global Potential Energy Surface (PES). Here, we assess universality within the fundamental limit of unary (elemental) systems, which serve as a necessary baseline for broader chemical generalization and provide a framework that can be systematically extended to multicomponent materials. We find that while most models exhibit high accuracy in reproducing equilibrium volumes for transition metals, significant performance gaps emerge in alkali and alkaline earth metal groups. Crucially, our MH results reveal a decoupling between search efficiency and structural fidelity, highlighting that smoother learned PESs do not necessarily yield more accurate energetic landscapes.",Materials Science,http://arxiv.org/abs/2512.20230v2,arXiv,2
"We present an ensemble machine-learning approach for composition-based, structure-agnostic screening of candidate superconductors among ternary hydrides under high pressure. Hydrogen-rich hydrides are known to exhibit high superconducting transition temperatures, and ternary or multinary hydrides can stabilize superconducting phases at reduced pressures through chemical compression. To systematically explore this vast compositional space, we construct an ensemble of 30 XGBoost regression models trained on a curated dataset of approximately 2000 binary and ternary hydride entries. The model ensemble is used to screen a broad set of A-B-H compositions at pressures of 100, 200, and 300 GPa, with screening outcomes evaluated statistically based on prediction consistency across ensemble members. This analysis highlights several high-scoring compositional systems, including Ca-Ti-H, Li-K-H, and Na-Mg-H, which were not explicitly included in the training dataset. In addition, feature-importance analysis indicates that elemental properties such as ionization energy and atomic radius contribute significantly to the learned composition-level trends in superconducting transition temperature. Overall, these results demonstrate the utility of ensemble-based machine learning as a primary screening tool for identifying promising regions of chemical space in superconducting hydrides.",Materials Science,http://arxiv.org/abs/2512.20228v1,arXiv,2
"Enhancing the kinetic stability of glasses often necessitates deepening thermodynamic stability, which typically compromises ductility due to increased structural rigidity. Decoupling these properties remains a critical challenge for functional applications. Here, we demonstrate that pattern engineering in metallic glasses (MGs) enables unprecedented kinetic ultrastability while retaining thermodynamic metastability and intrinsic plasticity. Through atomistic simulations guided by machine-learning interatomic potentials and replica-exchange molecular dynamics, we reveal that clustering oxygen contents, driven by reaction-diffusion-coupled pattern dynamics, act as localized pinning sites. These motifs drastically slow structural relaxation, yielding kinetic stability comparable to crystal-like ultrastable glasses while retaining an energetic as-cast state. Remarkably, the thermodynamically metastable state preserves heterogeneous atomic mobility, allowing strain delocalization under mechanical stress. By tailoring oxygen modulation via geometric patterning, we achieve an approximately 200 K increase in the onset temperature of the glass transition (Tonset) while maintaining fracture toughness akin to conventional MGs. This work establishes a paradigm of kinetic stabilization without thermodynamic compromise, offering a roadmap to additively manufacture bulk amorphous materials with combined hyperstability and plasticity.",Materials Science,http://arxiv.org/abs/2512.20196v1,arXiv,2
"Atomic cells made by anodically bonding silicon and borosilicate glasses are widely used in atomic devices.   One inherent problem in these cells is that the silicon material blocks beams with wavelengths shorter than   1000 nm, which limits available optical accesses when alkali metal atoms are involved. In this work, we   investigate the possibility of the silicon carbide material as an alternative of silicon materials in fabricating   anodically bonded cells. We demonstrate that the optical, thermal and mechanical properties of silicon carbide   help to improve the performance of atomic devices in certain applications.",Materials Science,http://arxiv.org/abs/2512.20690v1,arXiv,2
"The results of a comprehensive study of MnSb$_2$Te$_4$ single crystals are presented. The structure, Raman spectra, low-temperature transport, Hall effect, magnetization, and magnetic susceptibility are studied. It was established that the crystals are ferromagnetic, with a Curie temperature ranging from 22 to 45\,K for different samples. Hall and magnetization measurements demonstrated that the system is a soft ferromagnet, which is of interest for practical applications.",Materials Science,http://arxiv.org/abs/2512.20137v1,arXiv,2
"Shear band propagation and interaction are critical to the mechanical performance of metallic glasses and are strongly governed by thermal history, yet their microscopic mechanisms remain unclear. Here, using molecular dynamics simulations combined with a state-of-the-art annealing protocol, we systematically investigate these behaviors in a model metallic glass across effective quenching rates spanning six orders of magnitude. Through a double-notch model, we show that the normalized interaction distance relative to the single shear band width is significantly larger in slowly quenched samples than in rapidly quenched ones. Atomic-scale analysis reveals that rapidly quenched samples exhibit a high density of pre-existing soft regions, which trigger correlated shear transformation zones through local vortex fields, resulting in propagation path locking and weak inter-band coupling. In contrast, slowly quenched samples exhibit enhanced structural heterogeneity and a right-shifted activation energy spectrum, promoting a single large-scale vortex field ahead of the shear band front. This field facilitates long-range stress transmission and induces shear band deflection, convergence, and coalescence, a transition resembling a ""shielding effect"" in fracture mechanics, where vortex-mediated disturbances destabilize the advancing shear band front. Our findings establish a direct microscopic connection between glass stability and shear-band-mediated plasticity and suggest that regulating shear band interactions offers a promising route to enhance the room-temperature ductility of metallic glasses.",Materials Science,http://arxiv.org/abs/2512.20121v1,arXiv,2
"A systematic study of the structural, electronic, and optical properties of cage-like boron clusters, with the number of constituent atoms ranging from 20 to 122, has been carried out within the framework of density-functional theory (DFT), employing 6-31G(d, p) extended basis set. The dynamic stability of the clusters is analyzed through the vibrational frequency analysis, while to study the thermodynamic stability, we computed their binding energies per atom. The results suggest that the 32- and 92-atom cages are the most stable among the small and the large structures. The optical absorption spectra of these cages is computed using the time-dependent densityfunctional theory (TDDFT), which suggests their applications in optoelectronic devices in the visible range of the spectrum.",Materials Science,http://arxiv.org/abs/2512.20114v1,arXiv,2
"One focal challenge in engineering low-power and scalable all-oxide spintronic devices lies in exploring ferromagnetic oxide material with perpendicular magnetic anisotropy (PMA) and electronic conductivity while exhibiting tunable spin states. Targeting this need, spinel nickel cobaltite (NiCo2O4, NCO), featured by room-temperature ferrimagnetically metallic ground state with strong PMA, emerges as a promising candidate in the field of oxide spintronics. The cation distribution disorder inherent to NCO renders competing electromagnetic states and abnormal sign reversal of anomalous Hall effect (AHE), introducing an additional freedom to adjust electromagnetic transports. Here, we unveil multi-state electromagnetic phase modulations in NCO system through controllable cation disorder and proton evolution, extensively expanding electromagnetic phase diagram. The cation disorder in NCO tunable by growth temperature is identified as a critical control parameter for kinetically adjusting the proton evolution, giving rise to intermediate hydrogenated states with chemical stability. Hydrogen incorporation reversibly drives structural transformation and electromagnetic state evolutions in NCO, with rich spin-dependent correlated physics uncovered by combining the AHE scaling relation and synchrotron-based spectroscopy. Our work not only establishes NCO as a versatile platform for discovering spin-dependent physical functionality but also extends the horizons in materials design for state-of-the-art spintronic devices harnessing magneto-ionic control and inherent cation disorder.",Materials Science,http://arxiv.org/abs/2512.20099v1,arXiv,2
"Electromagnetic response is commonly computed in two languages: length-gauge molecular polarizabilities and velocity-gauge (Kubo) conductivities for periodic solids. We introduce a compact, gauge-invariant bridge that carries the same microscopic inputs-transition dipoles and interaction kernels-from molecules to crystals and heterogeneous media, with explicit SI prefactors and fine-structure scaling via $(Î±_{\rm fs})$. The long-wavelength limit is handled through a reduced dielectric matrix that retains local-field mixing, interfaces and 2D layers are treated with sheet boundary conditions (rather than naÃ¯ve ultrathin films), and length-velocity equivalence is enforced in practice by including the equal-time (diamagnetic/contact) term alongside the paramagnetic current. Finite temperature is addressed on the Matsubara axis with numerically stable real-axis evaluation (complex polarization propagator), preserving unit consistency end-to-end.   The framework enables predictive, unit-faithful observables from radio frequency to ultraviolet-RF/microwave heating and penetration depth, dielectric-logging contrast, interfacial optics of thin films and 2D sheets, and adsorption metrics via imaginary-axis polarizabilities. Numerical checks (gauge overlay and optical $(f)$-sum saturation) validate the implementation. Immediate priorities include compact, temperature- and salinity-aware kernels with quantified uncertainties and \emph{operando} interfacial diagnostics for integration into multiphysics digital twins.",Materials Science,http://arxiv.org/abs/2512.20075v1,arXiv,2
"The emergence of altermagnets has driven groundbreaking advances in spintronics. Notably, d-wave altermagnets support non-relativistic spin transport, efficient charge-to-spin conversion, and T-odd spin currents. In addition, their integration as electrodes in antiferromagnetic tunnel junctions (AFMTJs) enables a tunneling magnetoresistance (TMR) effect, allowing electrical detection of NÃ©el vectors for next-generation memory devices. In this work, we investigate the non-relativistic spin transport properties of the quasi-two-dimensional (quasi-2D) d-wave altermagnet KV\textsubscript{2}Se\textsubscript{2}O and the TMR effect in KV\textsubscript{2}Se\textsubscript{2}O-based AFMTJs via first-principles calculations. Our results reveal that KV\textsubscript{2}Se\textsubscript{2}O exhibits a non-relativistic longitudinal spin polarization and a spin Hall angle both exceeding 60\% at room temperature, while KV\textsubscript{2}Se\textsubscript{2}O-based AFMTJs achieve a giant TMR reaching approximately $10^{12}$\%, which remains robust against Fermi level shifts. These findings highlight the anisotropic spin polarization inherent to d-wave staggered magnetism and underscore the critical role of Fermi surface topology in enhancing T-odd spin transport and the TMR effect in AFMTJs.",Materials Science,http://arxiv.org/abs/2512.20072v1,arXiv,2
"Excellent gate electrostatics in field effect transistors (FETs) based on two-dimensional transition metal dichalcogenide (2D TMD) channels can dramatically decrease static power dissipation. Energy efficient FETs operate in enhancement mode with small and positive threshold voltage (Vth) for n-type devices. However, most state-of-the-art FETs based on monolayer MoS2 channel operate in depletion mode with negative Vth due to doping from the underlying dielectric substrate. In this work, we identify key properties of the semiconductor/dielectric interface (MoS2 on industrially relevant high dielectric constant (k) HfO2, ZrO2 and hBN for reference) responsible for realizing enhancement-mode operation of 2D MoS2 channel FETs. We find that hBN and ZrO2 dielectric substrates provide low defect interfaces with MoS2 that enables effective modulation of the Vth using gate metals of different work functions (WFs). We use photoluminescence (PL) and synchrotron X-ray photoelectron spectroscopy (XPS) measurements to investigate doping levels in monolayer MoS2 on different dielectrics with different WF gate metals. We complement the FET and spectroscopic measurements with capacitance-voltage analysis on dielectrics with varying thicknesses, which confirm that Vth modulation in ZrO2 devices is correlated with WF of the gate metals - in contrast with HfO2 devices that exhibit signatures of Vth pinning induced by oxide/interface defect states. Finally, we demonstrate FETs using a 2D MoS2 channel and a 6 nm of ZrO2 dielectric, achieving a subthreshold swing of 87 mV dec-1 and a threshold voltage of 0.1 V. Our results offer insights into the role of dielectric/semiconductor interface in 2D MoS2 based FETs for realizing enhancement mode FETs and highlight the potential of ZrO2 as a scalable high-k dielectric.",Materials Science,http://arxiv.org/abs/2512.20069v1,arXiv,2
"This study is focused on the mechanism of in vitro biomineralization on the surface of CaO.MgO.2SiO2 (diopside) nanostructured coatings by scanning electron microscopy, energy-dispersive X-ray spectroscopy and inductively coupled plasma spectroscopy assessments. A homogeneous diopside coating of almost 2 um in thickness was deposited on a medical-grade stainless steel by coprecipitation, dipping and sintering sequences. After soaking the sample in a simulated body fluid (SBF) for 14 days, a layer with the thickness of 8 Î¼m is recognized to be substituted for the primary diopside deposit, suggesting the mineralization of apatite on the surface. Investigations revealed that the newly-formed layer is predominantly composed of Ca, P and Si, albeit with a biased accumulations of P and Si towards the surface and substrate, respectively. The variations in the ionic composition and pH of the SBF due to the incubation of the sample were also correlated with the above-interpreted biomineralization. In conclusion, the multiple ion-exchange reactions related to Ca, Mg, Si and P were found to be responsible for the in vitro bioactivity of nanodiopside.",Materials Science,http://arxiv.org/abs/2512.20067v1,arXiv,2
"Porous ceramic microspheres are a desirable substance for bone tissue reconstruction and delivery applications. This study focuses on Mg-Ca silicate microspheres encapsulated in biodegradable poly (lactic-co-glycolic acid) (PLGA) to serve as a biocompatible carrier for the controlled release of vancomycin hydrochloride. In this regard, diopside (MgCaSi2O6), bredigite (MgCa7Si4O16) and akermanite (MgCa2Si2O7) powders were synthesized by sol-gel and subsequent calcination methods. Then, porous akermanite, diopside and bredigite microspheres of 700-1000 um in diameter were fabricated by using carbon porogen, droplet extrusion and sintering, then loaded with the drug and eventually coated with PLGA. The bare microspheres showed a considerable burst release mode of the drug into a physiological medium, whereas PLGA coating of the microspheres reduced the burst release level. To investigate effective mechanisms governing in the drug release from the carriers, the contribution of burst, degradation, and diffusion was analyzed by the sequential quadratic programming algorithm method. It was found that the relative contribution of diffusion to bioresorption is ranked as diopside > akermanite > bredigite, whereas PLGA coating dominates the diffusion mechanism. The dental pulp stem cells cytocompatibility MTT assay of the microspheres also showed that the drug loading deteriorates but PLGA coating improves the cell biocompatibility significantly. Comparatively, the biocompatibility of the PLGA-coated microspheres was ranked as akermanite > diopside > bredigite, as a result of a compromise between the release of the constituting ions of the ceramic carriers and vancomycin molecules. It was eventually concluded that PLGA-coated Mg-Ca silicate microspheres are promising candidates for drug-delivery bone tissue engineering and dental bone grafting applications.",Materials Science,http://arxiv.org/abs/2512.20065v1,arXiv,2
"The hole-doped NdNiO$_2$ layer deposited on the SrTiO$_{3}$ surface exhibits unconventional superconductivity. Here, we present a systematic study of the electronic and magnetic properties of the NdNiO$_2$ superconductor using the density functional theory (DFT). The strong local Coulomb interactions in the Ni($3d)$ and Nd($4f$) states are included within the DFT+$U$ method. The effect of Sr doping on the electronic band structure and density of states was studied for the NdNiO$_2$ thin films deposited on the SrTiO$_3$ (001) surface. The results obtained for the uncapped thin films were compared with the calculations for the NdNiO$_2$ films capped by the SrTiO$_3$ layer. We have found significant changes in the electronic structure and magnetic properties of the thin films compared to the bulk crystal.",Materials Science,http://arxiv.org/abs/2512.19916v1,arXiv,2
"The layered van der Waals Fe$_5$GeTe$_2$ (F5GT) compound exhibits room-temperature ferromagnetism, making it a promising candidate for technological applications. In our study, combined temperature- and polarization-dependent Raman measurements, along with modern {\it ab initio} calculations, reveal important aspects of the lattice dynamics. The angle dependence of Raman intensity under linear polarization configuration exhibits a strong tilt in the laboratory coordinate, indicating the existence of anisotropic electron-phonon coupling. The electron-phonon coupling was also examined via the Fano parameter of the asymmetric peak in the Raman spectra. Finally, the threefold rotational symmetry guarantees the existence of chiral phonons. We present direct spectroscopic evidence for these chiral vibrational modes through cross-circularly polarized Raman measurements, complemented by theoretical calculations of phonon circular polarization. Together, these results identify F5GT as an ideal platform for investigating emergent couplings among lattice, electronic, and magnetic degrees of freedom and for advancing the understanding of chiral phonons in magnetic van der Waals materials.",Materials Science,http://arxiv.org/abs/2512.19910v1,arXiv,2
"Antiferromagnetic EuM$_{2}$Pn$_{2}$ compounds, where M is a metal element and Pn is a pnictogen element, have been recognized as candidates for realizing a topologically nontrivial electronic structure. In this paper, we focus on EuMg$_2$Bi$_2$, whose topological nature still remains unclear. We present a comprehensive study based on several experimental and theoretical techniques. Magnetic susceptibility, electrical resistivity, and specific heat capacity measurements confirm the existence of an antiferromagnetic ordering. The electronic band structure was investigated by high-resolution angle-resolved photoemission spectroscopy (ARPES), supported by ab initio calculations. ARPES measurement reveals that the electronic structure of this system is dominated by linearly dispersive hole-like bands near the Fermi level. Theoretical analyses of the electronic band structure indicates that EuMg$_2$Bi$_2$ is a strong topological insulator, which should be reflected in the presence of a metallic surface state. We also theoretically examine the magnetic-field-induced anomalous Hall conductivity, confirming previously reported observations.",Materials Science,http://arxiv.org/abs/2512.19906v1,arXiv,2
"The complexity and richness of phenomena governing alloy crystal growth can be unraveled by examining the three-dimensional atomic-level distribution of elements and impurities incorporated during growth. These species act as atomic fingerprints, revealing the thermodynamic constraints that shape material structure and composition. Herein, we combine transmission electron microscopy and atom probe of tellurium (Te) inclusions within cadmium zinc telluride (CZT) single crystals. The correlative analysis uncovers nanoscale precipitates embedded within Te inclusions, consisting of CZT nanocrystals with a Zn content of 1.5 at.%. Surrounding these precipitates, an around 10 nm-thick shell is observed, enriched with copper and indium impurities. In addition, traces of sodium and sulfur are detected within the nanocrystals. These findings provide direct evidence of the complex segregation and precipitation processes occurring during CZT crystal growth, reflecting the interplay of thermodynamic driving forces and kinetic constraints that govern solute redistribution. The resulting insights contribute to a deeper understanding of impurity behavior and phase separation mechanisms in CZT alloys. This work establishes a framework for modeling and optimization of growth strategies of higher-quality CZT crystals for next-generation infrared and radiation detection technologies.",Materials Science,http://arxiv.org/abs/2512.19904v1,arXiv,2
"The discovery of high-temperature superconductors remains a central challenge in materials science. Hydrogen-rich compounds are among the most promising candidates, as they can exhibit phonon-mediated superconductivity at elevated critical temperatures, though their stabilization typically requires extreme pressures. % Here, we report the identification of YSbH$_{6}$ as a promising superconductor by a multi-stage high-throughput screening on ternary A15-type hydrides, followed by a high-throughput computational search of the Y--Sb--H system, accelerated by ephemeral data derived potentials. % The cubic $Pm\Bar{3}$ YSbH$_{6}$ phase exhibits a predicted critical temperature of 118\,K at 50\,GPa, among the highest $T_{\rm c}$ reported to date for an A15-hydride at this pressure. Thermodynamic analysis shows that YSbH$_{6}$ lies $\sim$100\,meV/atom above the convex hull at 50\,GPa, but only 26\,meV/atom above the hull at 120\,GPa, suggesting possible metastability and synthesis at similar high pressure conditions. The phase is dynamically stable over a wide pressure range (20--120\,GPa), displays kinetic stability at 50\,GPa and elastic stability at 20 and 50\,GPa, key ingredients for long-lived metastable behaviour at moderate pressures. % These results highlight YSbH$_{6}$ as a benchmark case illustrating the balance between high-$T_{\rm c}$ performance and limited thermodynamic stability in ternary hydrides, and underscore the importance of combined dynamic, thermodynamic, kinetic and elastic stability analyses for guiding experimental synthesis of metastable superconductors.",Materials Science,http://arxiv.org/abs/2512.19901v1,arXiv,2
"We present a semi-automated method for obtaining an initial estimate of Wannier functions, designed to facilitate the construction of Wannier functions for describing low-energy effective models of solids, particularly those relevant to strongly correlated electron systems. Our approach automatically determines the hydrogenic projections orbitals and the center of the Wannier functions from information on Bloch wavefunctions at the $Î$ point. This method is integrated into cif2qewan, enabling seamless generation of input files for Quantum ESPRESSO and Wannier90. We validate our method through applications to both inorganic and organic compounds, such as Si, SrVO$_3$, FeSe, Na$_8$Al$_6$Si$_6$O$_{24}$, and (TMTTF)$_2$PF$_6$. The obtained results demonstrate that our semi-automated projections give a good initial estimate of the Wannier functions. We also show the comparisons with other methods for estimating the initial states of the Wannier functions, such as the Selected Columns of the Density Matrix (SCDM). Our methodology shows an efficient way to construct Wannier functions, paving the way for high-throughput calculations in the study of complex materials.",Materials Science,http://arxiv.org/abs/2512.19900v1,arXiv,2
"We show that the nonlinear optical response probed by two-dimensional coherent spectroscopy (2DCS) can discriminate between excitonic and lattice driven order. In the excitonic regime of a realistic model of Ta$_2$NiSe$_5$, the third order 2DCS signals are strongly enhanced by the condensate's amplitude and phase modes, with negligible contributions from single-particle excitations. In the linear optical response, in contrast, single-particle and collective-mode contributions overlap. With increasing electron-phonon coupling, the amplitude mode contribution to 2DCS initially remains robust, but then drops rapidly and remains small in the phonon-dominated regime -- even in systems with large order parameter. 2DCS also aids the detection of the massive relative phase mode, which is analogous to the Leggett mode in superconductors. Our analysis, based on the time-dependent Hartree-Fock approach, demonstrates that 2DCS can track the emergence of the symmetry-broken state and the crossover from Coulomb-driven to phonon-driven order.",Materials Science,http://arxiv.org/abs/2512.19689v1,arXiv,2
"Uranium dioxide, UO$_2$, is a canonical example of a magnetic material with strong spin-orbit coupling. Here, we present a study of the magnetic diffuse scattering measured on a polycrystalline sample of UO$_2$, which we interpret in terms of its magnetic interactions between U$^{4+}$ magnetic moments. By refining values of the magnetic interaction parameters to magnetic diffuse-scattering data measured above the magnetic ordering transition temperature, we show that the dominant magnetic coupling in UO$_2$ is a bond-dependent interaction analogous to the Kitaev model of honeycomb magnets. We compare our experimental results with published theoretical predictions and experimental measurements of the magnetic excitation spectrum. Our results suggest that magnetic materials with $f$-electron magnetic ions, particularly actinides, may be promising candidates for realising Kitaev magnetism, and highlight the role that magnetic diffuse-scattering data can play in identifying such materials.",Materials Science,http://arxiv.org/abs/2512.19674v1,arXiv,2
"Quantum computing presents a promising alternative to classical computational methods for modeling strongly correlated materials with partially filled d orbitals. In this study, we perform a comprehensive quantum resource estimation using quantum phase estimation (QPE) and qubitization techniques for transition metal oxide molecules and a Pd zeolite catalyst fragment. Using the binary oxide molecules TiO, MnO, and FeO, we validate our active space selection and benchmarking methodology, employing classical multireference methods such as complete active space self-consistent field (CASSCF) and N-electron valence state perturbation theory (NEVPT2). We then apply these methods to estimate the quantum resources required for a full-scale quantum simulation of a $Z_2Pd$ ($Z=Al_2Si_{22}O_{48}$) fragment taken from the $Pd/2(Al_xSi_{(1-x)})$ catalyst family where x=Si/Al. Our analysis demonstrates that for large Pd zeolite systems, simulations achieving chemical accuracy would require ~$10^6-10^7$ physical qubits, and range that is consistent with the projected capabilities of future fault-tolerant quantum devices. We further explore the impact of active space size, basis set quality, and phase estimation error on the required qubit and gate counts. These findings provide a roadmap for near-term and future quantum simulations of industrially relevant catalytic materials, offering insights into the feasibility and scaling of quantum chemistry applications in materials science.",Materials Science,http://arxiv.org/abs/2512.19778v3,arXiv,2
"ScN alloyed AlN (ScxAl1-xN, ScAlN) is a wurtzite semiconductor with attractive ferroelectric, dielectric, piezoelectric, and optical properties. Here, we show that ScAlN films (with x spanning 0.18 to 0.36) contain nanoscale Sc-rich clusters which maintain the wurtzite crystal structure. While both molecular beam epitaxy (MBE) and sputter deposited Sc0.3Al0.7N films show Sc clustering, the degree of clustering is significantly stronger for the MBE-grown film, offering an explanation for some of the discrepancies between MBE-grown and sputtered films reported in the literature. Moreover, the MBE-grown Sc0.3Al0.7N film exhibits a dispersive and anomalously large dielectric permittivity, roughly double that of sputtered Sc0.3Al0.7N. We attribute this result to the Sc-rich clusters locally reaching x ~ 0.5 and approaching the predicted ferroelectric-to-paraelectric phase transition, resulting in a giant (local) enhancement in permittivity. The Sc-rich clusters should similarly affect the piezoelectric, optical, and ferroelectric responses, suggesting cluster-engineering as a means to tailor ScAlNs functional properties.",Materials Science,http://arxiv.org/abs/2512.19599v1,arXiv,2
"Energy conversion in AZ31B magnesium alloy depends strongly on the dominant deformation mechanism. In slip-dominated specimen, strained parallel to extrusion direction $\parallel$ ED, approximately 50$\%$ of plastic work is converted into heat, with Taylor-Quinney coefficient $Î²_{int}$ rising rapidly then gradually with strain. Twinning-dominated specimen ($\perp$ ED) initially stores most plastic work, showing minimal heat dissipation, reflecting the dislocation-mediated nature of twinning in HCP metals, and $Î²_{int}$ increasing to $\approx$ 0.4 at failure. The final microstructure tracks stored energy evolution: the $\parallel$ ED specimen, predominantly slip-dominated, exhibits fragmented grains and strong dislocation activity, with twinning appearing at the final stages, driving energy accumulation and lattice rotation. In contrast, the $\perp$ ED specimen shows limited refinement, early localization, and twinning-driven premature fracture.",Materials Science,http://arxiv.org/abs/2512.19548v1,arXiv,2
"This study aims to evaluate the optoelectronic properties of metal free porphyrin-based D-$Ï$-A dyes via in-silico performance investigation notifying energy informatics and decision support. To develop novel organic dyes, three acceptor/anchoring groups and five donating groups were introduced to strategic positions of the base porphyrin structure, resulting in a total of fifteen dyes. The singlet ground state geometries of the dyes were optimized utilizing density functional theory (DFT) with B3LYP and the excited state optical properties were explored through time-dependent DFT (TD-DFT) using the PCM model with tetrahydrofuran (THF) as solvent. Both DFT and TD-DFT calculations were carried out using the 6-311G(d,p) basis set. The HOMO energy levels of almost all the modified dyes are lower than the redox potential of I$^-$/I$3^-$ and LUMO energy levels are higher than the conduction band of TiO$2$. The absorption maxima values ranged from 690.64 to 975.55 nm. The dye N1 using triphenylamine group as donor and p-ethynylbenzoic acid group as acceptor, showed optimum optoelectronic properties ($ÎG{reg}=-9.73$ eV, $ÎG{inj}=7.18$ eV, $V_{OC}=1.47$ V and $J_{SC}=15.03$ mA/cm$^2$) with highest PCE 14.37%, making it the best studied dye. This newly modified organic dye with enhanced PCE is remarkably effective for the dye-sensitized solar cells (DSSC) industry. Beyond materials discovery, this study highlights the role of high-performance computing in enabling predictive screening of dye candidates and generating performance indicators (HOMO-LUMO gaps, absorption spectra, charge transfer free energies, photovoltaic metrics). These outputs can serve as key parameters for energy informatics and system modelling.",Materials Science,http://arxiv.org/abs/2512.19529v1,arXiv,2
"The effects of uniaxial and biaxial tensile strain on the $Ï_{xx}$ and $Ï_{yy}$ components of the resistivity tensor, and the commensurable-nearly commensurate CDW (CCDW-NCCDW) transition temperature in 1T-TaS$_2$ are studied. At room temperature, uniaxial tensile strain increases the resistivity tensor components by a comparable magnitude both parallel and perpendicular to the strain axis. In the case of biaxial strain, up to 20~K decrease in the CCDW-NCCDW phase transition temperature is observed. In the case of uniaxial strain, a new phase with two different CCDW-NCCDW phase transition temperatures is observed, the splitting exceeds 10 K. The occurrence of such a phase is associated with the transition of the CDW into the commensurate state along the tensile strain direction while maintaining nearly commensurability along the perpendicular one. The results allow to justify various models widely used in analysis of transport properties of 1T-TaS$_2$ in commensurate and nearly commensurate states.",Materials Science,http://arxiv.org/abs/2512.19503v1,arXiv,2
"The realization of long-range spin order in two-dimensions (2D) has catapulted the search for layered materials with magnetic ordering above room temperature. These efforts aim to understand and enhance the spin spin interactions in 2D. An emergent class of such magnets is the layered FeNGeTe2 (N = 3, 4, and 5). Here, we investigate the magnetic states over a wide field temperature phase space in the high-Tc ferromagnet Fe5GeTe2 using magnetization, ferromagnetic resonance (FMR), and magneto-transport measurements. Our findings reveal a magnetic phase transition from a collinear to a complex non-collinear magnetic order near the temperature T* = 160 K, below which magnetic susceptibility is reduced, FMR linewidth broadened, and anomalous Hall resistivity suppressed. Such non-collinearity results from the competition between magnetocrystalline anisotropy and Dzyaloshinskii Moriya interaction arising from the unusual Fe1 ordering in two possible split sites. Our study focuses on the strategy to quench the non-collinear spin order. Substituting 40% Ni in Fe5GeTe2 is found to be one such quenching strategy. This provides deeper insights into the magnetism of a high-Tc layered-ferromagnet, offering opportunities to develop 2D magnet-based devices.",Materials Science,http://arxiv.org/abs/2512.19478v1,arXiv,2
"Poly(N-isopropylacrylamide) (PNIPAM) is a temperature-responsive polymer that undergoes large volumetric deformations through a transition from a swollen to a collapsed state at the volume phase transition temperature (VPTT). Locally, these deformations stem from the coil-to-globule transition of individual chains. In this contribution, I revisit the study of Suzuki and Ishii (""Phase coexistence of neutral polymer gels under mechanical constraint""), which demonstrated that a PNIPAM rod can exhibit phase coexistence (i.e. comprise swollen and collapsed domains) near the VPTT when subjected to mechanical constraints. Specifically, that paper showed that (1) collapsed domains gradually form in a fixed swollen rod with time and (2) swollen domains can nucleate in a collapsed rod that under uniaxial extension. These behaviors originate from the local thermo-mechanical response of the chains, which transition between states in response to the applied mechanical loading. Here, I develop a statistical-mechanics based framework that captures the behavior of individual chains below and above the VPTT and propose a probabilistic model based on the local chain response that sheds light on the underlying mechanisms governing phase nucleation and growth. The model is validated through comparison with experimental data. The findings from this work suggest that in addition to the classical approaches, in which the VPTT is programmed through chemical composition and network topology, the transition can be tuned by mechanical constraints. Furthermore, the proposed framework offers a pathway to actively tailor the VPTT through the exertion of mechanical forces, enabling improved control and performance of PNIPAM hydrogels in modern applications.",Materials Science,http://arxiv.org/abs/2512.19464v1,arXiv,2
"Linear phase-contrast scanning transmission electron microscopy (STEM) techniques compatible with high-throughput 4D-STEM acquisition are widely used to enhance phase contrast in weakly scattering and beam-sensitive materials. In these modalities, contrast transfer is often suppressed at low spatial frequencies, resulting in a characteristic contrast gap that limits quantitative imaging. Approaches that retain low-frequency phase contrast exist but typically require substantially increased experimental complexity, restricting routine use. Dark-field STEM imaging captures this missing low-frequency information through electrons scattered outside the bright-field disk, but discards a large fraction of the scattered signal and is therefore dose-inefficient. Fused Full-field STEM (FF-STEM) is introduced as a 4D-STEM imaging modality that overcomes this limitation by combining ptychographic phase reconstruction with tilt-corrected dark-field imaging within a single acquisition. Bright-field data are used to estimate probe aberrations and reconstruct a high-resolution phase image, while dark-field data provide complementary low-frequency contrast. The two channels are optimally fused in Fourier space using minimum-variance weighting based on the spectral signal-to-noise ratio, yielding transfer-gap-free images with high contrast and quantitative fidelity. FF-STEM preserves the upsampling and depth-sectioning capabilities of ptychography, adds robust low-frequency contrast characteristic of dark-field imaging, and enables dose-efficient, near-real-time reconstruction.",Materials Science,http://arxiv.org/abs/2512.19460v2,arXiv,2
"Large Language Models (LLMs) have emerged as powerful tools for accelerating scientific discovery, yet their static knowledge and hallucination issues hinder autonomous research applications. Recent advances integrate LLMs into agentic frameworks, enabling retrieval, reasoning, and tool use for complex scientific workflows. Here, we present a domain-specialized agent designed for reliable automation of first-principles materials computations. By embedding domain expertise, the agent ensures physically coherent multi-step workflows and consistently selects convergent, well-posed parameters, thereby enabling reliable end-to-end computational execution. A new benchmark of diverse computational tasks demonstrates that our system significantly outperforms standalone LLMs in both accuracy and robustness. This work establishes a verifiable foundation for autonomous computational experimentation and represents a key step toward fully automated scientific discovery.",Materials Science,http://arxiv.org/abs/2512.19458v1,arXiv,2
"Aqueous zinc (Zn) batteries (AZBs) face significant challenges due to the limited compatibility of Zn anodes with conventional separators, leading to dendrite growth, hydrogen evolution reaction (HER), and poor cycling stability. While separator design is crucial for optimizing battery performance, its potential remains underexplored. The commonly used glass fiber (GF) filters were not originally designed as battery separators. To address their limitations, nanochitin derived from waste shrimp shells was used to fabricate separators with varying concentrations of amine and carboxylic functional groups. This study investigates how the type and concentration of these groups influence the separator's properties and performance. In a mild acidic electrolyte that protonates the amine groups, the results showed that the density of both ammonium and carboxylic groups in the separators significantly affected water structure and ionic conductivity. Quasi-Elastic Neutron Scattering (QENS) revealed that low-functionalized chitin, particularly with only ammonium groups, promotes strongly bound water with restricted mobility, thereby enhancing Zn plating and stripping kinetics. These separators exhibit exceptional Zn stability over 2000 hours at low current densities (0.5 mA/cm2), maintaining low overpotentials and stable polarization. Additionally, the full cell consisting of Zn||NaV3O8.1.5H2O showed a cycle life of over 2000 cycles at 2 A/g, demonstrating the compatibility of the nanochitin-based separators with low concentrations of functional surface groups. These results demonstrate the importance of a simple separator design for improving the overall performance of AZBs.",Materials Science,http://arxiv.org/abs/2512.19449v1,arXiv,2
"Measurement of the Hall effect is a ubiquitous probe for materials discovery, characterization, and metrology. Inherent to the Hall measurement geometry, the measured signal is often contaminated by unwanted contributions, so the data must be processed to isolate the Hall response. The standard approach invokes Onsager-Casimir reciprocity and antisymmetrizes the raw signal about zero applied magnetic field. In hysteretic materials this becomes nontrivial, since Onsager-Casimir relations apply only to microscopically reversible states. Incorrect antisymmetrization can lead to artifacts that mimic anomalous or topological Hall signatures. The situation is especially subtle when hysteresis loops are not centered at zero applied field, as in exchange-biased systems. A practical reference for generically extracting the Hall response in hysteretic materials is lacking. Here, using Co$_3$Sn$_2$S$_2$ as a bulk single-crystal model that can be prepared with or without exchange-biased hysteresis, we demonstrate two procedures that can be used to extract the Hall effect: (1) reverse-magnetic-field reciprocity and (2) antisymmetrization with respect to applied field. We then measure the Hall effect on CeCoGe$_3$, a noncentrosymmetric antiferromagnet which can be prepared to have asymmetric magnetization and magnetoresistance, and demonstrate how improper processing can generate artificial anomalous Hall signals. These methods are generic and can be applied to any conductor.",Materials Science,http://arxiv.org/abs/2512.19427v1,arXiv,2
"Magnetic two-dimensional (2D) crystals were isolated about a decade ago, triggering a tremendous research activity worldwide. This colloquium raises a stiff question: what is really new about them? At first sight, they seem to be purer implementations of 2D spin models than traditional systems such as ultra-thin films. Yet, they partly realized their promises so far, and whether they give fresh perspectives on long-standing predictions in statistical physics is still an open question. Undoubtedly, they are uniquely amenable to electric-field effect, susceptible to mechanical deformation, and sensitive to moirÃ©s, for example. They represent interesting platforms for exploring, challenging, or simply revisiting a wide range of phenomena in condensed matter magnetism. This colloquium intends to offer a critical, yet not necessarily skeptical, overview of the field, clarifying what we believe could be unique with 2D magnets, related quasi-2D van der Waals magnets, and their heterostructures.",Materials Science,http://arxiv.org/abs/2512.19417v1,arXiv,2
"Defects in atomically thin van der Waals materials have recently been investigated as sources of spin-photon entanglement with sensitivity to strain tuning. Unlike many two-dimensional materials, quasi-one-dimensional materials such as transition metal trichalcogenides exhibit in-plane anisotropy resulting in axis-dependent responses to compressive and tensile strains. Herein, we characterize the tunable spin and optical properties of intrinsic vacancy defects in titanium trisulfide (TiS3) and niobium trisulfide (NbS3) nanowires. Within our ab initio approach, we show that sulfur vacancies and divacancies (VS and VD , respectively) in TiS3 and NbS3 adopt strain-dependent defect geometries between in-plane strains of -3 % and 3 %. The calculated electronic structures indicate that both VS and VD possess in-gap defect states with optically bright electronic transitions whose position relative to the conduction and valence bands varies with in-plane strain. Further, our calculations predict that VS in TiS3 and VD in NbS3 exhibit transitions in their ground state spins; specifically, a compressive strain of 0.4 % along the direction of nanowire growth causes a shift from a triplet state to a singlet state for the VS defect in TiS3, whereas a tensile strain of 2.9 % along the same direction in NbS3 induces a triplet ground state with a zero-phonon line of 0.83 eV in the VD defect. Our work shows that the anisotropic geometry of TiS3 and NbS3 nanowires offers exceptional tunability of optically active spin defects that can be used in quantum applications.",Materials Science,http://arxiv.org/abs/2512.19391v2,arXiv,2
"Experimental data on the specific heat $C_p$ of quadruple perovskites ACu$_3$Fe$_2$Re$_2$O$_{12}$ (A = Mn, Cu, La, Ce, Dy) are presented, demonstrating an anomalous concave-down $C_p/T$ vs. $T^2$ curve and a bell-shaped feature in $Î²(T) = (C_p - Î³T)/T^3$ plotted against $T$ on a logarithmic scale. This feature is most pronounced for A = Cu and Mn. These findings can be explained by the rattling phenomenon, previously identified in other systems such as filled skutterudites and $Î²$-pyrochlores. Using first-principles DFT+U calculations, the presence of a rattling mode in A = Mn system is directly confirmed. A qualitative interpretation of the rattling mechanism in terms of a pseudo-Jahn-Teller effect is proposed.",Materials Science,http://arxiv.org/abs/2512.19384v1,arXiv,2
"The efficient and cost-effective conversion of nitro compounds to amines is crucial for industrial processes and environmental remediation, highlighting the growing demand for earth-abundant metal-based catalysts. In this study, magnetic Ni--NiO nanostructures and their composites with two-dimensional hexagonal boron nitride (\textit{h}-BN) were synthesized via a simple and scalable combustion method. The structural, morphological, and compositional properties of the synthesized materials were systematically investigated using powder X-ray diffraction (PXRD), scanning electron microscopy (SEM), transmission electron microscopy (TEM), X-ray photoelectron spectroscopy (XPS), and UV--Vis spectroscopy. The catalytic activity of both Ni--NiO and \textit{h}-BN/Ni--NiO nanostructures was evaluated using nitrophenol reduction as a model reaction. The \textit{h}-BN/Ni--NiO nanocomposite exhibited significantly enhanced catalytic performance compared to pristine Ni--NiO, highlighting the synergistic interaction between \textit{h}-BN and Ni--NiO nanoparticles. Notably, the magnetic nature of the Ni--NiO core enabled facile recovery of the catalyst using an external magnetic field, and the composite demonstrated excellent stability and reusability for up to six catalytic cycles with minimal loss of activity. The combination of high catalytic efficiency, magnetic separability, and structural stability positions the \textit{h}-BN/Ni--NiO nanocomposite as a promising candidate for green and sustainable catalytic applications, particularly in environmental remediation.",Materials Science,http://arxiv.org/abs/2512.19774v1,arXiv,2
"The strain-relaxation mechanism of a set of Si$_{0.6}$Ge$_{0.4}$ linearly graded buffers (LGBs), grown following different temperature profiles, has been investigated by means of defect-etching and variable-temperature high-resolution X-ray diffraction (VT-HRXRD). Defect-etching experiments demonstrate that a sharp increase of threading dislocation density (TDD) from $3 \times 10^{5}$\,cm$^{-2}$ to $1.2 \times 10^{6}$\,cm$^{-2}$ takes place when the final growth temperature exceeds a critical value T$_c\approx 530^\circ$C. VT-HRXRD measurements show that in low TDD samples extra relaxation takes place for annealing temperatures larger than T$_c$, thanks to the nucleation of new dislocations. These results indicate that, below T$_c$, strain relaxation is driven by the gliding of existing dislocations while above T$_c$ new dislocations are nucleated, suggesting a link with our results and the brittle-to-ductile transition in Si$_{1-x}$Ge$_x$ alloys.",Materials Science,http://arxiv.org/abs/2512.19121v1,arXiv,2
"Addressing a critical challenge in current tissue-engineering practices, this study aims to enhance vascularization in 3D porous scaffolds by incorporating bioceramics laden with pro-angiogenic ions. Specifically, freeze-dried gelatin-based scaffolds were infused with sol-gel-derived powders of Cu-doped akermanite (Ca2MgSi2O7) and bredigite (Ca7MgSi4O16) at various concentrations (10, 20, and 30 wt%). The scaffolds were initially characterized for their structural integrity, biodegradability, swelling behavior, impact on physiological pH, and cytocompatibility with human umbilical vein endothelial cells (HUVECs). The silicate incorporation effectiveness in promoting vascularity was then assessed through HUVEC attachment, capillary tube formation, and ex-ovo chick embryo chorioallantoic membrane assays. The findings revealed significant improvements in both in-vitro and ex-ovo vascularity of the gelatin scaffolds upon the addition of Cu-doped akermanite. The most effective concentrations were determined to be 10 and 20%, which led to notable HUVEC metabolic activity, a well-spread morphology with extensive peripheral filopodia and lamellipodia at 10% and a cobblestone phenotype indicative of in-vivo endothelium at 20% during cell attachment, the formation of complex networks of tubular structures, and robust vascularization in chick embryo development. Moving forward, the incorporation of Cu-doped akermanite into tissue-engineering scaffolds shows great potential for addressing the limitations of vascularization, especially for critical-sized bone defects, by facilitating the controlled release of pro-angiogenic and pro-osteogenic ions.",Materials Science,http://arxiv.org/abs/2512.19055v1,arXiv,2
"In this work, diopside (CaMgSi2O6) was doped with fluoride at a level of 1 mol.%, without the formation of any second phase, by a wet chemical precipitation method. The sintered structure of the synthesized nanopowders was studied by X-ray diffraction, Fourier transform infrared spectroscopy and field-emission scanning electron microscopy. Also, the samples' in vitro apatite-forming ability in a simulated body fluid was comparatively evaluated by electron microscopy, inductively coupled plasma spectroscopy and Fourier transform infrared spectroscopy. According to the results, the material's sinterability was improved by fluoride doping, as realized from the further development of sintering necks. It was also found that compared to the undoped bioceramic, a higher amount of apatite was deposited on the surface of the doped sample. It is concluded that fluoride can be considered as a doping agent in magnesium-containing silicates to improve biological, particularly bioactivity, behaviors.",Materials Science,http://arxiv.org/abs/2512.19014v1,arXiv,2
"Transition metal dichalcogenides (TMDs) enable magnetic property engineering via intercalation, but stoichiometry-structure-magnetism correlations remain poorly defined for Fe-intercalated $\mathrm{NbSe_2}$. Here, we report a systematic study of $\mathrm{Fe}_{x}\mathrm{NbSe_2}$ across an extended composition range $0.05 \le x \le 0.38$, synthesized via chemical vapor transport and verified by rigorous energy-dispersive X-ray spectroscopy (EDS) microanalysis. X-ray diffraction, magnetic, and transport measurements reveal an intrinsic correlation between Fe content, structural ordering, and magnetic ground states. With increasing $x$, the system undergoes a successive transition from paramagnetism to a spin-glass state, then to long-range antiferromagnetism (AFM), and ultimately to a reentrant spin-glass phase, with the transition temperatures exhibiting a non-monotonic dependence on Fe content. The maximum NÃ©el temperature ($T_{\mathrm{N}}$ = $\mathrm{175K}$) and strongest AFM coupling occur at $x=0.25$, where Fe atoms form a well-ordered $2a_0 \times 2a_0 $ superlattice within van der Waals gaps. Beyond $x = 0.25$, the superlattice transforms or disorders, weakening Ruderman-Kittel-Kasuya-Yosida (RKKY) interactions and reducing $T_{\mathrm{N}}$ significantly. Electrical transport exhibits distinct anomalies at magnetic transition temperatures, corroborating the magnetic state evolution. Our work extends the compositional boundary of Fe-intercalated $\mathrm{NbSe_2}$, establishes precise stoichiometry-structure-magnetism correlations, and identifies structural ordering as a key tuning parameter for AFM. These findings provide a quantitative framework for engineering altermagnetic or switchable antiferromagnetic states in van der Waals materials.",Materials Science,http://arxiv.org/abs/2512.18993v1,arXiv,2
"Two-dimensional electron gases (2DEGs) at the surfaces of KTaO3 have become an exciting platform for exploring strong spin-orbit coupling, Rashba physics, and low-carrier-density superconductivity. Yet, a large fraction of reported KTaO3-based 2DEGs has been realized through chemically complex overlayers that both generate carriers and can obscure the native electronic structure, making spectroscopic access to the underlying 2DEG challenging. Here, we demonstrate a simple and direct method to generate a superconducting 2DEG on KTaO3(111) using Mg-induced surface reduction in molecular-beam epitaxy (MBE). Mg has an extremely low sticking coefficient at elevated temperatures, enabling the formation of an ultrathin (less than 1-2 monolayers) MgO layer that is transparent to soft x-ray photoemission spectroscopy (XPS) and angle-resolved photoemission spectroscopy (ARPES). This allows direct measurement of the surface chemistry and low-energy electronic structure of the pristine reduced surface without the need for a several-nanometer-thick capping layer. XPS shows clear reduction of Ta5+ to lower oxidation states, while ARPES reveals a parabolic Ta 5d conduction band with an approximately 150 meV bandwidth and additional subband features arising from quantum confinement. Transport measurements confirm a superconducting transition below 0.6 K. Together, these results demonstrate a chemically straightforward and controllable pathway for fabricating spectroscopically accessible superconducting 2DEGs on KTaO3(111), and provide a powerful new platform for investigating the mechanisms underlying orientation-dependent superconductivity in KTaO3-based oxide interfaces.",Materials Science,http://arxiv.org/abs/2512.18944v1,arXiv,2
"The molecular mechanisms by which organic additives such as saccharin control microstructure in nickel electrodeposition remain inadequately understood, particularly the role of the intense interfacial electric field. This study employs density functional theory (DFT) calculations to elucidate the field dependent adsorption behavior of neutral saccharin and its deprotonated anion (saccharinate) on nickel. By employing the B3LYP functional and implicit solvent models, the field dependent adsorption energetics, frontier orbitals and electrostatic potentials are calculated on a nickel surface. Key findings reveal that while saccharinate dominates in bulk plating baths, its strong solvation shell impedes surface adsorption. In contrast, neutral saccharin exhibits energetically favorable adsorption via sulfonyl oxygen or aromatic $Ï$-face interactions, with specific orientations further stabilized by the interfacial field.This selective adsorption at growth sites rationalizes saccharin's role in inhibiting rapid crystallization, promoting grain refinement, and producing bright, level deposits.The results directly link field-modulated molecular stereochemistry to macroscopic coating properties, providing a mechanistic foundation for the rational design of electroplating additives beyond empirical approaches.",Materials Science,http://arxiv.org/abs/2512.18887v1,arXiv,2
"In many ways the solution to the Hedin equations represents an exact solution to the many body problem. However, for most systems of practical interest, the solution to the Hedin equations is rendered nearly numerically intractable because the Hedin equations are of functional derivative form. Integral equations are much more numerically tractable, than functional derivative equations, as they can often be solved iteratively. In this work we present a systematic set of integral equations (with no functional derivatives) - Hedin approximations I, II, III, IV etc. - whose solutions converge to the solutions of the exact Hedin equations. The Hedin approximations are well suited to iterative numerical solutions (which we also describe). Furthermore Hedin approximation I is just the GW approximation (as such this work may be viewed as a systematic improvement of the GW approximation). We present a systematic study of the Hedin equations for zero dimensional field theory (which, in particular, is a method to enumerate Feynman diagrams for field theories in arbitrary dimensions) and show better and better convergence to the solutions of the Hedin equations for higher and higher Hedin approximations, with Hedin approximations I, II and III being explicitly studied. We, in particular, show that the higher Hedin approximations capture more and and more Feynman diagrams for the self energy. We also show that already Hedin approximation II captures more diagrams than the state of the art diagrammatic vertex corrections approach. Furthermore Hedin approximation III is a near perfect match to the exact solutions of the Hedin equations, at least in the zero dimensional case, and enumerates a large number of Feynman diagrams.",Materials Science,http://arxiv.org/abs/2512.18782v1,arXiv,2
"While phonon topology in crystalline solids has been extensively studied, its influence on thermal transport-especially in nanostructures-remains elusive. Here, by combining first-principles-based machine learning potentials with the phonon Boltzmann transport equation and molecular dynamics simulations, we systematically investigate the role of topological surface phonons in the in-plane thermal transport of semiconductor thin films (Si, 4H -SiC, and c-BN). These topological surface phonons, originating from nontrivial acoustic phonon nodal lines, not only serve as key scattering channels for dominant acoustic phonons but also contribute substantially to the overall thermal conductivity. Remarkably, for these thin semiconductor films below 10 nm this contribution can be as large as over 30% of the in-plane thermal conductivity at 300 K, and the largest absolute contribution can reach 82 W/m-K, highlighting their significant role in nanoscale thermal transport in semiconductors. Furthermore, we demonstrate that both temperature and biaxial strain provide effective means to modulate this contribution. Our work establishes a direct link between topological surface phonons and nanoscale thermal transport, offering the first quantitative assessment of their role and paving the way for topology-enabled thermal management in semiconductors.",Materials Science,http://arxiv.org/abs/2512.18757v1,arXiv,2
"Real-time time-dependent density functional theory (RT-TDDFT) is a powerful approach for investigating various ultrafast phenomena in materials. However, most existing RT-TDDFT studies rely on adiabatic local or semi-local approximations, which suffer from several shortcomings, including the inability to accurately capture excitonic effects in periodic systems. Combining RT-TDDFT with range-separated hybrid (RSH) functionals has emerged as an effective strategy to overcome these limitations. The RT-TDDFT-RSH implementation for periodic systems requires careful treatment of the Coulomb singularity and choosing proper gauges for the incorporation of external fields. We benchmark two schemes for treating the Coulomb singularity - the truncated Coulomb potential and the auxiliary-function correction - and find that the latter shows better convergence behavior and numerical stability for long-range corrected hybrid functions. Additionally, we assess the impact of gauge choice in simulations using numerical atomic orbitals and show that the recently proposed hybrid gauge incorporating position-dependent phases provides a more accurate description of excitonic absorption than the conventional velocity gauge. Our implementation significantly improves the accuracy of RT-TDDFT-RSH for modeling ultrafast excitonic dynamics in periodic systems.",Materials Science,http://arxiv.org/abs/2512.18754v1,arXiv,2
"Water's ability to self-dissociate into H$_3$O$^+$ and OH$^-$ ions is central to acid-base chemistry and bioenergetics. Recent experimental advances have enabled the confinement of water down to the nanometre scale, even to the single-molecule limit, yet how this process is altered at the extreme nanoconfinement remains unclear. Using \emph{ab-initio} calculations and enhanced-sampling machine-learning potential molecular dynamics, we show that monolayer-confined water exhibits a markedly lower barrier to auto-dissociation than bulk water. Confinement restructures both intramolecular bonding and the intermolecular hydrogen-bond network, while enforcing quasi-2D dipolar correlations that amplify dielectric fluctuations. Our results imply that two-dimensional confined water could act as a \emph{superdielectric} medium and may exhibit \emph{superionic} behavior, as observed in recent experiments. These findings reveal confinement as a powerful route to enhanced proton activity, shedding light on geochemical niches, biomolecular environments, and nanofluidic systems where water's chemistry is fundamentally reshaped.",Materials Science,http://arxiv.org/abs/2512.18716v1,arXiv,2
"Machine Learning (ML) driven discovery of novel and efficient thermoelectric (TE) materials warrants experimental TE datasets of high volume, diversity, and quality. While the largest publicly available dataset, Starrydata2, has a high data volume, it contains inaccurate data due to the inherent limitations of Large Language Model (LLM)-assisted data curation, ambiguous nomenclature and complex formulas of materials in the literature. Another unaddressed issue is the inclusion of multi-source experimental data, with high standard deviations and without synthesis information. Using half-Heusler (hH) materials as an example, this work is aimed at first highlighting these errors and inconsistencies which cannot be filtered with conventional dataset curation workflows. We then propose a statistical round-robin error-based data filtering method to address these issues, a method that can be applied to filter any other material property. Lastly, a hybrid dataset creation workflow, involving data from Starrydata2 and manual extraction, is proposed and the resulting dataset is analyzed and compared against Starrydata2.",Materials Science,http://arxiv.org/abs/2512.18653v1,arXiv,2
"Unambiguously identifying unconventional magnetic orders requires probes that are directly sensitive to their momentum-dependent spin-split band structures. Here, we employ a framework based on Zeeman quantum geometry to study magnetotransport at the interface between a magnetic topological insulator and an unconventional magnetic insulator. By choosing the magnetic layer to be insulating, we ensure that the transport response originates solely from the proximity-induced magnetic exchange field, eliminating contributions from itinerant magnetic carriers. We focus on the linear intrinsic gyrotropic magnetic (IGM) response, which naturally decomposes into conduction and displacement current components governed by the Zeeman Berry curvature and the Zeeman quantum metric, respectively. We uncover a universal hierarchy in which the transverse displacement IGM response exhibits characteristic even-fold angular harmonics for magnetic orders ranging from $p$- to $i$-wave, while the longitudinal IGM response distinguishes the parity of the magnetic order through robust sign-reversal patterns. In contrast, the conduction IGM component remains largely insensitive to the underlying magnetic symmetry. Consequently, the displacement IGM current emerges as a high-fidelity symmetry fingerprint of unconventional magnetic order. Using realistic parameter estimates for experimentally accessible heterostructures, we demonstrate that these signatures are well within measurable ranges, establishing Zeeman quantum geometry as a powerful and general framework for characterizing unconventional magnetic insulators via their gyrotropic transport responses.",Materials Science,http://arxiv.org/abs/2512.18621v1,arXiv,2
"Barocaloric (BC) effects at liquid-vapor transitions in hydrofluorocarbons drive most commercial technologies used for heating and cooling in the heating, ventilation and air-conditioning sector. However, these fluids suffer from huge global warming potential and alternative gases are less efficient, toxic or flammable. Solid-solid and solid-liquid BC materials have zero global warming potential and could even improve on current device efficiencies. Whilst solid-liquid BCs typically outperform solid-solid BCs, the latter are advantageous as they avoid leaks and present easier handling and recyclability thus facilitating waste management. Here we confine the solid-liquid BC stearic acid inside the nanopores of a functionalised metal-organic framework (MOF) and demonstrate that the colossal BC properties are retained in a solid-state material. Moreover, the enhanced interactions between the pore surface and the BC material allow a level of active control over the thermal response, as opposed to passive encapsulation. Our results open novel avenues to exploit and tune colossal BC effects in a wide range of combinations of solid-liquid BC materials embedded within functionalized MOFs, without the associated engineering drawbacks.",Materials Science,http://arxiv.org/abs/2512.18541v1,arXiv,2
"Using a combination of band representation analysis, inelastic neutron scattering (INS), magneto-Raman spectroscopy measurements, and linear spin wave theory, we establish that the non-coplanar antiferromagnet MnTe$_2$ hosts symmetry-protected topological nodal lines, Weyl points, and a three-fold degeneracy in its magnon band structure. The non-coplanar nature of the antiferromagnetic ordering protects the topological magnon nodal lines that transition into Weyl magnons upon the application of specific symmetry-breaking perturbations using an external magnetic field. Zero-field INS measurements confirm the existence of the topological magnon nodal lines through the pseudo-spin winding of the scattering intensity in angular scans near the nodal lines, indicating the non-trivial topology of the magnon wavefunctions. This work establishes a clear magnonic analog to Weyl electrons, allowing further exploration of topological behavior in bosonic systems, and highlighting the rich interplay between magnetic order and band topology in non-coplanar antiferromagnets.",Materials Science,http://arxiv.org/abs/2512.18534v1,arXiv,2
"Water nanoconfinement is known to occur inside material void spaces, such as 2D confinement between surfaces, 1D confinement inside nanotubes, and variable-dimension confinement inside nanoporous materials. In the present work we investigate, through molecular dynamics simulations, the morphologies and self-diffusion coefficient of water channels that are nanoconfined in the void space between adjacent surfaces of nanotube bundles - an existing class of materials. In our simulations, we begin with water filling completely the void space, and then we progressively increase the inter-surface separation, maintaining the water content. We find that, as the inter-surface separation progresses, the dimensionality of the water channel decreases from 2D to 1D, the latter consisting of self-confined water channels along surface grooves. The morphologies and self-diffusion coefficients of these 1D water nanochannels are strongly dependent on the nature of the water-surface interaction and on the diameter of the nanotubes. Interestingly, as we decrease the nanotube diameter from 10 to 5 nm, the self-diffusion coefficients of the 1D channels increase by tenfold for hydrophilic surfaces and by sixfold for hydrophobic surfaces, surpassing, in both cases, the bulk water values. We also investigated the water channels at the interstitial voids of the bulk bundle material, finding 1D water channels that are similar to the surface ones.",Materials Science,http://arxiv.org/abs/2512.18514v1,arXiv,2
"We synthezised an AlZnMgCu alloy through mechanical alloying and, using Xray diffraction (XRD), identified the formation of the eta prime phase after 40 hours of grinding. Using reaction-free isoconversion theory, we determined that this phase exhibits two different behaviours depending on the heating rate (beta): the eta prime phase at low beta and the eta phase at high beta.The activation energy values at low beta are consistent with the diffusion energies of copper, zinc and magnesium in aluminium. Significant qualitative and quantitative differences in the thermodynamic barriers (Delta H, Delta G and Delta S) are observed for beta values above or below 20 deg C min minus 1.",Materials Science,http://arxiv.org/abs/2512.18507v1,arXiv,2
"Microscale additive manufacturing of reflective copper is becoming increasingly important for microelectronics and microcomputers, due to its excellent electrical and thermal conductivity. Yet, it remains challenging for state-of-the-art commercial metal 3D printers to achieve sub-100-micron manufacturing. Two aspects are sub-optimal using commercial laser powder bed fusion systems with infrared (IR) lasers (wavelength of 1060-1070 nm): (1) IR laser has a low absorption rate for Cu, which is energy-inefficient for manufacturing; (2) short wavelength lasers can potentially offer higher resolution processing due to the diffraction-limited processing. On the other hand, laser sintering or melting typically uses continuous wave (CW) lasers, which may reduce the manufacturing resolution due to a large heat-affected zone. Based on these facts, this study investigates the UV (wavelength of 355 nm) nanosecond (ns) laser sintering of Cu nanoparticles. Different laser processing parameters, as well as different nanoparticle packing densities, are studied. Our results show that a short-wavelength laser can reduce the required energy for sintering with decent morphology, and a densified nanoparticle powder bed favors continuous melting. We further show that sub-20 micron printing can be readily achieved with a UV ns laser. These findings provide new insights into short-wavelength laser-metal nanoparticle interactions, which may pave the way to achieve high-resolution micro and nano-scale additive manufacturing.",Materials Science,http://arxiv.org/abs/2512.18465v3,arXiv,2
"We develop a general multipolar theory of strong spin-orbit coupling for large total angular momentum $j$ in time-reversal-symmetric, noncentrosymmetric crystals. Using a $j\in\{1/2,3/2,5/2\}$ multiplet basis appropriate for heavy-element \textit{p}- and \textit{d}-bands, we systematically construct all symmetry-allowed spin-orbit coupling terms up to fifth order in momentum and generalize the usual spin texture to a total-angular-momentum texture. For $j>1/2$, multipolar spin-orbit coupling qualitatively reshapes Fermi surfaces and makes the topology of Bloch states band dependent. This leads to anisotropic high-$j$ textures that go beyond a single Rashba helix. We classify these textures by their total-angular-momentum vorticity $W_{n}$ for every energy band and identify distinct $|W_{n}|=1,2,5$ phases. We show that their crossovers generate enhanced and nonmonotonic current-induced spin-polarization responses, namely the Edelstein effect, upon tuning the chemical potential. Our results provide a symmetry-based framework for analyzing and predicting multipolar spin-orbit coupling, total-angular-momentum textures, and spintronic responses in heavy-element materials without an inversion center.",Materials Science,http://arxiv.org/abs/2512.18449v1,arXiv,2
"Amorphous oxide tunneling barriers, primarily formed from aluminum, represent one of the most widely adopted platforms for superconducting quantum bits (qubits). To overcome challenges associated with defects and sample variance among the tunneling barriers, the methodology of alternating bias assisted annealing (ABAA) was introduced in Pappas et. al[1]. The process of applying alternating bias to the barrier and subsequently aging before use was shown to reduce defects in the barrier. Namely, defects that give rise to two-level systems, coupling to the qubit and expediting decoherence. In this work we replicate an expedited ABAA process through a combination of ab-initio molecular dynamics and machine-learned potentials, illuminating how ABAA effects the energy landscape of the barrier.",Materials Science,http://arxiv.org/abs/2512.18420v1,arXiv,2
"The emergence of the orbital degree of freedom in modern orbitronics offers a promising alternative to heavy metals for the efficient control of magnetization. In this context, identifying interfaces that exhibit orbital-momentum locking and an orbital Rashba-Edelstein response to an external electric field is of primary importance. In this work, we experimentally investigate the Co/Al system and extend the study to Co/Pt/Al structures. We show that inserting ultrathin Pt layers between Co and Al can significantly modify the orbital properties, highlighting the critical role of Co/Al orbital bonding in generating orbital polarization. We further model the orbital response of these systems using semi-phenomenological approaches and linear-response theory within the framework of density-functional theory.",Materials Science,http://arxiv.org/abs/2512.18419v1,arXiv,2
"Computing spectral functions in large, non-periodic super-moirÃ© systems remains an open problem due to the exceptionally large system size that must be considered. Here, we establish a tensor network methodology that allows computing momentum-resolved spectral functions of non-interacting and interacting super-moirÃ© systems at an atomistic level. Our methodology relies on encoding an exponentially large tight-binding problem as an auxiliary quantum many-body problem, solved with a many-body kernel polynomial tensor network algorithm combined with a quantum Fourier transform tensor network. We demonstrate the method for one and two-dimensional super-moirÃ© systems, including super-moirÃ© with non-uniform strain, interactions treated at the mean-field level, and quasicrystalline super-moirÃ© patterns. Furthermore, we demonstrate that our methodology allows us to compute momentum-resolved spectral functions restricted to selected regions of a super-moirÃ©, enabling direct imaging of position-dependent electronic structure and minigaps in super-moirÃ© systems with non-uniform strain. Our results establish a powerful methodology to compute momentum-resolved spectral functions in exceptionally large super-moirÃ© systems, providing a tool to directly model scanning twisting microscope tunneling experiments in twisted van der Waals heterostructures.",Materials Science,http://arxiv.org/abs/2512.18397v2,arXiv,2
"Quasicrystals, structures that are ordered yet aperiodic, defy conventional band theory, confining most studies to finite-size real-space numerics. We overcome this limitation with a configuration-space framework that predicts and explains the positions and origins of energy gaps in quasicrystalline potentials. We find that a hierarchy of gaps stems from resonant hybridization between increasingly distant neighboring sites, pinning the integrated density of states below these gaps to specific irrational areas in configuration space. Large-scale simulations of a lowest-band tight-binding model built from localized Wannier functions show excellent agreement with these predictions. By moving beyond finite-size numerics, this study advances the understanding of quasicrystalline potentials, paving the way for new explorations of their quantum properties in the infinite-size limit.",Materials Science,http://arxiv.org/abs/2512.18328v1,arXiv,2
"Thin amorphous-CoFeB (a-CFB) is deposited by rf-magnetron sputtering on a self-oxidized Si (100) substrate with different film thicknesses ranging from 0.7 nm to 20 nm. The 5-nm-thick a-CFB film is capped with a W layer for comparison. The surface morphology is investigated by using the atomic force microscopy technique. The low roughness of all the surface of the film indicates uniformity, moderate corrosion resistance, and good structural quality. The X-ray diffraction spectra reveal the amorphous nature of the CFB layer, while the W capping is of mixed phase in the experimental thickness regime. In-plane and out-of-plane hysteresis loops obtained from the vibrating sample magnetometry technique show a transition from an upright S to nearly rectangular shape via a completely inverted profile. A self-sustained tilted magnetic anisotropy is stabilized in a seed-free environment based on the direct substrate-to-magnet interaction. The interface anisotropy is estimated to be 0.06 erg/cm2. The complex anisotropic behavior originates from the interplay between interface anisotropy, conventional shape anisotropy, growth-induced anisotropies, and inhomogeneity-induced anisotropies. In essence, effective anisotropy is responsible for the anomalous hysteresis behavior observed in these films, and this work might provide valuable insights to improve the functionalities of amorphous soft magnetic alloys.",Materials Science,http://arxiv.org/abs/2512.18316v1,arXiv,2
"This paper provides a construction and existence proof for a 1-parameter family of chiral unbalanced triply-periodic minimal surfaces of genus 4. We name these {\textit{gyrating H'-T} surfaces, because they are related to Schoen's H'-T surfaces in a similar way as the Gyroid is to the Primitive surface. Their chirality is manifest in a screw symmetry of order six. The two labyrinthine domains on either side of the surface are not congruent, rather one representing the quartz net (\texttt{qtz}) and the other one the dual of the quartz net (\texttt{qzd}). The family tends to the Scherk saddle tower in one limit and to the doubly periodic Scherk surface in the other. The motivation for the construction was to construct a chiral tunable unbalanced surface family, originally as a template for photonic materials. The numeric construction is based on reverse-engineering of the tubular surface of two suitably chosen dual nets, using the \textit{Surface Evolver}} to minimize area or curvature variations. The existence is proved using Weierstrass parametrizations defined on the branched torus.",Materials Science,http://arxiv.org/abs/2512.18308v1,arXiv,2
"We employ the Dynamical Projective Operatorial Approach (DPOA) to investigate the ultrafast optical excitations of germanium under intense, ultrashort pump pulses. The method has very low resource demand relative to many other available approaches and enables detailed calculation of the residual electron and hole populations induced by the pump pulse. It provides direct access to the energy distribution of excited carriers and to the total energy transferred to the system. By decomposing the response into contributions from different multi-photon resonant processes, we systematically study the dependence of excited-carrier density and absorbed energy on key pump-pulse parameters: duration, amplitude, and photon energy. Our results reveal a complex interplay between these parameters, governed by resonant Rabi-like dynamics and competition between different multi-photon absorption channels. For the studied germanium setup, we find that two-photon processes are generally dominant, while one- and three-photon channels become significant under specific conditions of pump-pulse frequency, duration, and intensity. This comprehensive analysis offers practical insights for optimizing ultrafast optical control in semiconductors by targeting specific multi-photon pathways.",Materials Science,http://arxiv.org/abs/2512.18299v1,arXiv,2
"First-principles studies were performed on two Mn-based ferrimagnetic Heusler compounds with L21 and B2 structures, that is, Mn2VZ (Z = Al or Ga). The aim was to investigate their magnetic properties, electronic structures, and spin-resolved longitudinal conductivity at finite temperatures. Density functional theory (DFT) and functional integral theory were used. This approach incorporates transverse spin fluctuations through a disordered local moment method and the coherent potential approximation. In all cases, the calculated theoretical Curie temperatures were lower than the experimental values. Alloys with a B2 structures exhibit higher Curie temperatures compared to compounds with an L21 structures. Calculations of the temperature dependence of the density of states (DOS) indicate that the half-metallic electronic structure collapses owing to the renormalization of transverse spin fluctuations at a finite temperatures. However, the spin-resolved longitudinal conductivities demonstrated an improved spin polarization, particularly for Mn2VGa with an L21 structure. This result contradicts predictions based on the temperature-dependent DOS. The competition between the metallic transitions, which are caused by a modification of the DOS, and scattering coming from spin-disorder explains this phenomenon. Both of these effects are induced by transverse spin fluctuations. Additionally, the results show that half-metallicity, as defined by the DOS or conductivity, is inconsistent at finite temperatures. Finally, the total energy landscape of the paramagnetic state was calculated using the fixed spin moment method to investigate the strength of the longitudinal spin fluctuations. These results suggest that the alloys may exhibit strong longitudinal spin fluctuations.",Materials Science,http://arxiv.org/abs/2512.18270v1,arXiv,2
"The foundations of irradiation damage theory were laid in the 1950s and 60s within the framework of chemical reaction kinetics. While helpful to analyze qualitative aspects of irradiation damage, the theory contained gaps that delayed its implementation and applicability as a predictive tool. The advent of computer simulations with atomistic resolution in the 80s and 90s revealed a series of mechanisms that have proved essential to understand key aspects of irradiation damage in crystalline solids. However, we still lack a comprehensive model that can connect atomic-level defect physics with experimental measurements of quantitative features of the irradiated microstructure. In this work we present a mesoscale model that draws from our improved understanding of irradiation damage processes collected over the last few decades, bridging knowledge gained from our most sophisticated atomistic simulations with defect kinetics taking place over time scales many orders of magnitude larger than atomic interaction times. Importantly, the model contains no adjustable parameters, and combines several essential pieces of irradiation damage physics, each playing an irreplaceable role in the context of the full model, but of limited utility if considered in isolation. Crucially, we carry out a set of experiments carefully designed to isolate the key irradiation damage variables and facilitate validation. Using tungsten as a model material, we find exceptionally good agreement between our numerical predictions and experimental measurements of defect densities and defect cluster sizes.",Materials Science,http://arxiv.org/abs/2512.18258v1,arXiv,2
"Magnetic refrigeration presents an energy-efficient and environmentally benign alternative to traditional vapour-compression cooling technologies. It relies on the magnetocaloric effect, in which the temperature of a magnetic material changes in response to variations in an applied magnetic field. Optimal magnetocaloric materials are characterized by a significant change in magnetic entropy under moderate magnetic field. In this study, we systematically investigated the inter-atomic exchange interactions, magnetic anisotropy energy and magnetocaloric properties of MnX (X = N, P, As, Sb, Bi) using a combination of density functional theory and Monte-Carlo simulations. Additionally, the magneto-optical Kerr and Faraday spectra were computed using the all-electron, fully relativistic, full-potential linearized muffin-tin orbital method. The largest Kerr effect observed in MnBi can be inferred as a combined effect of maximal exchange splitting of Mn 3d states and the large spin-orbit coupling of Bi. To extract site-projected spin and orbital moments, spin-orbit coupling and orbital polarization correction are accounted in the present calculation, which shows good agreement between the moment obtained from the X-ray magnetic circular dichroism sum rule analysis, spin-polarized calculation, and experimental studies. The magnetic transition temperatures predicted through Monte-Carlo simulations were in good agreement with the corresponding experimental values. Our results provide a unified microscopic understanding of magnetocaloric performance and magneto-optical activity in Mn-based pnictides and establish a reliable computational framework for designing next-generation magnetic refrigeration materials.",Materials Science,http://arxiv.org/abs/2512.18253v1,arXiv,2
"Crystal structure prediction is a fundamental problem in materials science. We present CrystalFormer-CSP, an efficient framework that unifies data-driven heuristic and physics-driven optimization approaches to predict stable crystal structures for given chemical compositions. The approach combines pretrained generative models for space-group-informed structure generation and a universal machine learning force field for energy minimization. Reinforcement fine-tuning can be employed to further boost the accuracy of the framework. We demonstrate the effectiveness of CrystalFormer-CSP on benchmark problems and showcase its usage via web interface and language model integration.",Materials Science,http://arxiv.org/abs/2512.18251v1,arXiv,2
"Strong light-matter coupling enables hybrid states in which photonic and electronic degrees of freedom become correlated even in the ground state. While many-body effects in long-range dispersion interactions are known to reshape electronic properties under such conditions, their impact on quantum-optical observables remains largely unexplored. Here, we address this problem using quantum electrodynamical density-functional theory (QEDFT) combined with the recently developed photon-many-body dispersion (pMBD) functional, which can capture higher-order electron-photon correlations and multi-photon processes. We compute ground-state photonic observables including photon number fluctuations, second-order correlations, and quadrature variances, and find squeezing and super-Poissonian photon statistics emerging from light-matter interactions in the strong coupling regime. Our results demonstrate that capturing the full hierarchy of many-body, electron-photon and multi-photon correlations is essential for a consistent description of quantum-optical properties in strongly coupled molecular systems, establishing QEDFT as a first-principles framework for predicting nonclassical photonic features in the ground state of complex systems.",Materials Science,http://arxiv.org/abs/2512.18242v1,arXiv,2
"Material scientists and condensed matter physicists have long been divided on the issue of choosing the conceptual framework for explaining why open-shell transition-metal oxides tend to be insulators, whereas otherwise successful theories such as DFT often predict them to be (false) metals. Strong correlation becomes the recommended medicine. We point out that strong correlation can be mitigated by allowing DFT to lower the energy by breaking structural, magnetic or dipolar symmetries. Such local motifs are observed experimentally by local probes beyond the 'average structure' determined by X-Ray diffraction. Observed broken symmetries can arise from slow fluctuations that persist over the observation time or longer. The surprising fact is that when symmetry breaking motifs are used as input to electronic structure calculations, false metals are converted into real insulators without the recommended medicine of strong correlation. Consistently, DFT calculations that show energy lowering symmetry breaking correct most cases where DFT, even with advanced exchange-correlation functionals, previously missed the correct metal vs insulator designation. Total energy calculations distinguish systems that support energy-lowering symmetry breaking from those that do not. This approach distinguishes between paramagnetic insulating and metallic phases and shows mass enhancement in Mott metals. The reason is that symmetry breaking removes many of the degeneracies that exist in a symmetry-unbroken system, reducing significantly the need for strong correlation. If one chooses to ignore symmetry breaking, the persistent degeneracies often call for strong correlation treatment. Thus, symmetry breaking transforms strong to normal correlation and false metals to true insulators. This view sheds light on the historic controversy between Mott and Slater that still reverberates today.",Materials Science,http://arxiv.org/abs/2512.18236v1,arXiv,2
"Antiperovskite nitrides (X3AN) are the structural analogues to perovskite oxides, while their epitaxial growth and electronic properties remain largely unexplored. We report the successful synthesis of Ni3InN thin films on substrates with different lattice constants. First-principles phonon calculations confirm the dynamical stability of cubic phase Ni3InN, providing the basis for epitaxial synthesis. High-resolution scanning transmission electron microscopy reveals coherent (001)-oriented interfaces when Ni3InN is grown on LaAlO3 and SrTiO3, while an unexpected (011)-orientation forms on DyScO3, aligning with surface-energy predictions. Transport measurements highlight a strain-controlled Fermi-liquid behavior, correlated with variations in the Ni-3d bandwidth and hybridization. Band structure calculations reveal a dual character near the Fermi level: a high-mobility Dirac-like band and a Ni-3d manifold that drives strange-metal transport with a reduced slope compared to oxide perovskites. The formal Ni valence (+2/3) places Ni3InN in an overdoped correlated-metal regime, distinguishing from most perovskite oxides. This positions antiperovskite nitrides as a promising platform for investigating overdoped Fermi liquids and strange-metal behavior.",Materials Science,http://arxiv.org/abs/2512.18195v1,arXiv,2
"Twistronics, which exploits moire modulation of lattice and electronic structures in twisted bilayers, has emerged as a powerful approach to engineer novel quantum states. Recent efforts have expanded beyond two dimensional van der Waals (vdWs) crystals to more complex, strongly correlated materials, where interfacial moire effects can dominate physical properties. Here we demonstrate a generalizable route to fabricate twisted bilayers of transition metal nitrides with vdWs like interfaces, using freestanding CrN membranes as a model system. Twisted bilayer CrN (tCrN) is realized by employing cubic alkaline earth metal monoxides as sacrificial layers, enabling the assembly of clean, controllable interfaces. Electron ptychography reveals well defined, periodic square moire superlattices in tCrN. For a twist angle of 16.3 degree, we identify a nearly commensurate moire lattice with coincident Cr columns, whereas at 45 degree we uncover localized octagonal quasicrystalline order with clear self-similarity. These results establish a practical platform for twisted TMNs and open avenues to explore moire-induced atomic configurations and emergent correlated phenomena in nitride based heterostructures.",Materials Science,http://arxiv.org/abs/2512.18193v1,arXiv,2
"High-entropy materials have emerged as a promising class of catalysts, driven by their high configurational entropy originating from structural disorder in single-phase multicomponent systems. Despite their potential, the catalytic performance of high-entropy oxides (HEOs) remains relatively underexplored. In this study, we present a simple solution-based combustion route to synthesize two low-cost, transition metal-rich multicationic oxides positioned in the medium-entropy (HEO-4) and high-entropy (HEO-5) regimes. Rietveld refinement of powder X-ray diffraction data confirmed single-phase formation with a face-centered cubic (fcc) crystal structure for both nanostructures.   The morphology, particle size, and multicationic elemental distribution were investigated using scanning and transmission electron microscopy. The catalytic performance of the synthesized HEOs was evaluated in the hydrogenation of a series of nitrophenol derivatives. Notably, HEO-5 exhibited significantly enhanced catalytic activity ($k_{\mathrm{app}} \approx 0.5~\mathrm{min^{-1}}$, TOF $= 2.1 \times 10^{-3}~\mathrm{mol\,g^{-1}\,s^{-1}}$), achieving rapid conversion of \emph{p}-nitrophenol compared to the medium-entropy oxide nanostructures ($k_{\mathrm{app}} \approx 0.02~\mathrm{min^{-1}}$, TOF $= 7.2 \times 10^{-4}~\mathrm{mol\,g^{-1}\,s^{-1}}$). Furthermore, the kinetic and thermodynamic parameters of the reaction, including the activation energy ($E_a$), enthalpy of activation ($ÎH^{\ddagger}$), Gibbs free energy of activation ($ÎG^{\ddagger}$), and entropy of activation ($ÎS^{\ddagger}$), were determined to gain mechanistic insight into the reduction process. This study opens new avenues for the rational design and facile synthesis of high-entropy oxide catalysts, highlighting their potential for efficient and sustainable large-scale amine production.",Materials Science,http://arxiv.org/abs/2512.18191v1,arXiv,2
"A minimal model of ionic aggregation in concentrated ZnCl$_2$ is developed, guided by molecular dynamics simulations with a machine-learned potential. It explicitly incorporates solvent-site depletion, correlated chloride binding, and allows for loops within Zn-Cl clusters. Dehydration is shown to drive ion binding through two sharp transitions set by the Zn coordination number $Z$: a crossover at $Z=2$ from isolated ions to Cl-bridged clusters, and gelation near $Z\approx 3$. The model agrees quantitatively with MD results, and the critical exponent of the cluster-size distribution matches percolation theory.",Materials Science,http://arxiv.org/abs/2512.18167v1,arXiv,2
"We present a lattice-renormalized formalism for configurational tunneling two-level systems (TLS) that overcomes limitations of minimum-energy-path and light-particle models. Derived from the nuclear Hamiltonian, our formulation introduces composite phonon coordinates to capture lattice distortions between degenerate potential wells. This approach resolves deficiencies in prior models and enables accurate computation of tunnel splittings and excitation spectra for hydrogen-based TLS in bcc Nb. Our results bound experimental tunnel splittings and reveal strong anharmonic couplings between tunneling atoms and lattice phonons, establishing a direct link between TLS dynamics and phonon-mediated strain interactions. The formalism further generalizes to multi-level systems (MLS), providing insight into defect-induced decoherence in superconducting qubits and guiding strategies for materials design to suppress TLS-related loss.",Materials Science,http://arxiv.org/abs/2512.18156v1,arXiv,2
"The Coulomb potential at an interior ion in a finite crystal of size $p$ is given by a linear superposition of contributions from displacement vectors ${\mathbf r}=(x,y,z)$ to its neighbors. This additive structure underlies universal relationships among Madelung constants and applies to both standard periodic boundary conditions and alternative Clifford supercells. Each pairwise contribution decomposes into three physically distinct components: a periodic bulk term, a quadratic boundary term, and a finite-size correction whose leading order term is $[24r^4-40(x^4+y^4+z^4)]/[9\sqrt{3} (2p+1)^2]$ for cubic crystals with unit lattice constant. Combining this decomposition with linear superposition yields a rapidly convergent direct-summation scheme, accurate even at $p=1$ ($3^3$ unit cells), enabling hands-on calculations of Madelung constants for a wide range of ionic crystals.",Materials Science,http://arxiv.org/abs/2512.18138v1,arXiv,2
"Aleatoric uncertainties - irremovable variability in microstructure morphology, constituent behavior, and processing conditions - pose a major challenge to developing uncertainty-robust digital twins. We introduce the Variational Deep Material Network (VDMN), a physics-informed surrogate model that enables efficient and probabilistic forward and inverse predictions of material behavior. The VDMN captures microstructure-induced variability by embedding variational distributions within its hierarchical, mechanistic architecture. Using an analytic propagation scheme based on Taylor-series expansion and automatic differentiation, the VDMN efficiently propagates uncertainty through the network during training and prediction. We demonstrate its capabilities in two digital-twin-driven applications: (1) as an uncertainty-aware materials digital twin, it predicts and experimentally validates the nonlinear mechanical variability in additively manufactured polymer composites; and (2) as an inverse calibration engine, it disentangles and quantitatively identifies overlapping sources of uncertainty in constituent properties. Together, these results establish the VDMN as a foundation for uncertainty-robust materials digital twins.",Materials Science,http://arxiv.org/abs/2512.18104v1,arXiv,2
"Sb2S3 is a promising material for low-toxicity, high-stability next-generation photovoltaics. Despite high optical limits in efficiency, progress in improving its device performance has been limited by severe voltage losses. Recent spectroscopic investigations suggest that self-trapping occurs in Sb2S3, limiting the open-circuit voltage (Voc) to a maximum of approximately 800 mV, which is the level the field has asymptotically approached. In this work, we surpass this voltage barrier through reductions in the defect density in Sb2S3 thin films by modulating the growth mechanism in chemical bath deposition using citrate ligand additives. Deep level transient spectroscopy identifies two deep traps 0.4-0.7 eV above the valence band maximum, and, through first-principles calculations, we identify these to likely be S vacancies, or Sb on S anti-sites. The concentrations of these traps are lowered by decreasing the grain boundary density from 1114+/-52 nm/um2 to 585+/-10 nm/um2, and we achieve a Voc of 824 mV, the record for Sb2S3 solar cells. This work addresses the debate in the field around whether Sb2S3 is limited by defects or self-trapping, showing that it is possible to improve the performance towards the radiative limit through careful defect engineering.",Materials Science,http://arxiv.org/abs/2512.18100v1,arXiv,2
"The lack of long-range electrostatics is a key limitation of modern machine learning interatomic potentials (MLIPs), hindering reliable applications to interfaces, charge-transfer reactions, polar and ionic materials, and biomolecules. In this Perspective, we distill two design principles behind the Latent Ewald Summation (LES) framework, which can capture long-range interactions, charges, and electrical response just by learning from standard energy and force training data: (i) use a Coulomb functional form with environment-dependent charges to capture electrostatic interactions, and (ii) avoid explicit training on ambiguous density functional theory (DFT) partial charges. When both principles are satisfied, substantial flexibility remains: essentially any short-range MLIP can be augmented; charge equilibration schemes can be added when desired; dipoles and Born effective charges can be inferred or finetuned; and charge/spin-state embeddings or tensorial targets can be further incorporated. We also discuss current limitations and open challenges. Together, these minimal, physics-guided design rules suggest that incorporating long-range electrostatics into MLIPs is simpler and perhaps more broadly applicable than is commonly assumed.",Materials Science,http://arxiv.org/abs/2512.18029v1,arXiv,2
"We review recent advances in Klein and anti-Klein tunneling in one- and two-dimensional materials. Using a general tight-binding framework applied to multiple periodic systems, we establish the criteria for the emergence of Klein tunneling based on the conservation of an effective reduced pseudospin. The inclusion of higher-order terms in the wave vector leads to nontrivial matching conditions for wave scattering at interfaces. We further examine the emergence of multiple types of Klein tunneling in two-dimensional materials beyond graphene, including phosphorene and borophene, as well as in one-dimensional systems such as Su-Schrieffer-Heeger lattices. Finally, we discuss how these tunneling phenomena can be tested in both synthesized and artificial lattices, including elastic metamaterials, optical, photonic, phononic, and superconducting platforms, demonstrating the universality of Klein tunneling across different wave natures and length scales.",Materials Science,http://arxiv.org/abs/2512.18026v1,arXiv,2
"Scintillating molecular crystals have emerged as prime candidates for directional dark matter detector targets. This anisotropy makes them exquisitely sensitive due to the daily modulation induced by the directional dark matter wind. However, predicting the interaction rate for arbitrary molecules requires accurate modeling of the many-body ground as well as excited states, a task that has been historically computationally expensive. Here, we present a theory and computational framework for efficiently computing dark matter scattering form factors for molecules. We introduce SCarFFF, a GPU-accelerated code to compute the fully three-dimensional anisotropic molecular form factor for arbitrary molecules. We use a full time-dependent density functional theory framework to compute the lowest-lying singlet excited states, adopting the B3YLP exchange functional and a double-zeta Gaussian basis set. Once the many-body electronic structure is computed, the form factors are computed in a small fraction of the time from the transition density matrix. We show that ScarFFF can compute the first 12 form factors for a molecule of 10 heavy atoms in approximately 5 seconds, opening the door to accurate, high-throughput material screening for optimal directional dark matter detector targets. Our code can perform the calculation in three independent ways, two semi-analytical and one fully numeric, providing optimised methods for every precision goal.",Materials Science,http://arxiv.org/abs/2512.18010v1,arXiv,2
"High-temperature and high-magnetic-field-induced re-entrant superconductivity has been discovered in the infinite-layer nickelate $\mathrm{Sm_{1-x-y} Eu_x Ca_y Ni O_2}$ (SECNO). Infinite-layer nickelates are the closest known analogues of high-$\mathrm{T}_c$ cuprate superconductors, yet they host distinct magnetic ground states. Using low-energy muon spin relaxation and polarized neutron reflectometry, we reveal the magnetic order in SECNO. We find that magnetic freezing occurs at a higher-temperature than in other nickelate compounds, and that a substantial net magnetization of 55 $\,\mathrm{kA}\,\mathrm{m}^{-1}$ $\pm10 \,\mathrm{kA}\,\mathrm{m}^{-1}$ emerges and remains largely unchanged across the superconducting transition. The magnetism in SECNO is disordered and nonuniform.",Materials Science,http://arxiv.org/abs/2512.18005v1,arXiv,2
"Understanding the fundamental properties that dictate photoexcited polarons in materials is critical to tuning their properties. Theoretical models of polarons have only recently been extended to the excited state. Experimental measurements of polaron formation and transport have been widely undertaken across a range of materials, from photocatalysts and superconductors to soft conducting polymers. Here, we map experimental measurements of quantities such as polaron strength onto phase diagrams of the Holstein, Hubbard-Holstein, and t-J-Holstein models. This work demonstrates that tuning electron-phonon coupling strength, electron localization, and spin exchange can be leveraged to suppress or control polaron formation in transition metal oxides. We find that the t-J-Holstein model best describes the measured iron oxides and could be generally applied to a wide range of systems that exhibit polaron formation in the excited state. This work combines experimental data with ground state models to provide a robust parameter space for informing photoexcited polaron design.",Materials Science,http://arxiv.org/abs/2512.17869v1,arXiv,2
"We carry out temperature-dependent scanning tunneling microscopy (STM) studies of the charge density wave (CDW) compound ZrTe$_3$ which is intentionally doped with Hf. Previous bulk studies tie Hf doping to an enhancement of the CDW transition temperature (T$_{CDW}$). In our work, by combining STM measurements with density functional theory (DFT) calculations, we observe and identify multiple defects in Zr$_{0.95}$Hf$_{0.05}$Te$_3$. Surprisingly, instead of finding clear structural or electronic signatures associated with Hf dopants, we determine the origin of the observed defects are consistent with Te and Zr vacancies. Further, our temperature dependent STM measurements allow us to examine CDW pinning to both types of observed defects below and above T$_{CDW}$.",Materials Science,http://arxiv.org/abs/2512.17867v1,arXiv,2
"Materials performance is deeply linked to their microstructures, which govern key properties such as strength, durability, and fatigue resistance. EBSD is a major technique for characterizing these microstructures, but acquiring large and statistically representative EBSD maps remains slow, costly, and often limited to small regions. In this work, we introduce InfinityEBSD, a diffusion-based method for generating monophase realistic EBSD maps of arbitrary size, conditioned on physically meaningful microstructural metrics. This approach supports two primary use cases: extending small experimental EBSD maps to arbitrary sizes, and generating entirely new maps directly from statistical descriptors, without any input map. Conditioning is achieved through eight microstructural descriptors, including grain size, grain perimeter, grain inertia ratio, coordination number and disorientation angle distribution, allowing the model to generate maps that are both visually realistic and physically interpretable. A patch-wise geometric extension strategy ensures spatial continuity across grains, enabling the model to produce large-scale EBSD maps while maintaining coherent grain boundaries and orientation transitions. The generated maps can also be exported as valid Channel Text Files (CTF) for immediate post-processing and analysis in software such as MTEX or simulation environments like DIGIMU. We quantitatively validate our results by comparing distributions of the guiding metrics before and after generation, showing that the model respects the statistical targets while introducing morphological diversity. InfinityEBSD demonstrates that diffusion models, guided by physical metrics, can bridge the gap between synthetic and realistic materials representation, paving the way for future developments such as 3D realistic microstructure generation from 2D data.",Materials Science,http://arxiv.org/abs/2512.17859v1,arXiv,2
"Local rearrangements are the elements of plastic deformation in an amorphous solid. In oscillatory shear, they can switch reversibly between two distinct configurations. While these repeating relaxations are typically considered in the limit of slow driving, their dynamics is less well understood. We perform experiments on a colloidal amorphous solid at an oil-water interface. The rearrangement timescales we observe span at least 1 decade, with no apparent upper bound. As frequency is increased, individual rearrangements appear faster and more hysteretic, but may disappear entirely above a crossover frequency -- suggesting that in practical experiments, the slowest rearrangements may be latent. We show how to find the effective potential energy that reproduces a particle's frequency-dependent motion. In rare cases, this potential energy has only one minimum. Our results have implications for the energy landscapes and rheology of amorphous or glassy solids, for sound propagation in nonlinear media, and for mechanical memory and history-dependence.",Materials Science,http://arxiv.org/abs/2512.17816v1,arXiv,2
"Temperature-dependent transport data, including diffusion coefficients and ionic conductivities, are routinely analysed by fitting empirical models such as the Arrhenius equation. These fitted models yield parameters such as the activation energy, and can be used to extrapolate to temperatures outside the measured range. Researchers frequently face challenges in this analysis: quantifying the uncertainty of fitted parameters, assessing whether the data quality is sufficient to support a particular empirical model, and using these models to predict behaviour at extrapolated temperatures. Bayesian methods offer a coherent framework that addresses all of these challenges. This tutorial introduces the use of Bayesian methods for analysing temperature-dependent transport data, covering parameter estimation, model selection, and extrapolation with uncertainty propagation, with illustrative examples from molecular dynamics simulations of superionic materials.",Materials Science,http://arxiv.org/abs/2512.17792v1,arXiv,2
"Donor-$Ï$-acceptor (D-$Ï$-A) functionalization of MOF linkers can enhance visible-light photocatalytic activity, yet the mechanisms responsible for these effects remain unclear. Here we combine EPR spectroscopy, transient photoluminescence, and first-principles calculations to examine how diazo-coupled anisole, diphenylamine (DPA), and N,N-dimethylaniline (NNDMA) groups modify the photophysics of UiO-66-NH$_2$. All donor units introduce new occupied states near the valence-band edge, enabling charge separation through dye-to-framework electron transfer. Among them, the anisole-modified material stands out for facilitating efficient intersystem crossing into a triplet charge-transfer configuration that suppresses fast recombination and yields long-lived charge carriers detectable by photo-EPR. Meanwhile, bulkier donors such as DPA and NNDMA - despite their stronger electron-donating character - also tend to introduce defect-associated trap states. These results underscore the interplay between donor-induced electronic-structure changes, triplet pathways, and defect-mediated recombination, offering a mechanistic basis for tuning photocatalytic response in D-$Ï$-A-modified MOFs.",Materials Science,http://arxiv.org/abs/2512.17778v1,arXiv,2
"Structure and morphology play a crucial role in enhancing the biomimetic oxidase activity of nanozymes. In this study, a facile \emph{in situ} chemical oxidation strategy was employed to synthesize MOF-derived MnO$_x$, utilizing the structural features of the parent MOF to enhance oxidase-mimicking activity. We systematically investigated the effects of phase evolution, structural modulation, and morphology on the oxidase activity of MnO$_x$ with Fe substitution. The oxidase-like activity was evaluated using the chromogenic substrate 3,3$'$,5,5$'$-tetramethylbenzidine (TMB), which produced a blue-colored oxidized TMB (ox-TMB) with an absorption peak at 652~nm upon oxidation. While all Fe-doped MnO$_x$ nanostructures exhibited oxidase-like activity, the 10\% Fe-doped sample (10Fe-MnO$_x$) demonstrated the highest performance, likely due to a synergistic effect of structure, morphology, and the presence of oxygen vacancies. The underlying oxidase mechanism was investigated using steady-state kinetics and electron paramagnetic resonance (EPR) analysis. In addition, a colorimetric assay was developed for the detection of Hg$^{2+}$ and hydroquinone (HQ) in real water samples collected from industrial and natural sources. The calculated detection limits of the 10Fe-MnO$_x$ colorimetric probe for HQ (1.74~$Î¼$M) and Hg$^{2+}$ (0.47~$Î¼$M) outperformed those of conventional metal oxide-based nanozymes. These findings pave the way for the development of easily synthesizable, scalable, and highly sensitive oxidase-based MOF-derived metal oxide nanomaterials with significant potential in biological and environmental applications.",Materials Science,http://arxiv.org/abs/2512.17731v1,arXiv,2
"We develop highly corrosion-resistant and conductive Ti-Nb-O coatings for metallic components -- bipolar plates (BPPs) and porous transport layers (PTLs) -- in PEM water electrolyzers. Using reactive high-power impulse magnetron sputtering (HiPIMS), we deposit compact 200 nm bilayer coatings onto SS316L substrates, systematically tailoring their composition. By precisely controlling oxygen partial pressure and Nb/Ti ratio, we adjust stoichiometry and structure, directly affecting electrical resistivity and corrosion resistance. We examine interfacial contact resistance (ICR) and electrochemical parameters before and after accelerated corrosion testing. Optimized coatings exhibit resistivity on the order of 10^-4 Ohmcm and extremely low corrosion current densities (J_corr = 0.01-0.08 uA/cm^2), well below the U.S. DOE 2026 target. Most importantly, these coatings enable the ICR target after accelerated corrosion testing with a Pt overlayer as thin as 5 nm, reducing Pt loading by up to two orders of magnitude compared to conventional approaches.",Materials Science,http://arxiv.org/abs/2512.17721v1,arXiv,2
"A precise detection of palladium (Pd) ions is a critical challenge with significant socio-economic implications across various industrial and chemical sectors. Due to its widespread use and poor biodegradability, Pd2+ accumulates in environmental ecosystems, posing severe risks to both the environment and living organisms. Consequently, there is a strong demand for selective, sensitive, and user-friendly detection methods. Among emerging strategies, optical detection techniques (both luminescent and colorimetric) using metal-based receptors have gained considerable attention. These sensors offer distinct advantages over traditional organic probes, including large Stokes shifts, long emission lifetimes, exceptional photostability, enhanced water solubility, recyclability, and remarkable chemical versatility. These attributes make them highly suitable for diverse applications in sensing and bioanalytical fields. This review provides a comprehensive overview of recent advancements in luminescent and colorimetric metal-based probes, including metal complexes and metal-organic frameworks (MOFs), for the selective detection of Pd2+. It discusses key design strategies, critical performance factors, and future prospects, offering valuable insights for researchers working on next-generation sensing platform.",Materials Science,http://arxiv.org/abs/2512.17719v1,arXiv,2
"The $LnMn_{6}Sn_{6}$ family of materials, where $Ln^{3+}$ is a lanthanide trivalent cation, have attracted extensive interest due to the interplay of electronic structure, magnetism, and topology present in this family that gives rise to complex electronic and magnetic phenomena. Specifically, the crystal field effects on the lanthanide ion and crystal field splitting of otherwise degenerate energy levels causes dramatic changes in the orbital magnetic behavior and overall magnetic structure of these materials. The coupling of the highly anisotropic lanthanide ions' spins (with large spin-orbit couplings) to the spins of the Mn atoms, which are arrayed in a kagome lattice, engenders exotic topological phenomena. This combination of magnetic anisotropy and electronic topology motivates investigation into the magnetic excitations of these materials, which unlike the ground state magnetic structures of this family, have not been extensively studied. Herein, we use Brillouin light scattering to measure the magnon spectra of $LnMn_{6}Sn_{6}$ (Ln = Tb, Dy, and Ho). This work represents the first detailed and comparative study on the magnetic dynamics in these materials and reveals that the identity of the lanthanide ion strongly influences the magnon frequency and demonstrates a direct correlation between the lanthanide's magnetic anisotropy and the observed spin wave excitations. Quantitative analysis indicates that the lanthanide ion's anisotropy controls the magnon frequency, while its total angular momentum influences the material's gyromagnetic ratio. These findings suggest that lanthanide substitution provides a pathway for tuning magnon properties in this material family.",Materials Science,http://arxiv.org/abs/2512.17715v1,arXiv,2
"The crystal structure of high-pressure solid hydrogen remains a fundamental open problem. Although the research frontier has mostly shifted toward ultra-high pressure phases above 400 GPa, we show that even the broken symmetry phase observed around 130~GPa requires revisiting due to its intricate coupling of electronic and nuclear degrees of freedom. Here, we develop a first principle quantum Monte Carlo framework based on a deep neural network wave function that treats both electrons and nuclei quantum mechanically within the constant pressure ensemble. Our calculations reveal an unreported ground-state structure candidate for the broken symmetry phase with $Cmcm$ space group symmetry, and we test its stability up to 96 atoms. The predicted structure quantitatively matches the experimental equation of state and X-ray diffraction patterns. Furthermore, our group-theoretical analysis shows that the $Cmcm$ structure is compatible with existing Raman and infrared spectroscopic data. Crucially, static density functional theory calculation reveals the $Cmcm$ structure as a dynamically unstable saddle point on the Born-Oppenheimer potential energy surface, demonstrating that a full quantum many-body treatment of the problem is necessary. These results shed new light on the phase diagram of high-pressure hydrogen and call for further experimental verifications.",Materials Science,http://arxiv.org/abs/2512.17703v2,arXiv,2
"The advent of high-intensity ultrafast laser pulses has opened new opportunities for controlling and designing quantum materials. In particular, terahertz (THz) pulses can resonantly drive optical phonon modes, enabling dynamic manipulation of lattice degrees of freedom. In this work, we investigate the ultrafast quantum thermodynamics of optical phonon mode driven by a THz pulse by treating the phonon as an open quantum system coupled to a thermal environment within a Caldeira-Leggett-type framework. We derive the quantum heat current between the phonon and the bath and analyze its behavior under realistic pulse protocols. Our results demonstrate that ultrafast laser driving can reveal and even induce significant deviations from the commonly adopted Markovian approximation, thereby providing a pathway to probe and control non-Markovian dissipation in driven solid-state systems.",Materials Science,http://arxiv.org/abs/2512.17669v1,arXiv,2
"Designing molecules that must satisfy multiple, often conflicting objectives is a central challenge in molecular discovery. The enormous size of chemical space and the cost of high-fidelity simulations have driven the development of machine learning-guided strategies for accelerating design with limited data. Among these, Bayesian optimization (BO) offers a principled framework for sample-efficient search, while generative models provide a mechanism to propose novel, diverse candidates beyond fixed libraries. However, existing methods that couple the two often rely on continuous latent spaces, which introduces both architectural entanglement and scalability challenges. This work introduces an alternative, modular ""generate-then-optimize"" framework for de novo multi-objective molecular design/discovery. At each iteration, a generative model is used to construct a large, diverse pool of candidate molecules, after which a novel acquisition function, qPMHI (multi-point Probability of Maximum Hypervolume Improvement), is used to optimally select a batch of candidates most likely to induce the largest Pareto front expansion. The key insight is that qPMHI decomposes additively, enabling exact, scalable batch selection via only simple ranking of probabilities that can be easily estimated with Monte Carlo sampling. We benchmark the framework against state-of-the-art latent-space and discrete molecular optimization methods, demonstrating significant improvements across synthetic benchmarks and application-driven tasks. Specifically, in a case study related to sustainable energy storage, we show that our approach quickly uncovers novel, diverse, and high-performing organic (quinone-based) cathode materials for aqueous redox flow battery applications.",Materials Science,http://arxiv.org/abs/2512.17659v1,arXiv,2
"Summarized abstract- Gallium nanoparticles (Ga-NPs) display tunable plasmonic resonances from the ultraviolet to the infrared, but achieving uniform and ordered Ga-NP arrays is hindered by coarsening-induced size dispersion during physical deposition. Here, the effect of substrate temperature on nucleation, growth, and homogenization of Ga-NPs thermally evaporated on GaAs is systematically analyzed using atomic force and scanning electron microscopies. An optimal 300-350 celsius degrees window yields narrowly dispersed, dense arrays, whereas higher temperatures enhance surface diffusion and desorption, producing larger, sparser, and morphologically relaxed (flattened) NPs. Temperature-dependent optical reflectance reveals localized surface plasmon resonances whose energies scale with NP size and aspect ratio, and a figure of merit combining density and size dispersion identifies intermediate temperatures as optimal, in agreement with increased plasmonic quality factors. In-situ post-deposition annealing demonstrates Ostwald ripening as the dominant high-temperature coarsening pathway and underscores the need for rapid cooling and oxide-shell stabilization to preserve homogeneity. Cross-sectional transmission electron microscopy and electron energy-loss spectroscopy confirm a core-shell architecture, quantify temperature-driven oxide thickening, and verify the liquid-metal nature of the Ga NPs core. Finally, extended deposition tests show that temperature-driven size homogenization persists for larger NPs, establishing a general framework for thermal control of uniformity and optical performance in Ga-NP arrays suitable for plasmonic devices.",Materials Science,http://arxiv.org/abs/2512.17658v1,arXiv,2
"Natural van der Waals (vdW) crystals with hyperbolic dispersion challenge artificial metamaterials but remain confined to the mid-infrared spectrum. The emergence of MoOCl2, a quasi-one-dimensional metal with in-plane hyperbolicity, overcomes this spectral limit, shifting the focus to the practical visible range. Here, utilizing angle-resolved Raman spectroscopy, we uncover a highly anisotropic vibrational response characterized by pronounced Fano resonances and polarization switching, which serve as signatures of strong coupling between phonons and the metallic continuum. Harnessing this interaction, we demonstrate ""Hyperbolic-Enhanced Raman"" (HypER) scattering, where MoOCl2 provides polarization-tunable enhancement factors exceeding 10^7 and picomolar-level detection down to 100 pM, without any nanostructuring. These results establish MoOCl2 as a simple, air-stable, wafer-compatible platform for visible-range hyperbolic nanophotonics and lithography-free sensing.",Materials Science,http://arxiv.org/abs/2512.17647v1,arXiv,2
"We investigate the stability of topological phases in doped Kitaev-Heisenberg ladders by studying the competition with itinerant electrons and the associated charge fluctuations in a Hubbard model on a honeycomb ribbon geometry. We analyze the evolution of string order parameters, spin correlations, and charge fluctuations as functions of hopping amplitude and interaction strength in a half-filled band. Our results from density matrix renormalization group (DMRG) calculations show that increasing electron bandwidth progressively suppresses the topological phases, shifting and narrowing their stability regions in the phase diagram. We identify the critical values of hopping where string order vanishes and characterize the interplay between magnetic order and charge fluctuations. These findings provide insight into the robustness of topological phases against doping and charge dynamics, with implications for candidate Kitaev materials and engineered quantum systems.",Materials Science,http://arxiv.org/abs/2512.17596v1,arXiv,2
"We establish a quantitative relation between the altermagnetic spin-splitting and different higher order multipoles of the charge and magnetization density around the magnetic atoms. Magnetic multipoles such as octupoles or triakontadipoles have been suggested as potential ferroic order parameters for d- and g-wave altermagnetism, respectively, based mainly on qualitative symmetry arguments. We use first-principles-based electronic structure calculations to establish a clear quantitative relation between the strength of the altermagnetic spin splitting and the magnitude of certain local multipoles. We vary the magnitude of these multipoles either by applying an appropriate constraint on the charge density or by varying a corresponding structural distortion mode, using two simple perovskite materials, SrCrO3 and LaVO3, as model systems. Our analysis indicates that in general the altermagnetic spin splitting is not exclusively determined by the lowest order nonzero magnetic multipole, but results from a superposition of contributions from different multipoles with comparable strength, suggesting the need for a multi-component order parameter to describe altermagnetism. We also discuss different measures to quantify the overall spin-splitting of a material, without relying on features that might be specific to only individual bands.",Materials Science,http://arxiv.org/abs/2512.17587v1,arXiv,2
"The present study provides the consideration of a mode III interface crack in one-dimentional (1D) piezoelectric quasicrystal under antiplane phonon and phason loading and inplane electric field. Due to complex function approach all required electromechanical parameters are presented through vector-functions analytic in the whole complex plane except the crack region. The cases of electrically impermeable (insulated) and electrically limited permeable conditions on the crack faces are considered. In the first case a vector Hilbert problem in the complex plane is formulated and solved exactly and in the second one the quadratic equation with respect to the electric flux through the crack region is obtained additionally. Its solution permits to find phonon and phason stresses, displacement jumps (sliding) and also electric characteristics along the material interface. Analytical formulas are also obtained for the corresponding stress intensity factors related to each field. The numerical computations for three selected variants of the loading conditions was conducted and the resulting field distributions are visualised on the crack continuation beyond the crack and also inside of the crack region.",Materials Science,http://arxiv.org/abs/2512.17981v1,arXiv,2
"When the dimensions of structures shrink to the order of the inelastic mean free path of the energy-carrying quasi-particles, the character of energy transport changes from diffusive to ballistic. However, the point of transition remains a matter of debate. Here, we determine the dominant channel of energy transport through a nanoscale Cu layer as a function of its thickness. The energy rapidly transferred across Cu via hot electrons from a photo-excited Pt layer into a buried Ni detection layer translates into a rapid expansion of the Ni layer probed via ultrafast x-ray diffraction. The non-linear dependence of the Ni strain amplitude on the absorbed laser fluence indicates that the transport through Cu becomes more efficient with increasing fluence. This fluence-dependent transport efficiency is reproduced by a diffusive energy transport model and serves as a generally applicable experimental approach to distinguish diffusion from ballistic transport. Following this approach, we identify diffusive electronic energy transport to govern the spatial energy distribution for Cu layer thicknesses larger than twice the electronic inelastic mean free path.",Materials Science,http://arxiv.org/abs/2512.17565v1,arXiv,2
"Controlled crystallization of amorphous alloys offers a powerful route to tailor magnetic and structural properties at the nanoscale. Thin films of CoFeB alloy are essential for the development of various spintronic devices. The crystallization mechanisms of CoFeB during the annealing process have been thoroughly investigated in earlier studies, demonstrating that boron diffuses from the amorphous film, allowing the remaining CoFe to form a body-centered cubic lattice. Here, a distinct transformation pathway in pulsed-laser-deposited amorphous Co40Fe40B20 films is revealed, where vacuum annealing drives the formation of a metastable tau-boride phase, (Co,Fe)23B6. Comprehensive structural characterization - combining X-ray diffraction, transmission electron microscopy, and compositional analysis - proves that tau-boride forms with high crystalline quality and minimal boron loss. Following the transition from amorphous CoFeB films to crystalline (Co,Fe)23B6, an improvement of magnetic properties is observed, with corresponding increases in such values as saturation magnetization, coercivity, loop squareness, and average magnetic moment. The reproducible stabilization of a boron-rich metastable phase in CoFeB thin films expands the known crystallization landscape of this technologically important alloy system. These findings provide new insight into phase engineering in transition-metal borides and open opportunities for designing nanostructured magnetic materials with tunable functionality for next-generation spintronic and nanoelectronic devices.",Materials Science,http://arxiv.org/abs/2512.17501v1,arXiv,2
"Halbach spheres provide a theoretically elegant means of generating highly homogeneous magnetic fields, but practical implementation is hindered by challenging fabrication and restricted interior access. This study examines discrete spherical Halbach configurations assembled from permanent magnets placed at the vertices of Platonic and Archimedean solids. Analytical calculations, numerical field simulations, and experimental measurements indicate that polyhedra with icosahedral symmetry achieve the most favorable balance among field strength, homogeneity, and interior accessibility. They produce exceptionally flat fourth-order central saddle points, resulting in a usable homogeneous field volume up to a factor of 260 larger than that of traditional Halbach disk or cylindrical arrays. Several magnet assemblies composed of cubical NdFeB magnets are fabricated and their three dimensional field distributions characterized, demonstrating homogeneous regions of up to several cubic centimeters with deviations below 1%. The findings establish discrete icosahedrally symmetric magnet arrays as practical, scalable building blocks for compact, highly homogeneous magnetic field sources suited to mobile magnetic resonance, and magnetophoretic applications.",Materials Science,http://arxiv.org/abs/2512.17497v1,arXiv,2
"Perovskite indoor photovoltaics (PIPVs) are emerging as a transformative technology for low-light intensity energy harvesting, owing to their high power conversion efficiencies (PCEs), low-cost fabrication, solution-processability, and compositionally tunable band gaps. In this work, methylammonium-free perovskite absorbers were compositionally engineered to achieve band gaps of 1.55, 1.72, and 1.88 eV, enabling matching the spectral photoresponse with the indoor lighting. Devices based on a scalable mesoscopic n-i-p architecture were systematically evaluated under white LED illumination across correlated color temperatures (3000-5500 K) and light intensities from 250 to 1000 lux with active area of 1 cm2. The 1.72 eV composition exhibited the most promising performance across different light intensities and colors, achieving PCEs of 35.04 % at 1000 lux and 36.6 % at 250 lux, with a stable device operation of over 2000 hours. On the other hand, the 1.88 eV band-gap variant reached a peak PCE of 37.4 % under 250 lux (5500 K), however performance trade-offs were observed across the different color lights LEDs. Our combined experimental and theoretical optical-electrical simulations suggest that decreasing trap-assisted recombination in wide-bandgap compositions may further improve PIPV performance across the different illumination conditions. In contrast, devices with 1.55 eV band gap underperformed in such conditions due to suboptimal spectral overlap and utilization. These findings establish bandgap optimization and device architecture as key design principles for high-efficiency, stable PIPVs, advancing their integration into self-powered electronic systems and innovative indoor environments.",Materials Science,http://arxiv.org/abs/2512.17479v1,arXiv,2
"Time-dependent deformation, particularly creep, in high-temperature alloys such as Inconel 625 is a key factor in the long-term reliability of components used in aerospace and energy systems. Although Inconel 625 shows excellent creep resistance, finite-element creep simulations in tools such as ANSYS remain computationally expensive, often requiring tens of minutes for a single 10,000-hour run. This work proposes deep learning based surrogate models to provide fast and accurate replacements for such simulations. Creep strain data was generated in ANSYS using the Norton law under uniaxial stresses of 50 to 150 MPa and temperatures of 700 to 1000 $^\circ$C, and this temporal dataset was used to train two architectures: a BiLSTM Variational Autoencoder for uncertainty-aware and generative predictions, and a BiLSTM Transformer hybrid that employs self-attention to capture long-range temporal behavior. Both models act as surrogate predictors, with the BiLSTM-VAE offering probabilistic output and the BiLSTM-Transformer delivering high deterministic accuracy. Performance is evaluated using RMSE, MAE, and $R^2$. Results show that the BiLSTM-VAE provides stable and reliable creep strain forecasts, while the BiLSTM-Transformer achieves strong accuracy across the full time range. Latency tests indicate substantial speedup: while each ANSYS simulation requires 30 to 40 minutes for a given stress-temperature condition, the surrogate models produce predictions within seconds. The proposed framework enables rapid creep assessment for design optimization and structural health monitoring, and provides a scalable solution for high-temperature alloy applications.",Materials Science,http://arxiv.org/abs/2512.17477v1,arXiv,2
"We study how topological crystalline defects--dislocations--reshape the real-space quantum geometric tensor and act as tunable sources of quantum geometry. We show that dislocations strongly enhance the quantum metric, establishing a direct link between lattice topology and the Hilbert-space geometry of states. We characterize the quantum geometry of topological magnons in ordered arrays of dislocations, demonstrating that defect-induced geometric enhancement controls their localization and topological protection. In disordered arrays, dislocation-driven geometry expands the accessible topological phase space and enables transitions to disorder-induced topological phases. Our results identify the quantum metric as a tunable bridge between crystalline topology, magnonic excitations, and emergent topological matter in aperiodic solid-state and synthetic systems.",Materials Science,http://arxiv.org/abs/2512.17454v1,arXiv,2
"The emergence of a charge density wave (CDW) in a Weyl semimetal -- a correlated topological phase, is exceptionally rare in condensed matter systems. In this context, the quasi-one-dimensional type-III Weyl semimetal (TaSe$_4$)$_2$I undergoes a CDW transition at $T_{\mathrm{CDW}} \approx 263$~K, providing an exceptional platform to investigate correlated topological CDW states. Here, we uncover an additional hidden-order phase transition at $T^* \sim 100$ K, well below the CDW onset, using low-frequency resistance noise spectroscopy, electrical transport, and thermoelectric measurements. This transition is characterized by a sharp enhancement in the noise exponent ($Î±$) and variance of resistance fluctuations. Analysis of higher-order statistics of resistance fluctuations reveals the correlated dynamics underlying the transition. A pronounced anomaly in the Seebeck coefficient near $T^*$ further suggests a Fermi surface reconstruction. First-principles calculations reveal a structural distortion from the high-symmetry $I422$ phase to a low-symmetry $C2$ phase, via an intermediate $I4$ symmetry. This leads to renormalization of the electronic structure near the Fermi level and opening of a bandgap in the hidden-order phase. These findings demonstrate a previously unidentified correlated phase transition in the topological CDW-Weyl semimetal (TaSe$_4$)$_2$I, enriching the phase diagram of this material and establishing it as an ideal platform for studying intertwined electronic and structural orders.",Materials Science,http://arxiv.org/abs/2512.17433v1,arXiv,2
"Existing digital manufacturing methods can be broadly divided into subtractive approaches, where material is removed from a bulk to reveal the desired form, and additive methods, in which material is introduced voxel-by-voxel to create an object.   We here show a fundamentally different method for the fabrication of three-dimensional objects that is neither subtractive nor additive. Instead of removal or layer-by-layer material deposition, in LiquiFab we shape an entire volume of liquid polymer by subjecting it to a set of geometrical constraints under conditions of weightlessness. The physics of liquid interfaces then drives the polymer to naturally adopt a configuration that minimizes its surface energy. On Earth, we achieve weightlessness through neutral buoyancy, and show that a small, well-defined set of boundary surfaces can be used to drive the liquid into a desired form that is then solidified. By sequentially applying this process, complex architectures can be assembled from successive liquid-formed elements.   Unlike additive manufacturing, where every point within the object must be individually visited by a print head or light field, LiquiFab forms the entire structure simultaneously. This makes the process highly scalable and opens the door to rapid manufacturing of large objects both on Earth and in space.",Materials Science,http://arxiv.org/abs/2512.19756v1,arXiv,2
"CrOCl is a van der Waals-layered insulator with an antiferromagnetic ground state, making it a promising platform for exfoliation and the exploration of low-dimensional magnetism. An accurate ab initio description is therefore essential. Previous density-functional studies have shown that DFT+$U$ calculations may erroneously favor ferromagnetic order depending on the choice of parametrization, an issue that cannot be remedied by simply adjusting the value of $U$. Here, we demonstrate that an explicit Hubbard correction is unnecessary: the PBE functional correctly reproduces the AFM ground state while simultaneously improving the description of structural properties. Moreover, PBE provides a reliable account of the electronic structure. These findings clarify the role of correlation effects in CrOCl and identify PBE as a robust starting point for future ab initio studies of CrOCl-based materials.",Materials Science,http://arxiv.org/abs/2512.17405v1,arXiv,2
"Nanopores (NPs) grown by anodizing can be partially hidden beneath a relatively compact surface oxide layer, which limits the volumetric surface area of these nanostructures. In this work, nitinol (NiTi) alloy was anodized in an electrolyte containing ethylene glycol, water, and sodium chloride in static and stirred electrolyte stages with the aim of removing the irregular surface array while achieving a thick NP layer. Electron micrographs showed that anodization in the static electrolyte provides a controlled thickness of NP layers covered by an irregular surface layer. In contrast, anodizing in the stirred electrolyte reduced the thickness and the degree of irregularity, which were controlled by the different kinetics of dissolution at the tops, perimeters and bottoms of NPs. To benefit simultaneously from the thickness and regularity of the oxide layers, two-stage anodizing under static and then stirred electrolyte conditions was found to be effective. Following a 30 min anodization in the static electrolyte, anodizing for 30 min under the stirred conditions provided the highest regularity in the oxide array, resulting in NPs of almost 40 nm and 11 Î¼m in diameter and layer thickness, respectively. Two-stage anodizing under static then stirred electrolyte conditions is proposed in order to promote NP structures for applications demanding higher surface areas.",Materials Science,http://arxiv.org/abs/2512.17397v2,arXiv,2
"Platinum (Pt) and ruthenium (Ru), both members of the platinum-group metals (PGMs), are renowned for their exceptional resistance to corrosion, oxidation, and high temperatures, making them promising candidates for advanced high-temperature applications. This study investigates the direct current (DC) electrodeposition of Pt, Ru, and a binary Pt-Ru alloy onto NiCoCrAlYTa-coated single-crystal superalloy CMSX-4, along with their vacuum annealing and respective effects on the isothermal oxidation behavior of the system at 1100 Â°C. All the electrodeposited overlays demonstrated substantial enhancement in oxidation resistance. However, Pt exhibited the highest protection efficiency, Ru the least, and the Pt-Ru alloy provided an intermediate level of performance. Microscopic and X-ray diffraction analyses revealed that the competitive formation of protective Î±-Al2O3 and spinel NiAl2O4 phases on the coated surfaces played a crucial role in determining the oxidation resistance, driven by atomic interactions between the elements in the NiCoCrAlYTa bond coat and the overlay metals. Despite Ru's relatively lower oxidation resistance compared to Pt, its significantly lower cost offers potential advantages in cost-sensitive, high-temperature applications. These findings provide valuable insights into optimizing Pt-group metal coatings for durability in high-performance systems.",Materials Science,http://arxiv.org/abs/2512.17392v1,arXiv,2
"This study investigates the influence of varying Pt/Ru ratios on the oxidation mechanism of NiCoCrAlYTa coatings with electrodeposited, vacuum-annealed Ptsingle bondRu overlays. Weight change measurements, scanning electron microscopy/energy dispersive X-ray spectrometry (SEM/EDS), X-ray diffraction (XRD), and X-ray photoelectron spectroscopy (XPS) were used for high-temperature oxidation analyses, showing superior resistance with higher Pt contents. This was attributed to the creation of a denser, thinner, and more homogeneous layer of alumina (alpha-Al2O3) in the thermally-grown oxide (TGO) layer. On the contrary, an increase in Ru contents led to the development of other oxides and microcracks along with alumina in the TGO layer, undermining oxidation protection. The accommodation of Ti and Ta, in the minimally-deteriorative form of carbide, along with Y into the TGO layer with increasing Pt contents further enhanced oxidation resistance. In addition to the explored significant impact of the Pt/Ru ratio on oxide scale characteristics and oxidation resistance, the lower cost of Ru compared to Pt suggests the potential for designing cost-effective systems through optimized Pt/Ru ratios and microstructural engineering.",Materials Science,http://arxiv.org/abs/2512.17384v2,arXiv,2
"Generating and controlling spin current in miniaturized magnetic quantum devices remains a central objective of spintronics, due to its potential to enable future energy-efficient information technologies. Among the existing magnetic phases, altermagnetism have recently emerged as a highly promising platform for spin current generation and control, going beyond ferromagnetism and antiferromagnetism. Here, we propose a symmetry-allowed spin photovoltaic effect in two-dimensional (2D) altermagnetic semiconductors that enables predictable control of giant spin injection currents. Distinct from parity-time ($\mathcal{PT}$)-antiferromagnets, Janus altermagnetic semiconductors generate not only shift current but also a unique injection current with spin momentum locked in a specific direction under linearly polarized light -- a mechanism absent in $\mathcal{PT}$-antiferromagnets. Through symmetry analysis and first-principles calculations, we identify Janus Fe$_2$SSeO as a promising candidate. Specifically, the monolayer Fe$_2$SSeO exhibits a polarization-dependent injection conductivity reaching $\sim$1,200~$Î¼$A/V$^{2}\!\cdot\!\hbar/2e$, and the giant spin injection current can be effectively switched by rotating the magnetization direction and engineering strains. These findings underscore the potential of 2D altermagnets in spin photovoltaics and open avenues for innovative quantum devices.",Materials Science,http://arxiv.org/abs/2512.17315v1,arXiv,2
"From a symmetry perspective, ferroaxial order belongs to the same symmetry as time-reversal-even pseudovectors. Experimentally, ${\rm K_2Zr(PO_4)_2}$ is known to undergo a displacive-type phase transition from a non-ferroaxial to a ferroaxial phase. To identify the key microscopic ingredients driving this transition, we carry out a quantitative analysis combining density-functional theory calculations and symmetry-adapted closest Wannier analysis. As a result, we show that electric toroidal dipole, electric toroidal octupole, and electric hexadecapole, which belong to the same irreducible representation, make dominant contributions to the ferroaxial transition. In particular, we find that spinless electric toroidal octupoles, which originate from spin-independent off-diagonal real hopping between the $p$ orbitals on P and O atoms and between the $d$ orbitals on Zr atoms and $p$ orbitals on O atoms, provide the most significant contributions. Moreover, we explicitly analyze the orbital characters involved in the relevant hybridizations associated with these multipoles. We further show that the relativistic spin--orbit coupling has a negligible influence on the ferroaxial transition. These results demonstrate that spin-independent orbital hybridization between different orbitals on different atoms plays a crucial role in inducing the ferroaxial transition.",Materials Science,http://arxiv.org/abs/2512.17290v1,arXiv,2
"Efficient charge-carrier injection from air-stable electrodes into organic semiconductors (OSCs) is essential for fabricating solution-processed organic optoelectronic devices under ambient conditions. Today, this is typically achieved by incorporating doped OSC interlayers, introducing self-assembled dipole monolayers, or adding mobile ions to the active material (AM). Here, we demonstrate an alternative approach that eliminates the need for additional injection layers or ionic additives. We achieve this by blending the dipolar compound TMPE-OH into the electroluminescent polymer Super Yellow (SY) and depositing this sole AM between two air-stable electrodes, forming a single-layer, dipole-doped OLED (D-OLED). By tracking its transient voltage-luminance response, performing impedance spectroscopy, and comparing these characteristics with two other single-layer device concepts, i.e. a neat-SY OLED without a dipolar compound and a light-emitting electrochemical cell (LEC) containing mobile ions, we can establish that the auxiliary dipoles in the D-OLED reorient under the applied driving voltage, enabling immediate luminance turn-on and lowering the injection barriers at both electrodes. Finally, we demonstrate that the D-OLED achieves current efficacies comparable to those of SY OLEDs incorporating dedicated injection layers or LECs. Our study establishes dipolar doping as a practical strategy for efficient bipolar charge injection from air-stable electrodes in solution-processed organic semiconductor devices.",Materials Science,http://arxiv.org/abs/2512.17287v1,arXiv,2
"Understanding and manipulating two-dimensional materials for real-world applications remains challenging due to a lack of effective and high-throughput characterization techniques. Soft X-ray time-of-flight photoemission electron microscopy (XPEEM) provides element- and depth-sensitive information of materials and buried interfaces. However, chromatic and spherical aberrations cannot be corrected with electron-lens combinations. These aberrations, combined with astigmatism and space-charge effects, significantly degrade the spatial and energy resolutions. To overcome this limitation, we outline a spatial-attention based deep learning approach to automatically correct for these effects and attain nanometer resolution over the entire field-of-view (FoV). The combination of this corrective algorithm with XPEEM, termed as nanoXPEEM, establishes a new record of 48-nm spatial resolution with a 232-micrometer diameter FoV in the soft x-ray regime (700-1000 eV). nanoXPEEM provides unique spatial mapping of the element-specificity, depth-sensitivity, and local structure on the nanoscale. It can bridge the current gap to achieve angstrom (atomic) scale resolution.",Materials Science,http://arxiv.org/abs/2512.17252v1,arXiv,2
"Spin chirality provides a powerful route to control magnetic and topological phases in materials, enabling next-generation spintronic and quantum technologies. Coplanar noncollinear antiferromagnets with Kagome lattice spin geometries host vector spin chirality (VSC), the handedness of spin arrangement, and offer an excellent platform for chirality-driven phase control. However, the microscopic mechanisms governing VSC switching and its coupling to magnetic order, electronic structure, and quantum geometry remain elusive, with experimental evidence still lacking. Here, we present conclusive experimental evidence of temperature-driven VSC switching in an archetypal noncollinear antiferromagnetic manganese chromium nitride (Mn3CrN) epitaxial thin films. The VSC switching induces a concomitant quantum-geometric and Lifshitz transition, manifested through a pronounced peak in anomalous Hall conductivity remanence, a metal-insulator-like crossover in longitudinal resistivity, and a distinct evolution of x-ray magnetic circular dichroic signal. The reversal of VSC reconstructs the spin configuration, Fermi surface topology and Berry curvature, marking a unified magnetic-electronic-quantum geometric transition. This emergent behaviour, captured through magneto-transport and magneto-optic measurements, and supported by first-principles theory establish VSC as an active control knob for chirality-driven phase engineering and the design of multifunctional quantum devices.",Materials Science,http://arxiv.org/abs/2512.17248v1,arXiv,2
"Understanding atomic structures is crucial, yet amorphous materials remain challenging due to their irregular and non-periodic nature. The wavelet-transform radial distribution function (WT-RDF) offers a physics-based framework for analyzing amorphous structures, reliably predicting the first and second RDF peaks and overall curve trends in both binary Ge 0.25 Se 0.75 and ternary Ag x(Ge 0.25 Se 0.75)100-x (x=5,10,15,20,25) systems. Despite these strengths, WT-RDF shows limitations in amplitude accuracy, which affects quantitative analyses such as coordination numbers. This study addresses the issue by optimizing WT-RDF parameters using a machine learning approach, producing the enhanced WT-RDF+ framework. WT-RDF+ improves the precision of peak predictions and outperforms benchmark ML models, including RBF and LSTM, even when trained on only 25 percent of the binary dataset. These results demonstrate that WT-RDF+ is a robust and reliable model for structural characterization of amorphous materials, particularly Ge-Se systems, and support the efficient design and development of phase-change thin films for next-generation electronic devices and components.",Materials Science,http://arxiv.org/abs/2512.17245v1,arXiv,2
"Here we investigate the electronic structure of the tetragonal tungsten bronze Ba$_{3-x}$Yb$_x$Ta$_{5}$O$_{15}$ by making use of hard x-ray photoemission spectroscopy. The core level spectroscopy shows that the substitution with Yb ions in the series first occurs on the compact S1 site. For $x\leq1$, Yb is found to be dominantly Yb$^{2+}$ with a small mixing of Yb$^{3+}$, while for $x>1$, a significant increase of Yb$^{3+}$ is found, suggesting not only that site S2 favours Yb$^{3+}$, but also that their presence affects also the valency of the ions in site S1. The valence band spectra shows a relatively deep Yb$^{2+}$ doublet, but at the same time indications of a Ta~$5d$-Yb~$4f$ interaction are found, suggesting the presence of Yb~$4f$ carriers at the Fermi level through this hybridization. Our results thus point towards an exotic form of $d$-$f$ electronic interplay that together with the structural degrees of freedom can result in the unusual trends observed in the physical properties of Ba$_{3-x}$Yb$_x$Ta$_{5}$O$_{15}$.",Materials Science,http://arxiv.org/abs/2512.17234v1,arXiv,2
"We conducted a comprehensive study on the compositional dependence of the anomalous Nernst effect (ANE) in amorphous (amo.) Tb-Fe-Co thin films. The anomalous Nernst coefficient strongly depends not only on the Tb composition but also on the transition metal composition, reaching a maximum of 1.8 uV/K for amo. Tb11.0(Fe50.0Co50.0) 89.0. By evaluating the electrical and thermoelectric properties, it was clarified that this maximum is achieved by the superposition of two large contributions: S_1 arising from direct transverse electron conduction due to a temperature gradient, and S_2 resulting from the combined Seebeck and anomalous Hall effects. We discovered that the anomalous Nernst conductivity, which is attributed to Berry curvature, varied significantly with the transition metal, even in an amorphous material lacking long-range crystalline order. Our research indicates that it is possible to control the electronic states that influence thermoelectric properties, even in the amorphous state.",Materials Science,http://arxiv.org/abs/2512.17223v1,arXiv,2
"We introduce QMBench, a comprehensive benchmark designed to evaluate the capability of large language model agents in quantum materials research. This specialized benchmark assesses the model's ability to apply condensed matter physics knowledge and computational techniques such as density functional theory to solve research problems in quantum materials science. QMBench encompasses different domains of the quantum material research, including structural properties, electronic properties, thermodynamic and other properties, symmetry principle and computational methodologies. By providing a standardized evaluation framework, QMBench aims to accelerate the development of an AI scientist capable of making creative contributions to quantum materials research. We expect QMBench to be developed and constantly improved by the research community.",Materials Science,http://arxiv.org/abs/2512.19753v1,arXiv,2
"We evaluate the Sample-based Krylov Quantum Diagonalization (SKQD) algorithm on one- and two-dimensional Heisenberg models, including strongly correlated regimes in which the ground state is dense. Using problem-informed initial states and magnetization-sector sweeps, SKQD accurately reproduces ground-state energies and field-dependent magnetization across a range of anisotropies. Benchmarks against DMRG and exact diagonalization show consistent qualitative agreement, with accuracy improving systematically in more anisotropic regimes. We further demonstrate SKQD on quantum hardware by implementing 18- and 30-qubit Heisenberg chains, obtaining magnetization curves that match theoretical expectations. Simulations on small 2D square-lattice systems further demonstrate that the method applies effectively beyond 1D geometries.",Materials Science,http://arxiv.org/abs/2512.17141v1,arXiv,2
"The effect of Ga substitution on the electronic, magnetic, and low-energy responses of the Zintl phase EuZn$_2$P$_2$ is investigated by electrical transport, electron spin resonance (ESR), and terahertz time-domain spectroscopy (THz-TDS). Incorporating Ga into EuZn$_2$P$_2$ (EuZn$_{1.8}$Ga$_{0.2}$P$_2$) reduces the electrical resistivity, indicating enhanced free-carrier density and a narrowed semiconducting gap. ESR confirms the persistence of Eu$^{2+}$ moments while showing a crossover from a Lorentzian to a Dysonian lineshape, consistent with reduced skin depth, increased carrier density, and the emergence of diffusive contributions. Ga-substituted compound display pronounced negative magnetoresistance linked to magnetic-polaron formation. THz-TDS reveals strong low-frequency absorption and a notable enhancement of the Drude conductivity in the substituted material, together with an increased carrier scattering time and enhanced carrier-density--to--effective-mass ratio. These results demonstrate that Ga substitution tunes charge transport, carrier dynamics, and short-range magnetic correlations in EuZn$_2$P$_2$, establishing EuZn$_{1.8}$Ga$_{0.2}$P$_2$ as a promising platform for engineering correlated narrow-gap magnetic semiconductors with enhanced electronic and spin-dependent functionalities.",Materials Science,http://arxiv.org/abs/2512.17123v1,arXiv,2
"We introduce 3D-DPhyne, a novel three-dimensional (3D) carbon allotrope derived from the dodecaphenylyne framework, and investigate its structural, electronic, optical, and mechanical properties using first-principles calculations. The proposed structure forms a tetragonal, topologically complex network of four-, six-, and twelve-membered carbon rings with mixed sp/sp^2 hybridization and a formation energy of -7.87 eV/atom, comparable to other stable carbon allotropes. Phonon dispersion calculations show no imaginary modes, and ab initio molecular dynamics simulations at 1000~K confirm robust thermal stability without bond breaking. Electronic structure analysis reveals metallic character, with multiple bands crossing the Fermi level and dominant contributions from carbon p orbitals, consistent with a fully delocalized 3D $Ï$-conjugated network. The optical response is anisotropic, exhibiting strong absorption in the visible and ultraviolet regions and low reflectivity across a broad range of photon energies. Mechanical analysis reveals pronounced elastic anisotropy, with Young's modulus varying from approximately 40 to 490 GPa depending on direction. Poisson's ratio displays unconventional directional behavior, including auxetic-like responses.",Materials Science,http://arxiv.org/abs/2512.17105v1,arXiv,2
"Gaining insight into the interaction between flexible piezoelectric structures and ocean waves can inform the development of compact, high-efficiency wave-energy converters that harvest renewable energy from the marine environment. In this paper, the problem of wave energy absorption by floating and submerged piezoelectric plates is investigated. The equations of motion for a plate consisting of two piezoelectric layers separated by an elastic substrate are derived in dimensional form from the full piezoelectric constitutive laws. A novel solution method based on conversion of hypersingular equations to a matrix operator is presented, which is general and can solve the equations of motion for submerged rigid, flexible elastic or flexible piezoelectric plates. Extensive numerical results are given for a range of parameters, including different piezoelectric materials: polyvinylidene fluoride (PVDF) and lead zirconate titanate (PZT-5H). Importantly, greater energy absorption is obtained for submerged plates when compared to plates floating on the surface. Furthermore, clamped boundary conditions give slightly larger energy absorption compared to the simply supported case. Our open-source code is provided at https://github.com/zjwegert/SemiAnalyticWECs.jl.",Materials Science,http://arxiv.org/abs/2512.17965v1,arXiv,2
"Kagome lattice bilayers offer unique opportunities for engineering electronic properties through interlayer stacking and strain. We report a comprehensive first-principles study of Pd$_3$O$_2$Cl$_2$ kagome bilayers, examining four stacking configurations (AA, AA$'$, AB, AB$'$). Our calculations reveal dramatic stacking-dependent band gap modulation from 0.08 to 0.76~eV, with the AB$'$ configuration being the most thermodynamically stable. All stackings exhibit robust mechanical stability with Young's moduli of 54.82-61.97~N/m and ductile behavior suitable for flexible electronics. Carrier effective masses show significant stacking dependence, ranging from 2.39-6.35~$m_0$ for electrons and 0.67-1.55~$m_0$ for holes. Strain engineering of the AB$'$ bilayer demonstrates non-monotonic band gap tuning and asymmetric modulation of carrier masses, with hole effective masses showing stronger strain sensitivity. These results establish Pd$_3$O$_2$Cl$_2$ bilayers as a promising platform for strain-engineered kagome-based quantum devices, where stacking order and mechanical deformation provide complementary control over electronic transport.",Materials Science,http://arxiv.org/abs/2512.17069v1,arXiv,2
"We employ correlation-consistent effective core potentials (ccECPs) to perform exact or nearly exact correlation and total energy calculations for the fifth-row elements (Rb-Xe). Total energies are calculated using various correlated methods: configuration interaction (CI), coupled-cluster (CC) up to perturbative quadruple excitations whenever feasible, and stochastic quantum Monte Carlo (QMC) approaches. In order to estimate the energy at the complete basis set (CBS) limit, the basis sets are constructed systematically through aug-cc-p(C)VnZ for each ccECP and further extrapolated to the CBS limit within the corresponding methods. Kinetic energies are evaluated at the FCI/CISD level to provide insights into the electron density and localization of the ccECPs. We also provide data sets for widely used diffusion Monte Carlo (DMC) to quantify fixed-node biases with single-reference trial wavefunctions. These comprehensive benchmarks validate the accuracy of ccECPs within the CC, CI, and QMC methodologies, thus providing accurate and tested valence-only Hamiltonians for many-body electronic structure calculations.",Materials Science,http://arxiv.org/abs/2512.17063v1,arXiv,2
"One of the distinguishing features of an altermagnet is that its spin-up and spin-down bands display a nodal momentum-dependent splitting even in the absence of spin-orbit coupling. While this property has been investigated in many weakly-correlated altermagnetic materials, the impact of strong electron-electron interactions on the spin-dependent electronic structure has remained little explored, particularly in metals. Here, we propose NiS2 as a prototypical strongly correlated metallic altermagnet. While at ambient pressure this compound is an altermagnetic Mott insulator, it undergoes a pressure-driven metal-insulator transition (MIT) while maintaining its altermagnetic ordered phase. By systematically comparing DFT, DFT+U, and DFT+DMFT calculations on the metallic altermagnetic phase near the MIT, we disentangle how strong static and dynamic correlations modify the electronic structure. Specifically, the spin splitting of the bands is modified not only through the enhancement of the local magnetic moment caused by static correlations, but also by the momentum-dependent bandwidth renormalization caused by dynamic correlations. Moreover, dynamic electronic correlations cause a pronounced lifetime asymmetry between the spin-up and spin-down quasiparticles, an effect that is amplified by the particle-hole asymmetry promoted by Hund's correlations. Our results not only shed light on the rich landscape of correlation effects in metallic altermagnets, but also establishes NiS2 as a platform to investigate the interplay between Mott and Hund physics and altermagnetic order.",Materials Science,http://arxiv.org/abs/2512.17059v1,arXiv,2
"The progress of magnonics ultimately depends on material platforms that offer precise control of spin waves propagation. Here, we put forward a chemical strategy to create locally tunable magnonic crystals by integrating switchable spin-crossover (SCO) molecules with 2D van der Waals magnets. Specifically, we investigate from first principles a hybrid molecular/2D heterostructure formed by [Fe((3,5-(CH3)2Pz)3BH)2] molecules (Fe-pz) deposited on a single-layer of semiconducting CrSBr. We show that Fe-pz molecules are stable on CrSBr while preserving its SCO bistability, particularly in densely packed molecular arrays. By patterning Fe-pz into periodic stripes separated by pristine CrSBr regions, the interface becomes a magnonic crystal that filters spin waves at selected frequencies. Crucially, light-driven excited spin-state trapping (LIESST) enables LS-HS switching and induces up to ~1.3% local strain in CrSBr, which in turn reshapes the magnonic band structure in a dynamic and reversible manner. These results establish Fe-pz@CrSBr as a switchable platform for on-chip, programmable magnonic devices.",Materials Science,http://arxiv.org/abs/2512.17003v1,arXiv,2
"EuAg$_4$Sb$_2$ is a rhombohedral europium triangle lattice material that exhibits a rich phase diagram of spin moirÃ© superlattices (SMS) and single-$q$ magnetic phases. In this paper, we characterize the incommensurate phases accessible with field applied in the plane with small angle neutron scattering (SANS). A variety of phases with unusual SANS patterns are accessible with magnetic field applied along the $a$ and $a^*$ directions. Many of these phases can be understood to be multi-$q$ phases. One phase in particular, ICM2b (ICM=incommensurate magnetic phase), is rather unconventional in that it is an anisotropic multi-$q$ phase that can rotate freely within the $ab$-plane, dependent on magnetic field direction and history. The stabilization of tunable multi-$q$ incommensurate spin textures \textit{via} in-plane field sets this class of materials apart from conventional skyrmion materials. We further identify that the propagation vectors of the in-plane phases have a significant commensuration with the diameter of the smallest pocket of the Fermi surface ($2k_{\text{F}}$). The multi/single-$q$ nature is also correlated with the enhancement of resistivity, suggesting that a gap opens in the electron bands at $q=2k_{\text{F}}$. We also compare with a phenomenological model of the phase diagram. The richness of phases revealed in this study hint at the frustrated nature of the incommensurate magnetism present in EuAg$_4$Sb$_2$ and motivate further probes of these phases and the origin of the stability of spin moirÃ© superlattices. Finally, the coupling of the multi-$q$ nature and $q=2k_{\text{F}}$ commensuration condition reveals the key requirements for a strong SMS transport response.",Materials Science,http://arxiv.org/abs/2512.16994v1,arXiv,2
"Recently, a host of exotic magnetic textures such as topologically protected skyrmion lattices has been discovered in several bulk metallic lanthanide compounds. In addition to hosting skyrmion phases, a hallmark of this class of materials is the appearance of numerous spin textures characterized by a superposition of multi-$q$ magnetic modulations: spin moirÃ© superlattices. The nuanced energy landscape thus motivates detailed studies to understand the underlying interactions. Here, we comprehensively characterize and model the three zero-field magnetic textures present in one such material, EuAg$_4$Sb$_2$. Systematic symmetry breaking experiments using magnetic field and strain determine that the ground state incommensurate magnetic phase (ICM1) is single-$q$. In contrast, ICM2 and ICM3 are both double-$q$, \textit{i.e.}, spin moirÃ© superlattices. Further, through application of polarized small angle neutron scattering and spherical neutron polarimetry, we demonstrate that ICM1 is a single-$q$ cycloid and ICM2 and ICM3 are double-$q$ vortex lattices, with Eu moments lying in the $ab$-plane in zero field and with a ferromagnetic component at finite field. Despite the quasi-2D nature of EuAg$_4$Sb$_2$, the modulations propagate out of the \textit{ab}-plane, leading to a shift of the spin texture between triangle lattice planes. Further, the ICM3 to ICM2 transition includes an unusual 45$^\circ$ rotation of the magnetic vortex lattice. Motivated by the coexistence of such drastically different phases in this compound, we conclude by developing a phenomenological model to understand the stability of these states. Our experimental probes and theoretical modeling definitively characterize three different and tunable phases in one material, and provide insight for the design of new topological spin-texture materials.",Materials Science,http://arxiv.org/abs/2512.16990v1,arXiv,2
"The world of 2D materials is rapidly expanding with new discoveries of stackable and twistable layered systems composed of lattices of different symmetries, orbital character, and structural motifs. Often, however, it is not clear a priori whether a pair of monolayers twisted at a small angle will exhibit correlated or interaction-driven phenomena. The computational cost to make accurate predictions of the single particle states is significant, as small twists require very large unit cells, easily encompassing 10,000 atoms, and therefore implementing a high throughput prediction has been out of reach. Here we show a path to overcome this challenge by introducing a machine learning (ML) based methodology that efficiently estimates the twisted interlayer tunneling at arbitrarily low twist angles through the local-configuration based approach that enables interpolating the local stacking for a range of twist angles using a random forest regression algorithm. We leverage the kernel polynomial method to compute the density of states (DOS) on large real space graphs by reconstructing a lattice model of the twisted bilayer with the ML fitted hoppings. For twisted bilayer graphene (TBG), we show the ability of the method to resolve the magic angle DOS at a substantial improvement in computational time. We use this new technique to scan through the database of stable 2D monolayers (MC2D) and reveal new twistable candidates across the five possible points groups in two-dimensions with a large DOS near the Fermi energy, with potentially exciting interacting physics to be probed in future experiments.",Materials Science,http://arxiv.org/abs/2512.16892v1,arXiv,2
"Equivariant atomistic machine learning models have brought substantial gains in both extrapolation capability and predictive accuracy. Depending on the basis of the space, two distinct types of irreducible representations are utilized. From architectures built upon spherical tensors (STs) to more recent formulations employing irreducible Cartesian tensors (ICTs), STs have remained dominant owing to their compactness, elegance, and theoretical completeness. Nevertheless, questions have persisted regarding whether ST constructions are the only viable design principle, motivating continued development of Cartesian networks. In this work, we introduce the Cartesian-3j and Cartesian-nj symbol, which serve as direct analogues of the Wigner-3j and Wigner-nj symbol defined for tensor coupling. These coefficients enable the combination of any two ICTs into a new ICT. Building on this foundation, we extend e3nn to support irreducible Cartesian tensor product, and we release the resulting Python package as cartnn. Within this framework, we implement Cartesian counterparts of MACE, NequIP, and Allegro, allowing the first systematic comparison of Cartesian and spherical models to assess whether Cartesian formulations may offer advantages under specific conditions. Using TACE as a representative example, we further examine whether architectures constructed from irreducible Cartesian tensor product and contraction(ICTP and ICTC) are conceptually well-founded in Cartesian space and whether opportunities remain for improving their design.",Materials Science,http://arxiv.org/abs/2512.16882v1,arXiv,2
"Molecular crystals possess a highly complex crystallographic landscape which in many cases results in the experimental observation of multiple crystal structures for the same compound. Accurate results can often be obtained for such systems by employing periodic density functional theory using hybrid functionals; however, this is not always computationally feasible. One possibility to circumvent these expensive periodic calculations is the utilization of multimer embedding methods. Therein, the fully periodic crystal is described at a lower level of theory, and subsequently monomer energies, dimer interaction energies, etc. are corrected via high-level calculations. In this paper, we further extend such a multimer embedding approach by one multimer order for all investigated properties, allowing us to compute lattice energies up to the tetramer embedding level, and atomic forces, the stress tensor, and harmonic phonons up to the trimer level. We test the significance of including these higher-order multimers by embedding PBE0+MBD multimers into periodic PBE+MBD calculations utilizing the X23 benchmark set of molecular crystals and comparing the results to explicit periodic PBE0+MBD calculations. We show that tetramer interactions systematically improve the lattice energy approximation and explore multiple possibilities for multimer selection. Furthermore, we confirm that trimer interactions are crucial for the description of the stress tensor, yielding cell volumes within 1 % of those of PBE0+MBD. Subsequently, this also results in an improvement of the description of vibrational properties, giving on average gamma point frequencies within 1.3 wave numbers and vibrational free energies within 0.3 kJ/mol of the PBE0+MBD results.",Materials Science,http://arxiv.org/abs/2512.16877v1,arXiv,2
"The use of highly sensitive pixelated direct detectors has dramatically improved the performance of high energy instrumentation such as transmission electron microscopy. Here, we describe a recently developed monolithic active pixel sensor designed for low energy scanning electron microscopy applications. This detector enables electron backscatter diffraction (EBSD) at lower energies and dose than are accessible with existing scintillator-coupled detectors, expanding grain orientation mapping capabilities to materials such as ceramics that are poor electron conductors. The high detector sensitivity allows collection of rich diffraction information - providing dislocation defect contrast that is otherwise not accessible via EBSD. Indeed, even the energy of single electron interaction events can be measured with this detector, which we demonstrate to energy filter diffraction patterns revealing details of how diffraction occurs at low energy.",Materials Science,http://arxiv.org/abs/2512.16839v1,arXiv,2
"Thermodynamic stability of N$_2$ clathrate hydrates in the sI and sII structures is investigated using density functional theory with several exchange-correlation functionals, explicitly accounting for composition (cage occupancies) and pressure at T = 0 K. Among the tested functionals, revPBE-D3(0) best reproduces experimental lattice parameters and bulk moduli B$_0$ . Energetic analyses confirm the strong impact of large cage double occupancy on sI, whereas the convex-hull results show that sI with single occupancy remains thermodynamically stable up to $\sim$ 0.8 GPa alongside sII with single occupancy. Increasing pressure then stabilizes sII with double occupancy, consistent with its larger large-cage volume and lower framework strain. These results provide a coherent, first-principles thermodynamic framework for N$_2$ hydrate stability and a baseline for finite-temperature extension.",Materials Science,http://arxiv.org/abs/2512.16819v1,arXiv,2
"Density-functional theory with on-site $U$ and inter-site $V$ Hubbard corrections (DFT+$U$+$V$) is a powerful and accurate method for predicting various properties of transition-metal compounds. However, its accuracy depends critically on the values of these Hubbard parameters. Although they can be determined empirically, first-principles methods provide a more consistent and reliable approach; yet, their results can vary, and a comprehensive comparison between methods is still lacking. Here, we present a systematic comparison of two widely used approaches for computing $U$ and $V$, namely linear-response theory (LRT) and the Hartree-Fock-based pseudohybrid functional formalism, applied to a representative set of oxides (MnO, NiO, CoO, FeO, BaTiO$_3$, ZnO, and ZrO$_2$). We find that for partially occupied transition-metal $d$ states, these two methods yield consistent $U$ values, but they differ for nearly empty or fully filled $d$ shells. For O-$2p$ states, LRT always predicts large $U$ values ($\sim$10 eV), whereas the pseudohybrid formalism produces system-dependent values depending on the level of localization and hybridization for the electronic states. Even larger differences are found for the inter-site $V$: the former predicts consistently small values ($<1$ eV), while the latter produces larger values ($\sim3$ eV), reflecting its explicit dependence on relative charge redistribution. Our results show that while parallels between these two methods exist, they rely on distinct assumptions for determining $U$ and $V$, leading to variations in predictions of material properties.",Materials Science,http://arxiv.org/abs/2512.16803v2,arXiv,2
"We present a spin-dependent extension of the non-orthogonal generalized Wannier function (NGWF) formalism within the framework of linear-scaling density functional theory (LS-DFT) as implemented in the ONETEP code. In traditional LS-DFT representations, both spin channels are constrained to share a common variational basis, which limits the accuracy for systems that are spin-polarized or exhibit magnetic order. Our approach allows NGWFs to vary independently for each spin channel, enabling a more accurate representation of spin-polarization in the electronic density. We demonstrate the efficacy of this method through a series of test cases, including localized magnetic defects in two-dimensional hBN, transition metal complexes, two-dimensional van der Waals magnetic materials, and both bulk and nanocluster ferromagnetic Co. In each scenario, the incorporation of spin-dependent NGWFs results in enhanced accuracy for total energy calculations, improved localization of spin density, and accurate predictions of magnetic ground states. This improvement is particularly notable when combined with DFT+U and DFT+U+J corrections. In this work, we take the opportunity to describe the combination of DFT+U+J and the projector-augmented wave (PAW) formalism within the LS-DFT framework, including how PAW participates in the ionic Pulay force, and in the minimum-tracking linear response approach for computing parameters in situ. Our findings demonstrate that spin-dependent NGWFs are a crucial and computationally efficient advancement in the linear-scaling DFT simulation of spin-polarized materials.",Materials Science,http://arxiv.org/abs/2512.16744v1,arXiv,2
"MXenes are promising candidates for electrochemical applications due to their high conductivity, tunable surface chemistry, and catalytic potential. However, their use in bulk electrode form remains unexplored despite advantages such as higher current density and improved mechanical integrity. Herein, we present a methodology for the fabrication of self-supported vdW solid Ti3C2Tz MXene electrodes, produced by cold compaction followed by vacuum heat treatment at 600 Â°C, which effectively removes interlayer confined water and stabilizes the bulk 3D structure. The resulting binder-free electrodes exhibit enhanced mechanical robustness along with structural and chemical stability in various electrolytes. The MXene electrodes demonstrate adequate HER activity while maintaining electrochemical stability over time, with minimal oxidation or changes in termination surface chemistry. This approach is scalable and cost-effective, overcoming limitations of nanoscale MXene architectures in electrochemical environments and offering a practical pathway toward MXene-based materials for sustainable hydrogen energy technologies.",Materials Science,http://arxiv.org/abs/2512.16729v2,arXiv,2
"Exfoliation and cleavage create two-dimensional (2D) materials and surfaces with physical and chemical properties distinct from their bulk parents. The rising class of non-van der Waals (non-vdW) 2D materials derived from non-layered crystals provides a fascinating new platform - greatly expanding the landscape of low-dimensional materials. Current computational models, however, provide limited guidance: existing descriptors are largely tailored to vdW layered systems. Here, we introduce a general framework predicting crystal cleavage and exfoliable 2D subunits directly from bulk structures. At its core is a universal eXfoliation and Cleavage Potential (XCP) enabling large-scale screening of diverse materials at negligible computational cost. Applying this approach, we obtain 37,208 cleavable surfaces and candidate non-vdW 2D materials from which we investigate 2,377 likely exfoliable ones using high-throughput density functional theory. We identify sheets with square and rectangular lattices, semiconducting systems exhibiting an indirect-to-direct band-gap transition upon exfoliation, and first non-vdW 2D metals. Our study thus opens a systematic route to explore and design new 2D materials with unprecedented chemical and structural diversity.",Materials Science,http://arxiv.org/abs/2512.16721v1,arXiv,2
"Foundational machine learning interatomic potentials (MLIPs) are being developed at a rapid pace, promising closer and closer approximation to ab initio accuracy. This unlocks the possibility to simulate much larger length and time scales. However, benchmarks for these MLIPs are usually limited to ordered, crystalline and bulk materials. Hence, reported performance does not necessarily accurately reflect MLIP performance in real applications such as heterogeneous catalysis. Here, we systematically analyze zero-shot performance of 80 different MLIPs, evaluating tasks typical for heterogeneous catalysis across a range of different data sets, including adsorption and reaction on surfaces of alloyed metals, oxides, and metal-oxide interfacial systems. We demonstrate that current-generation foundational MLIPs can already perform at high accuracy for applications such as predicting vacancy formation energies of perovskite oxides or zero-point energies of supported nanoclusters. However, limitations also exist. We find that many MLIPs catastrophically fail when applied to magnetic materials, and structure relaxation in the MLIP generally increases the energy prediction error compared to single-point evaluation of a previously optimized structure. Comparing low-cost task-specific models to foundational MLIPs, we highlight some core differences between these model approaches and show that -- if considering only accuracy -- these models can compete with the current generation of best-performing MLIPs. Furthermore, we show that no single MLIP universally performs best, requiring users to investigate MLIP suitability for their desired application.",Materials Science,http://arxiv.org/abs/2512.16702v1,arXiv,2
"TbFe$_{2}$D$_{4.2}$ deuteride crystallizes in a monoclinic structure ($Pc$ space group) with deuterium inserted into 18 tetrahedral [Tb2Fe2] and [TbFe3] interstitial sites. Its structural evolution versus temperature has been investigated by combining in-situ X-ray and neutron diffraction (XRD and NPD) with differential scanning calorimetry (DSC) experiments.Upon heating, the deuteride undergoes a reversible order-disorder transition from an ordered monoclinic structure to a disordered cubic structure between 320 and 380 K.Then, a multipeak thermal desorption occurs between 400 and 550 K, which can be explained by the transitions between different cubic deuterides separated by two-phase ranges. After controlled partial D desorption of TbFe$_{2}$D$_{4.2}$ , the XRD patterns of several TbFe$_{2}$D$_{x}$ deuterides were measured ex-situ using synchrotron radiation at room temperature, revealing the formation of different phases with cubic or monoclinic structures separated by two phase ranges.A tetragonal superstructure was observed for a phase with $x$ = 2. This work can explain previous results of the literature indicating the existence of cubic and, or rhombohedral hydrides depending on the hydrogenation conditions and the H content. The monoclinic structures reported here correspond to a slight distortion of the previous rhombohedral structures described by other authors.",Materials Science,http://arxiv.org/abs/2512.16692v1,arXiv,2
"Time-resolved multi-terahertz (THz) spectroscopy is used to observe pump fluence-dependent dynamics in the optical conductivity of photoexcited tin selenide (SnSe) over an ultrabroadband spectral range of 0.5 - 11 THz at fluences from 0.1 - 7.5 mJ/cm$^2$. At pump fluences below 3 mJ/cm$^2$, we observe a free carrier Drude spectrum and broad phonon resonances typical of a photoexcited semiconductor. With increasing fluence, a suppression of the DC photoconductivity is observed, indicating an interruption of long range transport due to phase disorder. Concomitantly, the c-axis polarized $B^2_{1u}$ optical phonon narrows and blueshifts, consistent with a transition to a higher symmetry structure. At intermediate fluences of 3.1 mJ/cm$^2$, a high frequency Lorentzian component appears that red shifts on a 50 ps time scale, where a Drude spectrum is eventually restored. Our results provide evidence for a non-thermal, photo-induced nucleation of higher symmetry, semi-metallic phase domains in SnSe appearing within 200 fs with the system relaxing back to its homogeneous $Pnma$ semiconducting phase on a 200 ps time scale.",Materials Science,http://arxiv.org/abs/2512.16669v1,arXiv,2
"Wigner crystals, lattices made purely of electrons, are a quintessential paradigm of studying correlation-driven quantum phase transitions. Despite decades of research, the internal dynamics of Wigner crystals has remained extremely challenging to access, with most experiments probing only static order or collective motion. Here, we establish monolayer WSe2 as a new materials platform to host zero-field Wigner crystals and then demonstrate that exciton spectroscopy provides a direct means to probe both static and dynamic properties of these electron lattices. We uncover striking optical resonances that we identify as Wigner polarons, quasiparticles formed when the electron lattice is locally distorted by exciton-Wigner crystal coupling. We further achieve all-optical control of spins in the Wigner crystal, directly probing valley-dependent Wigner polaron scattering well above the magnetic ordering temperature and in the absence of any external magnetic field. Finally, we demonstrate optical melting of the Wigner crystal and observe intriguingly different responses of the umklapp (static) and Wigner polaron (dynamic) resonances to optical excitation. Our results open up exciting new avenues for elucidating electron dynamics and achieving ultrafast optical control of interaction-driven quantum phase transitions in strongly correlated electron systems.",Materials Science,http://arxiv.org/abs/2512.16631v1,arXiv,2
"This paper reviews recent advances in the field of metallic glasses, focusing on the development of novel experimental techniques and in silico models. We discuss progress in experimental characterization, additive manufacturing, multiscale modeling approaches, and the growing role of machine learning in understanding and designing these complex materials. On the experimental side, we highlight measurements of thermophysical properties of supercooled liquids via fast chip calorimetry and enhancements in mechanical properties through rejuvenation treatments. This work underscores the crucial role of short-range order and medium-range order in controlling metallic glass mechanical properties. Recent progress in structural probes allows in situ observations of deformation mechanisms, positioning the field well to further advance our understanding of mechanical properties. Additive manufacturing of metallic glasses is discussed as one encouraging new manufacturing route for metallic glasses. We examine laser powder-bed fusion process physics and the central trade-off between amorphicity and densification, including heat affected zone devitrification and defects formation, together with emerging mitigation strategies and applications. On the theoretical and simulation side, we review advances in nanoscale, mesoscale, and continuum modeling of metallic glasses that have led to promising approaches by which multiscale schemes can incorporate data sourced from atomic-scale simulations. These efforts have helped to elucidate the connection between the glass structure and mechanical and rheological responses. We also cover the development of machine learning interatomic potentials for metallic glasses, along with machine learning driven prediction of glass forming ability and inverse design methods. Finally, challenges and directions for future research are presented and discussed.",Materials Science,http://arxiv.org/abs/2512.16590v1,arXiv,2
"Controlled activation of defect-bound excitonic states in two-dimensional semiconductors provides a route to isolated quantum emitters and a sensitive probe of defect physics. Here we demonstrate that \textit{in situ} high-temperature annealing of hBN-encapsulated monolayer WS$_2$ on a suspended microheater leads to the emergence of spectrally isolated single-photon emitters at cryogenic temperatures. Annealing at temperatures around 1100 K produces a sharp emission line, $X_L$, red-shifted by approximately 80 meV from the neutral exciton and exhibiting a linewidth below 200 $Î¼$eV. Photoluminescence excitation spectroscopy and power-dependent measurements show that $X_L$ originates from annealing-induced defects in the WS$_2$ monolayer, while second-order photon correlation measurements reveal clear antibunching with $g^{(2)}(0)<0.5$. These results establish high-temperature \textit{in situ} annealing as a controlled means to access defect-bound excitonic states and single-photon emission in van der Waals materials.",Materials Science,http://arxiv.org/abs/2512.16579v1,arXiv,2
"Copper nanoparticles (Cu NPs) have a broad applicability, yet their synthesis is sensitive to subtle changes in reaction parameters. This sensitivity, combined with the time- and resource-intensive nature of experimental optimization, poses a major challenge in achieving reproducible and size-controlled synthesis. While Machine Learning (ML) shows promise in materials research, its application is often limited by scarcity of large high-quality experimental data sets. This study explores ML to predict the size of Cu NPs from microwave-assisted polyol synthesis using a small data set of 25 in-house performed syntheses. Latin Hypercube Sampling is used to efficiently cover the parameter space while creating the experimental data set. Ensemble regression models, built with the AMADEUS framework, successfully predict particle sizes with high accuracy ($R^2 = 0.74$), outperforming classical statistical approaches ($R^2 = 0.60$). Overall, this study highlights that, for lab-scale synthesis optimization, high-quality small datasets combined with classical, interpretable ML models outperform traditional statistical methods and are fully sufficient for quantitative synthesis prediction. This approach provides a sustainable and experimentally realistic pathway toward data-driven inorganic synthesis design.",Materials Science,http://arxiv.org/abs/2512.16545v1,arXiv,2
"High-temperature solid oxide electrolysis cells (SOECs) are a potential core power-to-X (P2X) technology due to their unparalleled system efficiencies, that can exceed 85 % when excess heat from exothermic downstream processes is available. Recent advancements in materials, cell and stack design have enabled the deployment of megawatt (MW) scale demonstration plants and gigawatt (GW) scale manufacturing capacities. Consequently, key challenges to industrial-scale adoption scale now increasingly lie at the system level. Unlike previous SOEC reviews focused on materials and stack-level innovations, this work uniquely addresses emerging interdisciplinary system-level challenges and highlights the need for a paradigm shift. Several key insights are identified. Pressurized operation plays a crucial role in enhancing SOEC system performance and enabling better process integration. The dynamic capabilities of SOECs are better than often assumed and can further be improved via advanced operating strategies and modularization. Balance-of-plant (BoP) component costs rival stack capital expenditure, emphasizing the need for cost reductions through economies of scale via mass production and cross-industry synergies. Co-electrolysis remains at a lower technology readiness level and lacks MW scale demonstration. Furthermore, demonstrated integration with downstream processes across entire P2X chains remains scarce. Future research and development strategies are proposed, offering a roadmap to overcome these challenges and accelerate SOEC commercialization.",Materials Science,http://arxiv.org/abs/2512.16488v1,arXiv,2
"Titanium silicide is a key contact material in advanced three-dimensional semiconductor device architectures. Here, we examine the formation of ultrathin Ti-silicide on Si(100) using a combination of non-destructive in-situ and ex-situ ion scattering techniques capable of resolving composition and structure at the nanoscale. In-situ Time-of-Flight Low-Energy Ion Scattering (ToF-LEIS) indicates intermixing after annealing at 350 Â°C, with further compositional changes after annealing at 500 Â°C, including the emergence of a Si terminating layer at the surface. Consecutive ex-situ Time-of-Flight Medium-Energy Ion Scattering (ToF-MEIS) reveals a Ti-rich polycrystalline surface layer and a Si-rich interface layer exhibiting strong ordering along the Si [100] axis. High-Resolution Transmission Electron Microscopy (HR-TEM) images confirm these findings, revealing a $\approx$1.5 nm thick epitaxial silicide layer at the interface. The presence of an epitaxial interface is particularly promising for minimizing contact resistivity in ultrathin contact layers, where interfacial order can dominate electronic performance. In addition, both ToF-MEIS and HR-TEM unveil significant variations in the thickness of the silicide layer, with a substantial interface roughness but no translation of this roughness to the surface.",Materials Science,http://arxiv.org/abs/2512.16479v1,arXiv,2
"We extend the capabilities of correlation energy functionals based on the adiabatic-connection fluctuation-dissipation theorem by implementing the analytical atomic forces within the random phase approximation (RPA), in the context of plane waves and pseudopotentials. Forces are calculated at self-consistency through the optimized effective potential method and the Hellmann-Feynman theorem. In addition, non-self-consistent RPA forces, starting from the PBE generalized gradient approximation, are evaluated using density functional perturbation theory. In both cases, we find forces of excellent numerical quality. Furthermore, for most molecules and solids studied, self-consistency is found to have a negligible impact on the computed geometries and vibrational frequencies. The RPA is shown to systematically improve over PBE and, by including the exact-exchange kernel within RPA + exchange (RPAx), through finite-difference total energy calculations, we obtain an accuracy comparable to advanced wavefunction methods. Finally, we estimate the anharmonic shift and provide accurate theoretical references based on RPA and RPAx for the zone-center optical phonon of diamond, silicon, and germanium.",Materials Science,http://arxiv.org/abs/2512.16460v1,arXiv,2
"High-temperature superconductivity in cuprate materials remains a major challenge in physics due to the complexity of their strongly correlated electronic states. Interfacial strain is a powerful lever for tuning electronic correlations in complex oxides, offering new pathways to control emergent quantum phases. Here, we report the discovery of interfacial strain modulated correlated plasmons observed exclusively in superconducting La1.85Sr0.15CuO4 (LSCO) through spectroscopic ellipsometry. This form of plasmons is absent in the non-superconducting LSCO counterparts. Detailed analysis reveals that these correlated plasmons, arising from the collective excitations within Mott-correlated bands, are driven by long-range electronic correlations in the Cu-O planes. Furthermore, long-range electronic correlations, intricately modulated by interfacial strain, may play a crucial role in the emergence of superconductivity and in tuning the transition temperature. Dynamical cluster approximation (DCA) with quantum Monte Carlo (QMC) calculations of the extended Hubbard model suggest that long-range Coulomb interactions play an important role in LSCO, showing good agreement with our experimental findings. The collective evidence from both the experimental results and theoretical findings provides new insights into the nature of collective excitations and their pivotal role in the emergence of high-temperature superconductivity.",Materials Science,http://arxiv.org/abs/2512.16417v1,arXiv,2
"Tailoring the nanomorphology of organic photoactive layers through a specialized chain of processing steps is an imperative challenge on the path towards reliable and performant organic electronic manufacturing. This hurdle generally proves delicate to be overcome, as organic materials can be subject to many different phase transformation phenomena that are able to interfere with each other and produce a wide variety of morphological configurations with distinct structural, mechanical, and optoelectronic properties. A typical combination of such mechanisms, which the present systems are often prone to, and which is complex to investigate experimentally at the nanoscale, is the phase separation resulting from the interplay between amorphous demixing and crystallization.   In this work, an in-house Phase-Field modeling framework is employed to simulate and, consequently, explain the phenomenological behavior of a photoactive bulk heterojunction during a thermal annealing treatment. The model predictions are validated against available electron microscopy imaging of the nanostructural evolution during the process. It is demonstrated that the simulations can successfully provide a detailed comprehension of crystal nucleation and growth shaped by amorphous spinodal decomposition, so as to yield valuable insights for physically-based morphology control. In addition, this study shows the relevance of extensive thermodynamic and kinetic characterizations of organic semiconductor mixtures (e.g., phase diagram assessments, surface tension measurements, composition-dependent molecular diffusivity evaluations) for the associated field of research.",Materials Science,http://arxiv.org/abs/2512.16390v1,arXiv,2
"In recent years, counterparts of phenomena studied in spintronics have been actively explored in the orbital sector. The relationship between orbital degrees of freedom and crystal chirality has also been intensively investigated, although the distinction from gyrotropic properties has not been fully clarified. In this work, we investigate spin and orbital Edelstein effects as well as the nonlinear responses in the ternary transition-metal chalcogenide Cu$_2$WSe$_4$, which has a gyrotropic but achiral crystal structure. We find that in the Edelstein effect, magnetization is dominated by the orbital contribution rather than the spin contribution. On the other hand, both the nonlinear chiral thermoelectric (NCTE) Hall effect--a response to the cross product of the electric field and the temperature gradient--and the nonlinear Hall effect--conventional second-order response to the electric field--are found to be dominated by the Berry curvature dipole. We further find that spin-orbit coupling plays only a minor role in these effects, whereas the orbital degrees of freedom are essential. Finally, we demonstrate that the orbital magnetic-moment contributions to both the Edelstein effect and the NCTE Hall effect are closely linked to chirality, and we discuss the possibility of using them as a chirality indicator.",Materials Science,http://arxiv.org/abs/2512.16387v2,arXiv,2
"Rare-earth iron garnet (RE3Fe5O12) films are promising insulating ferrimagnets. They can show low magnetic damping, perpendicular magnetic anisotropy, and ultrafast spin dynamics, which makes them ideal for spin transport applications. In this work, we investigate the interaction between the magnetic sublattices in Er3Fe5O12 thin films grown by pulsed laser deposition on a Gd3Ga5O12 substrate. Structural and magnetic characterization reveals high-quality single-crystal growth, with compensation temperature close to the reported bulk value (~80 K). Magnetic phase diagrams based on element-specific measurements map out the regions where ferrimagnetic, canted, and aligned phases are stable across the compensation temperature. The micromagnetic dynamics resulting from perpendicular magnetic pulse perturbation of an in-plane magnetized layer was investigated at room temperature and reveals complex configurations. These results are a key feature for modulating magnetization dynamics through the compensation phenomenon, which is essential for spin-based devices operating in a low-temperature regime.",Materials Science,http://arxiv.org/abs/2512.16382v1,arXiv,2
"Recently, two unusual features were theoretically predicted for the Raman response of out-of-plane phonons in magnetic two-dimensional materials hosting massive Dirac fermions. First, the phase difference between certain Raman tensor elements was found to be quantized to $\pm Ï/2$, sensitive only to the sign of the Dirac fermion mass. Second, a selection rule was identified in the Raman intensity under circularly polarized light, which generalizes the well-known optical valley selection rule. These predictions were based on a low-energy effective model in the continuum approximation. Here, we test the robustness of those results for more realistic theoretical approaches. First, we calculate the Raman tensor for an electronic tight-binding model on a honeycomb lattice with broken time-reversal and inversion symmetries. Second, we compute the Raman tensor from density functional theory (DFT) for a monolayer of ferromagnetic 2H-RuCl$_2$. Both calculations corroborate the analytical results found in the continuum model, thereby theoretically confirming the peculiar behaviour of the Raman tensor for two dimensional massive Dirac fermion systems.",Materials Science,http://arxiv.org/abs/2512.16377v1,arXiv,2
"The Flory-Huggins theory is a well-established lattice model that is commonly used to study the mixing of distinct chemical species. It can successfully predict phase separation phenomena in blends of incompatible materials. However, it is limited to amorphous mixtures, excluding systems where the phase segregation is shaped by the concurrent crystallization of one or several blend components. A generalization of the Flory-Huggins formalism is thus necessary to capture the coupling and the interplay of crystallization with amorphous demixing mechanisms, such as spinodal decomposition. This work therefore revolves around the derivation of a free energy model for multicomponent mixtures that encompasses the physics of both processes. It is detailed which concepts from the original Flory-Huggins theory are required to apprehend the presented developments and how the current framework is built upon them. Furthermore, additional discussion points address chemical potential calculations and selected examples of binary and ternary phase diagrams, thereby highlighting the variety of blend behaviors that can be represented.",Materials Science,http://arxiv.org/abs/2512.16370v1,arXiv,2
"With the availability of ever-increasing computational capabilities, robust and automated research workflows are essential to enable and facilitate the execution and orchestration of large numbers of interdependent simulations in supercomputer facilities. However, the execution of these workflows still typically requires technical expertise in setting up calculation inputs, interpreting outputs, and handling the complexity of parallel code execution on remote machines. To address these challenges, the AiiDAlab platform was developed, making complex computational workflows accessible through an intuitive user interface that runs in a web browser. Here, we discuss how AiiDAlab has matured over the past few years, shifting its focus from computational materials science to become a powerful platform that accelerates scientific discovery across multiple disciplines. Thanks to its design, AiiDAlab allows scientists to focus on their research rather than on computational details and challenges, while keeping automatically track of the full simulation provenance via the underlying AiiDA engine and thus ensuring reproducibility. In particular, we discuss its adoption into quantum chemistry, atmospheric modeling, battery research, and even experimental data analysis at large-scale facilities, while also being actively used in educational settings. Driven by user feedback, significant effort has been made to simplify user onboarding, streamline access to computational resources, and provide robust mechanisms to work with large datasets. Furthermore, AiiDAlab is being integrated with electronic laboratory notebooks (ELNs), reinforcing adherence to the FAIR principles and supporting researchers in data-centric scientific disciplines in easily generating reproducible Open Research Data (ORD).",Materials Science,http://arxiv.org/abs/2512.22173v1,arXiv,2
"The discovery of high-temperature superconductivity in La$_3$Ni$_2$O$_7$ under high pressure has sparked a surge of research into Ruddlesden-Popper (RP) nickelates. Currently, stabilizing the bilayer RP phases with smaller $A$-site ions remains a significant challenge. In this work, we have successfully synthesized medium- and high-entropy bilayer nickelates, La$_{1.2}$Pr$_{0.6}$Nd$_{0.6}$Sm$_{0.6}$Ni$_2$O$_{7-Î´}$ and La$_{0.67}$Pr$_{0.67}$Nd$_{0.67}$Sm$_{0.33}$Eu$_{0.33}$Gd$_{0.33}$Ni$_2$O$_{7-Î´}$, by utilizing the concept of configuration entropy stabilization. The high-entropy nickelate exhibits the smallest unit-cell volume and the largest orthorhombic distortion reported to date. The chemical pressure induced by the smaller A-site ions significantly enhances the NiO$_6$ octahedral rotation/distortion and shortens the interlayer Ni-Ni interatomic spacing. Physical property measurements reveal bad electrical conductivity alongside a markedly elevated density-wave transition temperature. Notably, the superconducting transition temperature extrapolated from structural correlations is projected to exceed 100 K. Our work not only demonstrates entropy stabilization of bilayer nickelates, but also reveals the effect of $A$-site-ion size on the crystal structure and physical properties, opening a new pathway for developing nickelate superconductors and tuning their electronic properties.",Materials Science,http://arxiv.org/abs/2512.16268v1,arXiv,2
"We investigated the out-of-plane magnetoresistance of pressurized black phosphorus (BP) with a longitudinal field configuration. Despite the absence of the Lorentz force in the present configuration, we observed a significant enhancement of magnetoresistance marked with a clear onset field in both the semiconducting (1.1 GPa) and semimetallic (1.3 GPa) phases. The insulating behavior observed near the semiconductor-semimetal transitio pressure is possibly associated with emergence of an excitonic phase, which has been suggested in a recent theoretical study. BP under finely tuned pressure can be a candidate to realize the field-induced electronic phase transition in a moderate magnetic field below 9 T.",Materials Science,http://arxiv.org/abs/2512.16252v1,arXiv,2
"Radiation damage in high-temperature cuprate superconductors represents one of the main technological challenges for their deployment in harsh environments, such as fusion reactors and accelerator facilities. Their complex crystal structure makes modeling irradiation effects in this class of materials a particularly demanding task, for which existing damage models remain inadequate. In this work, we develop an atomistic-based approach for describing primary radiation damage in YBa2Cu3O7, by coupling Molecular Dynamics and Binary Collision Approximation simulations in a way that makes them complementary. When integrated with Primary Knock-on Atom spectra obtained from Monte Carlo codes, our results establish a framework for multiscale modeling of radiation damage, enabling quantitative estimates of several damage descriptors, such as defect production, defect clustering, and the effective damaged volume for any specific irradiation conditions where collision cascades dominate. This computational approach is suitable for the prediction of irradiation effects in any complex functional oxide, with applications ranging from aerospace to nuclear fusion and high-energy physics.",Materials Science,http://arxiv.org/abs/2512.16249v1,arXiv,2
"Spatially periodic modulations of the superconducting gap have been recently reported in diverse materials and are often attributed to pair density wave order. An alternative mechanism, termed pair-breaking scattering interference (PBSI), was proposed to produce gap modulations without finite-momentum pairing. Here we investigate signatures of PBSI in bulk FeSe using scanning tunneling microscopy with superconductive tips, enabling enhanced energy resolution and Josephson tunneling. Subsurface magnetic scatterers with Yu-Shiba-Rusinov states are identified in FeSe, around which we observe particle-hole symmetric gap modulations accompanied by spatial modulation of the Josephson current. Those modulations have wavevectors consistent with intra-pocket PBSI. We further demonstrate that phase-referenced quasiparticle interference imaging offers an independent and direct probe of PBSI beyond gap mapping. These results establish PBSI as a viable origin of gap modulations in superconductors lacking preexisting charge/spin density wave orders, and motivate further investigation of the intriguing gap modulation phenomenology.",Materials Science,http://arxiv.org/abs/2512.16211v1,arXiv,2
"The unexpected emergence of ferroelectricity in HfO2 at reduced dimensions has attracted considerable attention, as it provides a pathway toward the realization of ultrasmall ferroelectric devices. Ab initio calculations suggest that this effect arises from a unique mode coupling, in which an antipolar displacement mode stabilizes a robust polar distortion. Based on these insights, Landau-Devonshire energy models have been proposed using such lattice modes as order parameters. However, most existing models are limited to a simplified one-dimensional model because of the computational cost of ab initio calculations and the limitations of conventional Landau polynomials. Here, we constructed a three-dimensional Landau-Devonshire potential for HfO2 by employing the tetragonal, antipolar, and polar modes as coupled order parameters, based on the latest machine-learning technologies. We generated a large-scale dataset of energies over a three-dimensional structural space, with the computational cost drastically reduced through the use of machine-learning interatomic potentials, and trained a multilayer perceptron (MLP) to learn the relationship between the order parameters and the energy. The energy predicted by the MLP successfully captures the characteristic coupling behavior whereby the antipolar modes induce the polar mode. Furthermore, by extending this MLP-based Landau potential to a position-dependent functional, that is, to a phase-field modeling framework, we revealed that the polarization magnitude in thin films decreases compared with the bulk state, while the critical strain required for the onset of spontaneous polarization increases due to surface effects. This study presents a new framework for the on-demand construction of Landau energy and phase-field modeling using the latest machine-learning techniques, enabling multiscale analysis of complex ferroelectric phenomena.",Materials Science,http://arxiv.org/abs/2512.16207v1,arXiv,2
"Establishing symmetry-protected topological (SPT) phases with interactions in chemically realistic systems remains an open challenge. We show that a single, synthetically plausible organic one-dimensional chain, tunable via chemical modification of its radical sites, hosts two such phases: an odd-Haldane phase of a dimerized $S=\tfrac{1}{2}$ Heisenberg chain and a Haldane phase of an $S=1$ chain realized when Hund coupling locks two $S=\tfrac{1}{2}$ spins per monomer into $S=1$. Density-functional theory places the active manifold deep in the Mott regime ($U/t\!\approx\!126$), justifying a spin-only Heisenberg description; a compact $(t,U)\!\to\!J$ mapping then fixes exchange couplings. Exact diagonalization and DMRG reveal a consistent SPT fingerprint across both phases, including a quantized many-body Zak phase, even-degenerate entanglement spectrum, protected edge spins, and characteristic triplon/Haldane features in $S^{+-}(q,Ï)$. Our results identify a chemically programmable molecular platform for interacting SPT physics in one dimension and suggest concrete spectroscopic routes to organic Haldane spin chains for nanoscale quantum devices.",Materials Science,http://arxiv.org/abs/2512.16173v1,arXiv,2
"The thermal stability and structural evolution of a GaN high-electron-mobility transistor (HEMT) heterostructure grown on a Si (111) substrate were investigated using in situ high-temperature X-ray diffraction (HT-XRD), reciprocal space mapping (RSM), Raman spectroscopy, and rocking-curve (RC) analysis at varying temperatures. The heterostructure, consisting of a p-GaN cap, an AlGaN barrier, and a GaN channel supported by two AlGaN/AlGaN superlattice (SL) buffer layers, maintained clear and periodic satellite peaks up to a temperature of 1000 deg C, confirming excellent structural integrity. Symmetric and asymmetric RSM results reveal that both the Si and GaN diffraction peaks shift to lower angles with increasing temperature, consistent with thermal expansion, and show no significant broadening or relaxation throughout the heating process. The c-lattice constant follows the theoretical expansion predicted by the multi-frequency Einstein model, whereas the a-lattice expansion is slower due to in-plane strain constraints imposed by the underlying Si substrate and buffer layers. Rapid lattice contraction during the fast-cooling stage induces a residual compressive strain of approximately 0.3 percent in the GaN channel after cooling. Raman spectra further confirm this strain state through a blue shift of approximately 1.5 cm-1 of the GaN E2 (high) phonon mode, corresponding to an in-plane strain of about 0.2 percent. Rocking-curve analysis reveals an increase in both screw and edge dislocation densities by 28 percent and 12 percent, respectively. These results collectively demonstrate that the GaN HEMT heterostructure exhibits robust crystalline stability up to 1000 deg C, with only minor strain redistribution and limited dislocation activity, providing experimental evidence for GaN device applications under high-temperature conditions.",Materials Science,http://arxiv.org/abs/2512.16172v1,arXiv,2
"Catalyst design is crucial for materials synthesis, especially for complex reaction networks. Strategies like collaborative catalytic systems and multifunctional catalysts are effective but face challenges at the nanoscale. Carbon nanotube synthesis contains complicated nanoscale catalytic reactions, thus achieving high-density, high-quality semiconducting CNTs demands innovative catalyst design. In this work, we present a holistic framework integrating machine learning into traditional catalyst design for semiconducting CNT synthesis. It combines knowledge-based insights with data-driven techniques. Three key components, including open-access electronic structure databases for precise physicochemical descriptors, pre-trained natural language processing-based embedding model for higher-level abstractions, and physical - driven predictive models based on experiment data, are utilized. Through this framework, a new method for selective semiconducting CNT synthesis via catalyst - mediated electron injection, tuned by light during growth, is proposed. 54 candidate catalysts are screened, and three with high potential are identified. High-throughput experiments validate the predictions, with semiconducting selectivity exceeding 91% and the FeTiO3 catalyst reaching 98.6%. This approach not only addresses semiconducting CNT synthesis but also offers a generalizable methodology for global catalyst design and nanomaterials synthesis, advancing materials science in precise control.",Materials Science,http://arxiv.org/abs/2512.16151v1,arXiv,2
"Particulate composites underpin many solid-state chemical and electrochemical systems, where microstructural features such as multiphase boundaries and inter-particle connections strongly influence system performance. Advances in X-ray microscopy enable capturing large-scale, multimodal images of these complex microstructures with an unprecedentedly high throughput. However, harnessing these datasets to discover new physical insights and guide microstructure optimization remains a major challenge. Here, we develop a machine learning (ML) enabled framework that enables automated transformation of experimental multimodal X-ray images of multiphase particulate composites into scalable, topology-aware graphs for extracting physical insights and establishing local microstructure-property relationships at both the particle and network level. Using the multiphase particulate cathode of solid-state lithium batteries as an example, our ML-enabled graph analysis corroborates the critical role of triple phase junctions and concurrent ion/electron conduction channels in realizing desirable local electrochemical activity. Our work establishes graph-based microstructure representation as a powerful paradigm for bridging multimodal experimental imaging and functional understanding, and facilitating microstructure-aware data-driven materials design in a broad range of particulate composites.",Materials Science,http://arxiv.org/abs/2512.16085v1,arXiv,2
"Infrared transparent conductors have long been sought due to their broad optoelectronic applications in the infrared wavelength range. However, the search for ideal materials has been limited by the inherent trade-off between electrical conductance and optical transmittance. Band engineering offers an effective approach to modulate carrier type and density, enabling concurrent tuning of both conductance and transmittance. In this work, we present a band engineering strategy that enables effective tuning of both infrared transmittance and electrical conductance in topological insulator (Bi1-xSbx)2Te3, bridging the gap and paving the way for applying topological insulators to infrared photoelectric devices. More importantly, with the combination of high carrier mobility and a large optical dielectric constant as suggested by previous report, Sb2Te3 achieves a high electrical conductance (~1000 S/cm) and outstanding infrared transmittance (92.3%) in the wavelength range of 8~13 um, demonstrating strong potential as an infrared transparent conductor. Our findings reveal that concurrent enhancement of both carrier mobility and optical dielectric constant is key to overcoming the conductance-transmittance trade-off. This work provides valuable insight for the exploration of high-performance infrared transparent conducting materials.",Materials Science,http://arxiv.org/abs/2512.16064v1,arXiv,2
"Understanding the growth of large cracks in brittle materials is the most fundamental problem in fracture mechanics. Under out-of-plane shear loading, an initially planar crack may fragment into multiple cracks, forming an echelon crack pattern. Explaining this phenomenon is essential for developing a general theory of crack growth. Although numerous empirical criteria have been proposed in the literature, none provide a unified explanation of all observed features and are largely restricted to two-dimensional growth in linear elastic isotropic materials. In this Letter, we confront a classical set of echelon crack growth experiments using two phase-field approaches: the classical variational model and a strength-constrained model. We show that, contrary to prevailing views, the variational model based solely on Griffith's energetic competition between elastic and fracture energies is fundamentally incomplete even for predicting the growth of large cracks. By incorporating a material strength surface that constrains the regions in which a crack can grow, the resulting model accurately predicts echelon crack growth without invoking any ad hoc assumptions about material or geometrical disorder. Results are presented for both soft and hard materials, confirming the model's general applicability to any brittle material. We further identify two governing non-dimensional parameters that control crack orientation and morphology and demonstrate that one of them, the ratio of shear to tensile strength, determines whether crack paths are more influenced by energy-based or stress-based empirical criteria, thereby reconciling these criteria within a single framework.",Materials Science,http://arxiv.org/abs/2512.16053v1,arXiv,2
"Quantum wells made of two-dimensional organic-inorganic hybrid perovskites (2D-PKs) offer a high degree of flexibility in tailoring optoelectronic properties through carrier confinement and functional interlayers. Compared to their 3D counterparts, 2D-PKs exhibit tunable photoluminescence, excitonic binding at room temperature and enhanced structural stability. However, the dynamics of photo-induced charge carriers and their transport properties are highly intertwined due to the interplay of diverse excitation species, charge carrier cooling, transport, and radiative and non-radiative recombination. In this study, we employ optical-pump terahertz-probe spectroscopy (OPTP) to analyze the local conductivity dynamics of 2D and 3D methylammonium lead iodide (MAPI) perovskites at timescales down to picoseconds. Remarkably, we observe an intensity-dependent, 2D-specific buildup of an ultrafast, few-picosecond decay in local conductivity. By combining OPTP with transient absorption (TA) and picosecond time-resolved photoluminescence (TRPL), we demonstrate the disentanglement of photoconductivity and carrier population. This allows us to attribute the 2D-specific ultrafast THz response to delayed hot-carrier cooling and subsequent exciton formation, which effectively reduces the free-carrier conductivity. This intensity-dependent, ultrafast THz response is a signature of the recently identified hot-carrier bottleneck in 3D MAPI, and this effect manifests itself in a unique form in the 2D material. These results encourage further investigations on the impact of functional organic interlayers and provide insights into designing tunable carrier responses for ultrafast devices via adapted heterostructures and confinement.",Materials Science,http://arxiv.org/abs/2512.16052v1,arXiv,2
"Non-volatile capacitive memories (nvCAPs) exhibiting AC small-signal capacitance on/off ratio (Con/Coff) with non-destructive read have emerged as a promising device for next-generation memory paradigms. Recently, BEOL-compatible ferroelectric nvCAPs with an amorphous oxide semiconductor channel have been reported, suggesting the possibility of monolithic 3D integration of nvCAPs on top of CMOS. So far, the characterization studies on oxide-channel ferroelectric nvCAPs have been done using dual DC sweep C-V measurements which are typically performed over a time scale of a few seconds. However, non-volatile memory arrays typically require nvCAPs to operate under pulse-mode. It is thus crucial to advance understanding of the behavior of oxide-channel ferroelectric nvCAPs under pulse-mode operation, governed by the unique interplay between ferroelectric layer and oxide channel physics. In this study, we provide a systematic study of the pulse-mode operation of ferroelectric nvCAPs with an amorphous oxide semiconductor channel, including its pulse-based write characteristics and reliability characteristics. We examine overlap area, wake-up and pulse-width dependent Con and Coff writing characteristics under pulse-mode. Further, we suggest the importance of optimizing ferroelectric depolarization for Con retention, while reducing read-after-delay for Coff retention under pulse-mode. Lastly, non-destructive read operation for >10^9 read stress cycles at |Vread|=1V is demonstrated.",Materials Science,http://arxiv.org/abs/2512.16040v1,arXiv,2
"Ammonia oxidation on platinum catalysts is pivotal for industrial nitric acid production and environmental abatement, yet the role of surface oxides in this process remains debated. Using operando surface X-ray diffraction (SXRD), crystal truncation rod (CTR) analysis, and near-ambient pressure X-ray photoelectron spectroscopy (NAP-XPS), we reveal that Pt(111) does not form stable surface oxides under ammonia oxidation conditions. Instead, transient hexagonal monolayers and a Pt(111)-(8x8) superstructure emerge under oxygen-rich atmospheres and above the catalyst light-off temperature, but vanish upon ammonia exposure. Real-time mass spectrometry and NAP-XPS demonstrate that the reaction proceeds via a Langmuir-Hinshelwood mechanism, where adsorbed NHx and O species availability dictate selectivity toward NO or N2. Reducing the oxygen pressure by an order of magnitude slows the kinetics of oxide growth, only detected after 24 hr, and facilitated by transient and precursor structures.",Materials Science,http://arxiv.org/abs/2512.16020v1,arXiv,2
"We develop a general theoretical framework for computing the time-resolved magneto-optical Kerr effect in ultrafast pump-probe setups, formulated within the Dynamical Projective Operatorial Approach (DPOA) and its application to the generalized linear-response theory for pumped systems. Furthermore, we exploit this formalism to express the post-pump optical conductivity and consequently the Kerr rotation in terms of the time-evolved single-particle density matrix (SPDM), providing a transparent and computationally efficient description of photo-excited multi-band systems. This extension, in addition to its lower computational cost, has the advantage of allowing the inclusion of phenomenological damping. We illustrate the formalism using both (i) a two-band tight-binding model, which captures the essential physics of ultrafast spin-charge dynamics and the Kerr rotation, and (ii) weakly spin-polarized germanium, as a realistic playground with a complex band structure. The results demonstrate that, by exploiting DPOA and/or its SPDM extension, one can reliably reproduce both the short-time features under the pump-pulse envelope and the long-time dynamics after excitation, offering a versatile framework for analyzing time-resolved magneto-optical Kerr effect experiments in complex materials. Moreover, this analysis clearly shows that the Kerr rotation can be used to deduce experimentally the relevant n-photon resonances for a given specific material.",Materials Science,http://arxiv.org/abs/2512.16014v1,arXiv,2
"Understanding the atomic-scale mechanisms governing metal-mediated nucleation and growth of gallium nitride (GaN) and related alloys is critical for tailoring their structural and functional properties in advanced electronic, optoelectronic, and quantum devices. Using real-time environmental transmission electron microscopy (E-TEM) in conjunction with Gibbs free energy calculations, we elucidate the distinct processes of GaN nucleation and growth from Ga droplet arrays with and without GaN pre-nuclei. For the lowest temperatures, although GaN nucleation at Ga droplet arrays is not observed, GaN growth occurs preferentially at pre-existing GaN nuclei, presumably due to the reduced Gibbs free energy for NH3 decomposition at Ga/GaN interfaces. For intermediate to high temperatures, E-TEM reveals nucleation and growth of GaN from Ga droplets with and without GaN nuclei, with enhanced crystallinity for the GaN nuclei, due to epitaxial templating. These results highlight the critical role of the Ga/GaN interface in facilitating NH3 decomposition and GaN growth, offering fundamental insights into metal-mediated nucleation and growth of GaN and related materials.",Materials Science,http://arxiv.org/abs/2512.15989v1,arXiv,2
"The inhomogeneous distribution of P1 centers in type 1b HPHT diamond samples allows multiple DNP mechanisms to occur within the same crystal, resulting in complex DNP spectra. At some crystal orientations, different DNP mechanisms can compete to drive hyperpolarization with different signs at the same applied microwave frequency. We perform microwave-irradiated DNP using both monochromatic and frequency-modulated microwave excitation to explore the competition between these DNP mechanisms in diamond at room temperature. We demonstrate that frequency-modulated DNP is a tool for suppressing certain DNP mechanisms while enhancing others in a single-crystal diamond sample. Frequency modulation also enables higher enhancement of the NMR signal beyond traditional monochromatic DNP under some conditions. In a powder sample, competing enhancement mechanisms can also arise from different crystallite orientations in the powder. We observe that at certain microwave frequencies the DNP signal changes sign during the polarization build-up, even with monochromatic microwave irradiation. We do not observe this phenomenon in any single-crystal spectrum. We discuss both methods of investigating competing mechanisms of DNP as a means of selectively enhancing different DNP mechanisms driving $^{13}$C NMR signal enhancement.",Materials Science,http://arxiv.org/abs/2512.15982v1,arXiv,2
"Refractory complex concentrated alloys (RCCAs) are of significant interest for advanced high-temperature applications, owing to their broad compositional range and potential for attractive mechanical properties and oxidation resistance. However, their compositional complexity poses significant challenges to conventional alloy discovery methodologies. In this study, an active learning framework is introduced that integrates Gaussian process regression with Bayesian global optimization to accelerate identification of oxidation-resistant RCCAs. Focusing on aluminum-containing quaternary systems, alloy and oxide descriptors were used to predict oxidation performance at 1000$^\circ$C. Beginning with a dataset of 81 experimentally validated RCCAs, this framework was used to iteratively select alloy batches (five alloys per batch) with optimization based on a balance between exploration and exploitation to minimize associated experimental costs. After six iterations, two alloys were identified (nominal Al$_{30}$Mo$_5$Ti$_{15}$Cr$_{50}$ and Al$_{40}$Mo$_5$Ti$_{30}$Cr$_{25}$) that exhibited specific mass gains less than 1 mg/cm$^2$ at 1000$^\circ$C in air. Both of these alloys formed adherent external $Î±$-Al$_2$O$_3$ scales and exhibited parabolic oxidation kinetics consistent with diffusion-limited scale growth. Furthermore, our multiobjective analysis demonstrates that these alloys simultaneously achieve high specific hardness ($>0.12$ HV$_{0.5}$m$^3$/kg) and thermal expansion compatibility with thermal barrier coating systems, positioning them as promising bond coat candidates. This work underscores the efficacy of active learning in traversing complex compositional landscapes, and offers a scalable strategy for the development of advanced materials suitable for extreme environments.",Materials Science,http://arxiv.org/abs/2512.15958v1,arXiv,2
"A machine-learned interatomic potential (MLIP) for multilayer MoS2 was developed using the ultra-fast force field (UF3) framework. The UF3 MLIP reproduces key properties in strong agreement with DFT including lattice constants, interlayer binding energies, and phase-stability. Furthermore, the potential reasonably captures the phonon spectra and the highly anisotropic elastic tensor across monolayer (1H) and bulk (2H, 3R) MoS2 phases. Critically, defect and edge formation energies are captured with excellent fidelity, exhibiting a strong correlation with DFT (R^2 = 0.91) across ten defective monolayers and reproducing the relative difference between the free energies of zigzag and armchair edges within 5% of DFT. Non-equilibrium molecular dynamics simulations reveal layered homoepitaxial growth consistent with experimental observations, demonstrating the formation of van der Waals gaps between successive epilayers and triangular domains bounded by zigzag edges. The robust UF3 MLIP, which is only ~2X slower than the fastest empirical potentials, enables large-scale atomistic simulations of MoS2 epitaxial growth.",Materials Science,http://arxiv.org/abs/2512.15952v1,arXiv,2
"We investigate many-body effects on the spin-split electron bands in altermagnets by computing the electron self-energy arising from interactions with magnons, phonons, and hybridized magnon-phonon modes. These interactions lead to band broadening, which can obscure the intrinsic spin-splitting in spectroscopic measurements. We consider a $d$-wave Lieb lattice altermagnet as a representative example. Our results reveal that the spin-splitting remains spectroscopically resolvable and provide theoretical estimates of lifetime effects relevant for experimental detection. For electron-magnon coupling, we find a distinct difference between spectral function broadening for up and down spins close to the Fermi surface, which is not present in the case of electron-phonon coupling. We relate it to the spin splitting of the magnon modes in altermagnets. The results, including magneto-elastic coupling, are very similar to the pure magnon case. This provides insights into quasiparticle dynamics in altermagnets and contributes to the broader understanding of many-body interactions in spin-split systems. By including the temperature dependence of the self-energies, we also quantify how thermal fluctuations influence the broadening of the electronic states.",Materials Science,http://arxiv.org/abs/2512.15859v1,arXiv,2
"Van der Waals heterostructures promise on-demand designer quantum phases through control of monolayer composition, stacking, twist angle, and external fields. Yet, experimental efforts have been narrowly focused, leaving much of this vast moirÃ© landscape unexplored and potential promises unrealized. Here, we present a scalable workflow for high-throughput characterization of twisted homobilayers and apply it to $K$-valley semiconductors. Combining small-scale density functional theory with perturbation theory, we efficiently extract moirÃ© band gaps, valley Chern numbers, magic angles, and the threshold for lattice relaxation. Beyond this rapid high-throughput characterization, we parameterize a continuum model for each material, which provides a starting point for more detailed study. Our survey delivers an actionable map for systematic exploration of correlated and topological phases in moirÃ© homobilayers, and identifies promising new platforms: chromium-based transition metal dichalcogenides for high-temperature quantum anomalous Hall effects, transition metal nitride halides for intertwined superconducting and moirÃ© physics, and atomically thin $\rm{III-V}$ semiconductors for room-temperature-scale moirÃ© effects.",Materials Science,http://arxiv.org/abs/2512.15851v1,arXiv,2
"Single crystal spinel CoFe$_2$O$_4$ exhibits the largest room-temperature saturation magnetostriction among non-rare-earth compounds and a high Curie temperature ($T_c \sim 780$ K), properties that are critical to a wide range of industrial and medical applications. Neutron spectroscopy reveals a large band splitting ($\sim$ 60 meV) between two ferrimagnetic magnon branches, which is driven by site mixing between Co$^{2+}$ and Fe$^{3+}$ cations, and a significantly weaker magnetocrystalline anisotropy ($\sim$ 3 meV). Central to this behavior is the competition between extremely large mismatched molecular fields on the tetrahedral $A$-site and octahedral $B$-site sublattices and the single-ion anisotropy on the $B$-site. This creates a strong energetic anisotropy that locks the magnetic moment within each structural domain in place. As a result of these differing energy scales, switching structural domains is energetically favored over a global spin reorientation under applied magnetic fields, and this is what amplifies the magnetostrictive nature of CoFe$_2$O$_4$.",Materials Science,http://arxiv.org/abs/2512.15683v1,arXiv,2
"We report a comprehensive study of pressure-induced evolution of the magnetism and development of superconductivity (SC) in MnBi8Te13, a promising ambient pressure, ferromagnetic (FM) topological insulator candidate. By employing high-pressure electrical transport, magnetoresistance, DC magnetic susceptibility, and X-ray diffraction measurements, we construct a detailed temperature-pressure phase diagram. At ambient pressure, MnBi8Te13 exhibits FM ordering with an easy-axis along the c-axis which is progressively suppressed under pressure and replaced by an antiferromagnetic (AFM) order. Density functional theory calculations predicted an evolution from FM to a G-type AFMg2 phase near 5 GPa. Above 16.6 GPa, a bulk SC state emerges with a maximum transition temperature ~6.8 K, as confirmed by resistance and magnetic susceptibility measurements. This pressure-induced SC may co-exist with another AFM dome that shows a weak anomaly in the transport data. In contrast, our work on MnBi6Te10 shows no SC up to 40 GPa. Indeed, in contrast to MnBi8Te13, the MnBi2Te4(Bi2Te3)n compounds with n < 3, didn't exhibit SC, highlighting the crucial role of Mn concentration in stabilizing SC. The observation of pressure-induced FM-AFM-SC transitions in MnBi8Te13 not only establishes it as a rare Mn-based SC but also provides a platform to study the interplay between magnetism, SC, and potentially nontrivial band topology in correlated magnetic materials.",Materials Science,http://arxiv.org/abs/2512.15667v2,arXiv,2
"Making solution-cast organic solar cells industrially available generally comes at the cost of significant performance losses compared to device prototypes manufactured under laboratory conditions. Adjusting solvent evaporation kinetics is postulated to recover efficiency. Yet, a comprehensive characterization of their effect, independently of other property-defining parameters, is lacking. Thus, the present objective is to isolate the influence of the solvent drying rate on solution-deposited organic active layer nanomorphologies and performances. To this end, a specially designed gas quenching technique is employed to fabricate PM6:Y6 donor-acceptor films under systematic variations of evaporation conditions. Using an extensive investigation protocol that combines insights from numerical simulations and experimental measurements, process-structure-performance relationships are unraveled. It is found that higher drying rates imply finer and more dispersed nanomorphologies with increased fractions of amorphous material. This enhances electric charge generation, thereby improving short-circuit current density and overall cell performance. The open-circuit voltage is also boosted under accelerated evaporation due to changes in the aggregation mode of the Y6 small molecule that induce higher effective bandgaps. The results demonstrate that the developed gas-quenching technique is a valuable tool for optimizing the performance of upscaled organic photovoltaics, as it is readily compatible with high-throughput equipment, such as roll-to-roll coating machines.",Materials Science,http://arxiv.org/abs/2512.15638v1,arXiv,2
"Cavity quantum electrodynamics offers a powerful route to manipulate material properties. However, it is unclear whether and how quantized fields affect crystals periodicity. Here, we extend Bloch's theorem to crystals under the strong light-matter coupling, showing that polariton quasiparticles preserve lattice periodicity. We formulate a general framework to incorporate the effect of multimode cavity fields in a simple and tractable way. We find that the additional modes contribute to the system's energy by small modifications that become noticeable only at low frequencies. Within the single-photon approximation the multimode contribution manifests as a spatially uniform effective field in the crystal's plane. This provides a formal justification for the single-mode and long-wavelength approximations commonly used in molecular polaritonics. This work establishes a rigorous theoretical framework that clarifies how polaritonic states in crystalline solids should be described.",Materials Science,http://arxiv.org/abs/2512.15623v1,arXiv,2
"R2In (R = rare earth) intermetallics exhibit unusual magnetic and magnetocaloric properties, driven by subtle electronic effects, lattice distortions, and spin-lattice coupling. Most of these binary compounds adopt the hexagonal Ni2In-type structure at room temperature, with Eu2In and Yb2In stabilizing in the orthorhombic Co2Si-type lattice. Lighter lanthanide compounds Eu2In, Nd2In, and Pr2In undergo first-order magnetic transitions with negligible hysteresis and minimal lattice volume change and exhibit giant cryogenic magnetocaloric effects, while heavy lanthanide R2In compounds including Gd2In show second-order transitions with moderate magnetocaloric effect. No lanthanide-based R2In compound exhibits symmetry-breaking structural transition, while Y2In transforms from hexagonal to orthorhombic structure near 250 K. Secondary low-temperature transitions, including spin reorientation or antiferromagnetic ordering, further enrich the magnetic phase landscape in these compounds. Integrating theoretical descriptors such as charge-induced strain and electronic structure provides predictive insight into phase stability and magnetocaloric performance, guiding the design of rare-earth intermetallics with tunable magnetic properties for cryogenic applications",Materials Science,http://arxiv.org/abs/2512.15589v1,arXiv,2
"Determining the nature of surface roughness and electrode pore structure on H2 bubble evolution rate and quantity, and bubble trapping under electrolytic conditions is important for quantifying useful gas production during total water splitting and hydrogen evolution reactions. Controlled electrode systems involving the design of geometry, surface area, and porosity provides options to understand trapped/redissolved gas bubble evolution and improve overall efficiency. In this study, we use vat polymerization (Vat-P) 3D-printing to create ordered microlattice electrode structures from metal and metal-oxide coated photopolymerized methacrylate-based resins. These micro-lattice structures are designed with various geometries to influence bubble traffic from gas nucleation and evolution during electrochemical HER processes. Using cyclic and linear sweep voltammetry, and chronopotentiometry, this work analyzes the response of metallized (NiO/Ni(OH)2 and Au) microlattice HER electrodes as a function of geometric structure, to gauge influence of material activity, small scale surface roughness, and the larger substrate pore network on the traffic or larger bubbles formed during HER. This work also uses broadband acoustic resonance dissolution spectroscopy (BARDS) to quantify bubble evolution and reabsorption in the electrolyte during electrolysis. The results show that coated 3D printed electrodes are robust HER electrodes, allow efficient transport of small bubbles, but significant limitations are found for larger bubble transport through ordered porous microlattice shown through model simulations and experimental measurements.",Materials Science,http://arxiv.org/abs/2512.15826v1,arXiv,2
"The dynamics of disordered nuclear spin ensembles are the subject of nuclear magnetic resonance studies. Due to the through-space long-range dipolar interaction generically many spins are involved in the time evolution, so that exact brute force calculations are impossible. The recently established spin dynamic mean-field theory (spinDMFT) represents an efficient and unbiased alternative to overcome this challenge. The approach only requires the dipolar couplings as input and the only prerequisite for its applicability is that each spin interacts with a large number of other spins. In this article, we show that spinDMFT can be used to describe spectral spin diffusion in static samples and to simulate zero-quantum line shapes which eluded an efficient quantitative simulation so far to the best of our knowledge. We perform benchmarks for two test substances that establish an excellent match with published experimental data. As spinDMFT combines low computational effort with high accuracy, we strongly suggest to use it for large-scale simulations of spin diffusion, which are important in various areas of magnetic resonance.",Materials Science,http://arxiv.org/abs/2512.15572v1,arXiv,2
"Large language models (LLMs) are increasingly applied to scientific research, yet prevailing science benchmarks probe decontextualized knowledge and overlook the iterative reasoning, hypothesis generation, and observation interpretation that drive scientific discovery. We introduce a scenario-grounded benchmark that evaluates LLMs across biology, chemistry, materials, and physics, where domain experts define research projects of genuine interest and decompose them into modular research scenarios from which vetted questions are sampled. The framework assesses models at two levels: (i) question-level accuracy on scenario-tied items and (ii) project-level performance, where models must propose testable hypotheses, design simulations or experiments, and interpret results. Applying this two-phase scientific discovery evaluation (SDE) framework to state-of-the-art LLMs reveals a consistent performance gap relative to general science benchmarks, diminishing return of scaling up model sizes and reasoning, and systematic weaknesses shared across top-tier models from different providers. Large performance variation in research scenarios leads to changing choices of the best performing model on scientific discovery projects evaluated, suggesting all current LLMs are distant to general scientific ""superintelligence"". Nevertheless, LLMs already demonstrate promise in a great variety of scientific discovery projects, including cases where constituent scenario scores are low, highlighting the role of guided exploration and serendipity in discovery. This SDE framework offers a reproducible benchmark for discovery-relevant evaluation of LLMs and charts practical paths to advance their development toward scientific discovery.",Materials Science,http://arxiv.org/abs/2512.15567v1,arXiv,2
"This research work introduces the DFT through FP-LAPW+lo technique in WIEN2k software to obtain information about structural, thermoelectric, and optoelectronic characteristics of CaZnC and CaZnSi materials. The structural optimization was performed using PBE-GGA functional, while the rest of the characteristics were obtained with the PBE-GGA + TB-mBJ approach. The thermoelectric parameters were evaluated using BoltzTraP software. The elastic constants and other mechanical parameters were computed by utilizing the ELAST code within the WIEN2k software, while the thermodynamic characteristics were evaluated using the Gibbs2 program. The findings show a correlation between atomic composition and lattice dimensions while finding that CaZnC has a direct ($Î$-$Î$) band gap of $1.186$ eV, whereas CaZnSi has an indirect ($Î$-$X$) band gap of $1.067$ eV. The optical studies of the compounds show potential applications for photovoltaics while the thermoelectric results find optimized power factors and figure of merit values for energy conversion performance. The elastic parameters of CaZnC and CaZnSi demonstrate material stability and brittleness. Lastly, the thermodynamic evaluations provide information about the thermal mechanism and disorder of the materials. As a result, this research work provides significant advancements in the understanding of the fundamentals of these compounds and highlights their promising applications in renewable energy technologies.",Materials Science,http://arxiv.org/abs/2512.15822v2,arXiv,2
"We fabricate (In,Ga)N pseudo-substrates with a total thickness of ~1 um grown on GaN templates using plasma-assisted molecular beam epitaxy. In a three-step process, we change growth conditions from N-rich to metal-rich in order to sequentially form a roughened GaN layer, relaxed (In,Ga)N nanostructures, and a coalesced, smooth (In,Ga)N layer. Samples are analyzed by scanning electron and atomic force microscopy, X-ray diffraction, as well as photo- and cathodoluminescence spectroscopy. Compared to a reference layer grown directly on GaN, the pseudo-substrate exhibits a higher In content (~0.3), strain relaxation degree (~80%), narrower photoluminescence linewidth, and larger area fraction of bright regions in cathodoluminescence maps, showing the benefits of the three-step growth protocol. This straightforward approach does not necessitate any ex-situ processing and could enable the scalable fabrication of (In,Ga)N pseudo-substrates for high-efficiency red-emitting (In,Ga)N devices.",Materials Science,http://arxiv.org/abs/2512.15565v1,arXiv,2
"Recent reports of strong room-temperature photoluminescence in hexagonal diamond (2H) germanium stand in marked contrast to theoretical predictions of very weak band-edge optical transitions. Here we address radiative emission in 2H-Ge and related materials through a comprehensive investigation of their excitonic properties and radiative lifetimes, performing Bethe-Salpeter calculations on pristine and uniaxially strained 2H-Ge, 2H-Si$_x$Ge$_{1-x}$ alloys with $x=\frac{1}{6},\,\frac{1}{4},\,\frac{1}{2}$, and wurtzite GaN as a reference. Pristine 2H-Ge features sizable exciton binding energies ($\sim\!30$ meV) but extremely small dipole moments, yielding radiative lifetimes above $10^{-4}$ s. Alloying with Si reduces the lifetime by nearly two orders of magnitude, whereas a 2% uniaxial strain along the $c$ axis induces a band crossover that strongly enhances the in-plane dipole moment of the lowest-energy exciton and drives the lifetime down to the nanosecond scale. Although strained 2H-Ge approaches the radiative efficiency of GaN, its much lower exciton energy prevents a full match. These results provide the missing excitonic description of 2H-Ge and 2H-Si$_x$Ge$_{1-x}$, demonstrating that, even when excitonic effects are fully accounted for, the strong photoluminescence reported experimentally cannot originate from the ideal crystal.",Materials Science,http://arxiv.org/abs/2512.15559v1,arXiv,2
"Two-dimensional (2D) materials, such as graphene, transition metal dichalcogenides (TMDs), and hBN, exhibit intriguing properties that are sensitive to their atomic-scale structures and can be further enriched through van der Waals (vdW) integration. However, the precise synthesis and clean integration of 2D materials remain challenging. Here, using graphene or hBN as a vdW capping layer, we create a nano-confined environment that directs the growth kinetics of 2D TMDs (e.g., NbSe2 and MoS2), enabling precise formation of TMD monolayers with tailored morphologies, from isolated monolayer domains to large-scale continuous films and intrinsically-patterned rings. Moreover, Janus S-Mo-Se monolayers are synthesized with atomic precision via vdW-protected bottom-plane chalcogen substitution. Importantly, our approach simultaneously produces ultraclean vdW interfaces. This in situ encapsulation reliably preserves air-sensitive materials, as evidenced by the enhanced superconductivity of nano-confined NbSe2 monolayers. Altogether, our study establishes a versatile platform for the controlled synthesis and integration of 2D TMDs for advanced applications.",Materials Science,http://arxiv.org/abs/2512.15518v1,arXiv,2
"The development of oxidation-resistant high-entropy alloy (HEA) bond coats is restricted by the limited understanding of how multi-principal element interactions govern scale formation across temperatures. This study uncovers new oxidation trends in NiCoCrAl HEAs using a data-driven analysis of high-fidelity experimental oxidation data. The results reveal a clear temperature-dependent transition between alumina- and chromia-dominated protection, identifying the compositional regimes where alloys rich in Al dominate at $\ge1150$ Â°C, mixed Al-Cr chemistries are optimal at intermediate temperatures, and, unexpectedly, Cr-rich low-Al alloys perform best at 850 Â°C-challenging the assumption that high Al is universally required. The effects of Hf and Y are shown to be strongly composition-dependent with Hf producing the largest global reduction in oxidation rate, while Y becomes effective primarily in NiCo-lean alloys. Y-Hf co-doping offers consistent improvement but exhibits site-saturation behavior. These insights identify new high-performing HEA bond-coat families, including $\mathrm{Ni_{17}Co_{23}Cr_{30}Al_{30}}$ as a substitute for conventional mutlilayer thermal barrier coatings.",Materials Science,http://arxiv.org/abs/2512.15517v1,arXiv,2
"We analyze the electromechanical response of the 180 degree ferroelectric domain wall in tetragonal PbTiO3 by combining first-principles calculations with a Landau-Ginzburg-Devonshire (LGD) description. Using regular multidomain structures with varying domain-wall density, we extract polarization profiles and lattice distortions and map them onto the continuum model to determine conventional (homogeneous) and gradient (inhomogeneous) electrostriction. Conventional electrostriction yields only a small negative length change of the sample, whereas gradient electrostriction--arising from the coupling between strain and polarization gradients--produces a positive contribution nearly an order of magnitude larger and localized at the wall core. Our results demonstrate that gradient electrostriction dominates the electromechanical response of 180 degree walls in PbTiO3, supporting its inclusion in LGD models that stabilize Bloch-type domain wall structures.",Materials Science,http://arxiv.org/abs/2512.15498v1,arXiv,2
"Point defects introduce localized electronic states that critically affect carrier trapping, recombination, and transport in functional materials. The associated charge transition levels (CTLs) can depend on temperature, requiring accurate treatment of vibrational and electronic free-energy contributions. In this work, we use machine-learned interatomic potentials to efficiently compute temperature-dependent CTLs for vacancies in MgO, LiF, and CsSnBr3. Using thermodynamic integration, we quantify free-energy differences between charge states and calculate the vibrational entropy contributions at finite temperatures. We find that CTLs shift with temperature in MgO, LiF and CsSnBr3 from both entropy and electronic contributions. Notably, in CsSnBr3 a neutral charge state becomes thermodynamically stable above 60 K, introducing a temperature-dependent Fermi-level window absent at 0 K. We show that the widely used static, zero-kelvin defect formalism can miss both quantitative CTL shifts and the qualitative emergence of new stable charge states.",Materials Science,http://arxiv.org/abs/2512.15463v1,arXiv,2
"Polariton condensation is a potential system state for performing analog computations, given that it exhibits quantum behavior at macroscopic scales readily probed with low-cost optical methods. Current methods of fabricating devices in polariton microcavities largely involve patterning the devices via e-beam lithography before the cavity is completed, which offers less flexibility in device creation and reduces the maximum possible refractive index contrast. Moreover, the momentum and spatial distributions of the condensate are highly dependent on the host platform, and it has been difficult to preserve the desired behavior when modifying a given cavity. Here we introduce a method that addresses both of these challenges with the creation of polaritonic circuits of arbitrary forms etched via Focused Ion Beam into an organic microcavity based on Rhodamine 3B Perchlorate within a Small Molecule Ionic Isolation Lattices complex. We demonstrate room temperature condensation and propagation of polaritons in rectangular and trapezoidal waveguides by analyzing spatial and angle-resolved photoluminescence. We also discuss the blue-shifting and non-zero momentum of the condensate and show that it is strongly confined up to several higher energy levels. As an example, we report the spatial profiles of condensation in custom devices, such as a ring waveguide, a Y-splitter, and a Mach-Zehnder interferometer. This work represents a first step towards the realization of more complex, fully integrated, coherent polaritonic circuits operating at room temperature.",Materials Science,http://arxiv.org/abs/2512.15451v1,arXiv,2
"Designing lithium-ion batteries for long service life remains a challenge, as most cells are optimized for beginning-of-life metrics such as energy density, often overlooking how design and operating conditions shape degradation. This work introduces a degradation-aware design framework built around finite, interacting reservoirs (lithium, porosity, and electrolyte) that are depleted over time by coupled degradation processes.   We extend a physics-based Doyle-Fuller-Newman model to include validated mechanisms such as SEI growth, lithium plating, cracking, and solvent dry-out, and simulate how small design changes impact lifetime. Across more than 1,000 cycles, we find that increasing electrolyte volume by just 1% or porosity by 5% can extend service life by over 30% without significantly affecting cell energy density. However, lithium excess, while boosting initial capacity, can accelerate failure if not supported by sufficient structural or ionic buffers.   Importantly, we show that interaction between reservoirs is crucial to optimal design: multi-reservoir tuning yields either synergistic benefits or compound failures, depending on operating conditions. We also quantify how C-rate and operating temperature influence degradation pathways, emphasizing the need for co-optimized design and usage profiles.   By reframing degradation as a problem of managing finite internal reservoirs, this work offers a predictive and mechanistic foundation for designing lithium-ion batteries that balance energy, durability, and application-specific needs.",Materials Science,http://arxiv.org/abs/2512.15440v1,arXiv,2
